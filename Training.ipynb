{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Hr_ezXe1Ez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "501226a9-2e9b-4d02-f939-08901cb2bb74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GgYfTupe7zd"
      },
      "source": [
        "import math\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpHKuugonzR"
      },
      "source": [
        "def custom_tiny_model(num_classes):\n",
        "    input_image = Input(shape=(416, 416, 3))\n",
        "    final_out = (num_classes + 5) * 3\n",
        "    x   = input_image\n",
        "    filters = 16 \n",
        "    for i in range(4): # idx : from 0 to 3\n",
        "        x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "        x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "        x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "        x   = MaxPool2D(pool_size=[2, 2], strides=[2, 2], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "        filters *= 2\n",
        "    # idx : 4\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : \n",
        "    #i += 1\n",
        "    skip_2 = x\n",
        "    x   = MaxPool2D(pool_size=[2, 2], strides=[2, 2], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "    # # idx : 5\n",
        "    filters *= 2\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    x   = MaxPool2D(pool_size=[2, 2], strides=[1, 1], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "    # idx : 6\n",
        "    filters *= 2\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 7\n",
        "    ################################################ Transfer Learning ####################################################\n",
        "    i += 1\n",
        "    x   = Conv2D(256, 1, strides = 1, padding = 'same', use_bias=False, name = 'TL_conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # \n",
        "    skip_1 = x    \n",
        "    i += 1\n",
        "    x   = Conv2D(512, 3, strides = 1, padding = 'same', use_bias=False, name = 'TL_conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='TL_bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='TL_leaky_' + str(i))(x)\n",
        "    # \n",
        "    i += 1\n",
        "    y_large   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'TL_conv_' + str(i))(x)\n",
        "    # \n",
        "    i += 1\n",
        "    x   = Conv2D(128, 1, strides = 1, padding = 'same', use_bias=False, name = 'TL_conv_' + str(i))(skip_1)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='TL_bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='TL_leaky_' + str(i))(x)\n",
        "    # \n",
        "    i += 1\n",
        "    x   = UpSampling2D(2, name = 'TL_upsampling_' + str(i))(x)\n",
        "    # \n",
        "    i += 1\n",
        "    x = Concatenate(name = 'TL_concatenate_' + str(i))([x, skip_2])\n",
        "    # \n",
        "    i += 1\n",
        "    x   = Conv2D(256, 3, strides = 1, padding = 'same', use_bias=False, name = 'TL_conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='TL_bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='TL_leaky_' + str(i))(x)\n",
        "    # \n",
        "    i += 1\n",
        "    y_small   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'TL_conv_' + str(i))(x)\n",
        "    # reshape\n",
        "    y_small_shape = tf.shape(y_small) \n",
        "    y_large_shape = tf.shape(y_large)\n",
        "    y_small = tf.reshape(y_small, (y_small_shape[0], y_small_shape[1], y_small_shape[2], 3, -1),name='TL_reshape_small')\n",
        "    y_large = tf.reshape(y_large, (y_large_shape[0], y_large_shape[1], y_large_shape[2], 3, -1),name='TL_reshape_large')\n",
        "    new_model = tf.keras.Model(input_image, (y_small, y_large))\n",
        "    return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-WWLBLbrGj5"
      },
      "source": [
        "def TL_tiny_model(num_classes):\n",
        "    model = custom_tiny_model(num_classes)\n",
        "    for layer in model.layers:\n",
        "        if not(layer.name.startswith(\"TL\")):\n",
        "            layer.trainable = False\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQYvANV0sAEz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4180eac-a2ee-46a7-99cd-0c9d099b8c4a"
      },
      "source": [
        "TL_tiny_model(6).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_0 (Conv2D)                 (None, 416, 416, 16) 432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_0 (BatchNormalization)    (None, 416, 416, 16) 64          conv_0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_0 (LeakyReLU)             (None, 416, 416, 16) 0           bnorm_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_0 (MaxPooling2D)           (None, 208, 208, 16) 0           leaky_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 208, 208, 32) 4608        bool_0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_1 (BatchNormalization)    (None, 208, 208, 32) 128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_1 (LeakyReLU)             (None, 208, 208, 32) 0           bnorm_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_1 (MaxPooling2D)           (None, 104, 104, 32) 0           leaky_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 104, 104, 64) 18432       bool_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_2 (BatchNormalization)    (None, 104, 104, 64) 256         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_2 (LeakyReLU)             (None, 104, 104, 64) 0           bnorm_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_2 (MaxPooling2D)           (None, 52, 52, 64)   0           leaky_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 52, 52, 128)  73728       bool_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_3 (BatchNormalization)    (None, 52, 52, 128)  512         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_3 (LeakyReLU)             (None, 52, 52, 128)  0           bnorm_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_3 (MaxPooling2D)           (None, 26, 26, 128)  0           leaky_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 26, 26, 256)  294912      bool_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_4 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_4 (LeakyReLU)             (None, 26, 26, 256)  0           bnorm_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_4 (MaxPooling2D)           (None, 13, 13, 256)  0           leaky_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_5 (Conv2D)                 (None, 13, 13, 512)  1179648     bool_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_5 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_5 (LeakyReLU)             (None, 13, 13, 512)  0           bnorm_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bool_5 (MaxPooling2D)           (None, 13, 13, 512)  0           leaky_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_6 (Conv2D)                 (None, 13, 13, 1024) 4718592     bool_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_6 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_6 (LeakyReLU)             (None, 13, 13, 1024) 0           bnorm_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_7 (Conv2D)              (None, 13, 13, 256)  262144      leaky_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bnorm_7 (BatchNormalization)    (None, 13, 13, 256)  1024        TL_conv_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_7 (LeakyReLU)             (None, 13, 13, 256)  0           bnorm_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_10 (Conv2D)             (None, 13, 13, 128)  32768       leaky_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "TL_bnorm_10 (BatchNormalization (None, 13, 13, 128)  512         TL_conv_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "TL_leaky_10 (LeakyReLU)         (None, 13, 13, 128)  0           TL_bnorm_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "TL_upsampling_11 (UpSampling2D) (None, 26, 26, 128)  0           TL_leaky_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "TL_concatenate_12 (Concatenate) (None, 26, 26, 384)  0           TL_upsampling_11[0][0]           \n",
            "                                                                 leaky_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_13 (Conv2D)             (None, 26, 26, 256)  884736      TL_concatenate_12[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_8 (Conv2D)              (None, 13, 13, 512)  1179648     leaky_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "TL_bnorm_13 (BatchNormalization (None, 26, 26, 256)  1024        TL_conv_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "TL_bnorm_8 (BatchNormalization) (None, 13, 13, 512)  2048        TL_conv_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "TL_leaky_13 (LeakyReLU)         (None, 26, 26, 256)  0           TL_bnorm_13[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "TL_leaky_8 (LeakyReLU)          (None, 13, 13, 512)  0           TL_bnorm_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_14 (Conv2D)             (None, 26, 26, 33)   8481        TL_leaky_13[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "TL_conv_9 (Conv2D)              (None, 13, 13, 33)   16929       TL_leaky_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(4,)]               0           TL_conv_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_1 (TensorFlow [(4,)]               0           TL_conv_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_TL_reshape_small/sh [(5,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_TL_reshape_large/sh [(5,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_4[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_TL_reshape_small (T [(None, None, None,  0           TL_conv_14[0][0]                 \n",
            "                                                                 tf_op_layer_TL_reshape_small/shap\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_TL_reshape_large (T [(None, None, None,  0           TL_conv_9[0][0]                  \n",
            "                                                                 tf_op_layer_TL_reshape_large/shap\n",
            "==================================================================================================\n",
            "Total params: 8,687,794\n",
            "Trainable params: 2,386,498\n",
            "Non-trainable params: 6,301,296\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6mEDBX4fMJ6"
      },
      "source": [
        "def make_tiny_yolov3_model():\n",
        "    input_image = Input(shape=(416, 416, 3))\n",
        "    final_out = (80 + 5) * 3\n",
        "    x   = input_image\n",
        "    filters = 16\n",
        "    for i in range(4): # idx : from 0 to 3\n",
        "        x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "        x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "        x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "        x   = MaxPool2D(pool_size=[2, 2], strides=[2, 2], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "        filters *= 2\n",
        "    # idx : 4\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : \n",
        "    #i += 1\n",
        "    skip_2 = x\n",
        "    x   = MaxPool2D(pool_size=[2, 2], strides=[2, 2], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "    # # idx : 5\n",
        "    filters *= 2\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    x   = MaxPool2D(pool_size=[2, 2], strides=[1, 1], padding = 'same', name = 'bool_' + str(i))(x)\n",
        "    # idx : 6\n",
        "    filters *= 2\n",
        "    i += 1\n",
        "    x   = Conv2D(filters, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 7\n",
        "    i += 1\n",
        "    x   = Conv2D(256, 1, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # \n",
        "    #i += 1\n",
        "    skip_1 = x\n",
        "\n",
        "    # idx : 8\n",
        "    i += 1\n",
        "    x   = Conv2D(512, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 9\n",
        "    i += 1\n",
        "    y_large   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'conv_' + str(i))(x)\n",
        "    # idx : 10\n",
        "    i += 1\n",
        "    x   = Conv2D(128, 1, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(skip_1)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 11\n",
        "    i += 1\n",
        "    x   = UpSampling2D(2, name = 'upsampling_' + str(i))(x)\n",
        "    # idx : 12\n",
        "    i += 1\n",
        "    x = Concatenate(name = 'concatenate_' + str(i))([x, skip_2])\n",
        "    # idx : 13\n",
        "    i += 1\n",
        "    x   = Conv2D(256, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 14\n",
        "    i += 1\n",
        "    y_small   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'conv_' + str(i))(x)\n",
        "    return tf.keras.Model(input_image, (y_large, y_small)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBnn_s8NutdG"
      },
      "source": [
        "#**Transfer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCIi5UlMvb7l"
      },
      "source": [
        "def our_tiny_model(num_classes, first_time):\n",
        "    final_out = (5 + num_classes)*3\n",
        "    model  = make_tiny_yolov3_model()\n",
        "    if first_time:\n",
        "      model.load_weights('/content/gdrive/My Drive/tiny_weights/tiny-yolo3.h5')\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    x      = model.get_layer('leaky_7').output\n",
        "    skip_2 = model.get_layer('leaky_4').output\n",
        "\n",
        "    #\n",
        "    i   = 20\n",
        "    x   = Conv2D(256, 1, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    skip_1 = x\n",
        "    i += 1\n",
        "    x   = Conv2D(512, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 9\n",
        "    i += 1\n",
        "    y_large   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'conv_' + str(i))(x)\n",
        "    # idx : 10\n",
        "    i += 1\n",
        "    x   = Conv2D(128, 1, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(skip_1)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 11\n",
        "    i += 1\n",
        "    x   = UpSampling2D(2, name = 'upsampling_' + str(i))(x)\n",
        "    # idx : 12\n",
        "    i += 1\n",
        "    x = Concatenate(name = 'concatenate_' + str(i))([x, skip_2])\n",
        "    # idx : 13\n",
        "    i += 1\n",
        "    x   = Conv2D(256, 3, strides = 1, padding = 'same', use_bias=False, name = 'conv_' + str(i))(x)\n",
        "    x   = BatchNormalization(epsilon=0.001, name='bnorm_' + str(i))(x)\n",
        "    x   = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(x)\n",
        "    # idx : 14\n",
        "    i += 1\n",
        "    y_small   = Conv2D(final_out, 1, strides = 1, padding = 'same', name = 'conv_' + str(i))(x)\n",
        "    # reshape\n",
        "    y_small_shape = tf.shape(y_small) \n",
        "    y_large_shape = tf.shape(y_large)\n",
        "    y_small = tf.reshape(y_small, (y_small_shape[0], y_small_shape[1], y_small_shape[2], 3, -1),name='detector_reshape_small')\n",
        "    y_large = tf.reshape(y_large, (y_large_shape[0], y_large_shape[1], y_large_shape[2], 3, -1),name='detector_reshape_large')\n",
        "    new_model = tf.keras.Model(model.input, (y_small, y_large))\n",
        "    return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNxeyiPHkjL"
      },
      "source": [
        "#**utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOvZ7EIlf08e"
      },
      "source": [
        "def xywh_to_x1y1x2y2(box):\n",
        "    xy = box[..., 0:2]\n",
        "    wh = box[..., 2:4]\n",
        "\n",
        "    x1y1 = xy - wh / 2\n",
        "    x2y2 = xy + wh / 2\n",
        "\n",
        "    y_box = tf.concat([x1y1, x2y2], axis=-1)\n",
        "    return y_box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spf2tJssgpBP"
      },
      "source": [
        "def broadcast_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    calculate iou between one box1iction box and multiple box2 box in a broadcast way\n",
        "    inputs:\n",
        "    box1: a tensor full of boxes, eg. (3, 4)\n",
        "    box2: another tensor full of boxes, eg. (3, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # assert one dimension in order to mix match box1 and box2\n",
        "    # eg: \n",
        "    # box1 -> (3, 1, 4)\n",
        "    # box2 -> (1, 3, 4)\n",
        "    box1 = tf.expand_dims(box1, -2)\n",
        "    box2 = tf.expand_dims(box2, 0)\n",
        "\n",
        "    # derive the union of shape to broadcast\n",
        "    # eg. new_shape -> (3, 3, 4)\n",
        "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box1), tf.shape(box2))\n",
        "\n",
        "    # broadcast (duplicate) box1 and box2 so that\n",
        "    # each box2 has one box1 matched correspondingly\n",
        "    # box1: (3, 3, 4)\n",
        "    # box2: (3, 3, 4)\n",
        "    box1 = tf.broadcast_to(box1, new_shape)\n",
        "    box2 = tf.broadcast_to(box2, new_shape)\n",
        "\n",
        "    # minimum xmax - maximum xmin is the width of intersection.\n",
        "    # but has to be greater or equal to 0\n",
        "    interserction_w = tf.maximum(\n",
        "        tf.minimum(box1[..., 2], box2[..., 2]) - tf.maximum(\n",
        "            box1[..., 0], box2[..., 0]), 0)\n",
        "    # minimum ymax - maximum ymin is the height of intersection.\n",
        "    # but has to be greater or equal to 0\n",
        "    interserction_h = tf.maximum(\n",
        "        tf.minimum(box1[..., 3], box2[..., 3]) - tf.maximum(\n",
        "            box1[..., 1], box2[..., 1]), 0)\n",
        "    intersection_area = interserction_w * interserction_h\n",
        "    box1_area = (box1[..., 2] - box1[..., 0]) * \\\n",
        "        (box1[..., 3] - box1[..., 1])\n",
        "    box2_area = (box2[..., 2] - box2[..., 0]) * \\\n",
        "        (box2[..., 3] - box2[..., 1])\n",
        "    # intersection over union\n",
        "    return intersection_area / (box1_area + box2_area - intersection_area)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA7YwBAagqcT"
      },
      "source": [
        "def binary_cross_entropy(logits, labels):\n",
        "    epsilon = 1e-7\n",
        "    logits = tf.clip_by_value(logits, epsilon, 1 - epsilon)\n",
        "    return -(labels * tf.math.log(logits) +\n",
        "             (1 - labels) * tf.math.log(1 - logits))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnjf9OdAgyEJ"
      },
      "source": [
        "def get_absolute_yolo_box(y_pred, valid_anchors_wh, num_classes):\n",
        "    \"\"\"\n",
        "    Given a cell offset prediction from the model, calculate the absolute box coordinates to the whole image.\n",
        "    It's also an adpation of the original C code here:\n",
        "    https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L83\n",
        "    note that, we divide w and h by grid size \n",
        "    inputs:\n",
        "    y_pred: Prediction tensor from the model output, in the shape of (batch, grid, grid, anchor, 5 + num_classes)\n",
        "    outputs:\n",
        "    y_box: boxes in shape of (batch, grid, grid, anchor, 4), the last dimension is (xmin, ymin, xmax, ymax)\n",
        "    objectness: probability that an object exists\n",
        "    classes: probability of classes\n",
        "    \"\"\"\n",
        "\n",
        "    t_xy, t_wh, objectness, classes = tf.split(\n",
        "        y_pred, (2, 2, 1, num_classes), axis=-1)\n",
        "\n",
        "    objectness = tf.sigmoid(objectness)\n",
        "    classes = tf.sigmoid(classes)\n",
        "    #####################################################################\n",
        "    # Pr(class(i)) = Pr(class(i)|Object) * Pr(Object)\n",
        "\t  # multiply each class probability with the objectness score\n",
        "    classes = classes * objectness\n",
        "\n",
        "    grid_size = tf.shape(y_pred)[1]\n",
        "    # meshgrid generates a grid that repeats by given range. It's the Cx and Cy in YoloV3 paper.\n",
        "    # for example, tf.meshgrid(tf.range(3), tf.range(3)) will generate a list with two elements\n",
        "    # note that in real code, the grid_size should be something like 13, 26, 52 for examples here and below\n",
        "    #\n",
        "    # [[0, 1, 2],\n",
        "    #  [0, 1, 2],\n",
        "    #  [0, 1, 2]]\n",
        "    #\n",
        "    # [[0, 0, 0],\n",
        "    #  [1, 1, 1],\n",
        "    #  [2, 2, 2]]\n",
        "    #\n",
        "    C_xy = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
        "\n",
        "    # next, we stack two items in the list together in the last dimension, so that\n",
        "    # we can interleve these elements together and become this:\n",
        "    #\n",
        "    # [[[0, 0], [1, 0], [2, 0]],\n",
        "    #  [[0, 1], [1, 1], [2, 1]],\n",
        "    #  [[0, 2], [1, 2], [2, 2]]]\n",
        "    #\n",
        "    C_xy = tf.stack(C_xy, axis=-1)\n",
        "\n",
        "    # let's add an empty dimension at axis=2 to expand the tensor to this:\n",
        "    #\n",
        "    # [[[[0, 0]], [[1, 0]], [[2, 0]]],\n",
        "    #  [[[0, 1]], [[1, 1]], [[2, 1]]],\n",
        "    #  [[[0, 2]], [[1, 2]], [[2, 2]]]]\n",
        "    #\n",
        "    # at this moment, we now have a grid, which can always give us (y, x)\n",
        "    # if we access grid[x][y]. For example, grid[0][1] == [[1, 0]]\n",
        "    C_xy = tf.expand_dims(C_xy, axis=2)  # [gx, gy, 1, 2]\n",
        "\n",
        "    # YoloV2, YoloV3:\n",
        "    # bx = sigmoid(tx) + Cx\n",
        "    # by = sigmoid(ty) + Cy\n",
        "    #\n",
        "    # for example, if all elements in b_xy are (0.1, 0.2), the result will be\n",
        "    #\n",
        "    # [[[[0.1, 0.2]], [[1.1, 0.2]], [[2.1, 0.2]]],\n",
        "    #  [[[0.1, 1.2]], [[1.1, 1.2]], [[2.1, 1.2]]],\n",
        "    #  [[[0.1, 2.2]], [[1.1, 2.2]], [[2.1, 2.2]]]]\n",
        "    #\n",
        "    b_xy = tf.sigmoid(t_xy) + tf.cast(C_xy, tf.float32)\n",
        "\n",
        "    # finally, divide this absolute box_xy by grid_size, and then we will get the normalized bbox centroids\n",
        "    # for each anchor in each grid cell. b_xy is now in shape (batch_size, grid_size, grid_size, num_anchor, 2)\n",
        "    #\n",
        "    # [[[[0.1/3, 0.2/3]], [[1.1/3, 0.2/3]], [[2.1/3, 0.2/3]]],\n",
        "    #  [[[0.1/3, 1.2/3]], [[1.1/3, 1.2]/3], [[2.1/3, 1.2/3]]],\n",
        "    #  [[[0.1/3, 2.2/3]], [[1.1/3, 2.2/3]], [[2.1/3, 2.2/3]]]]\n",
        "    #\n",
        "    b_xy = b_xy / tf.cast(grid_size, tf.float32)\n",
        "\n",
        "    # YoloV2:\n",
        "    # \"If the cell is offset from the top left corner of the image by (cx , cy)\n",
        "    # and the bounding box prior has width and height pw , ph , then the predictions correspond to: \"\n",
        "    #\n",
        "    # https://github.com/pjreddie/darknet/issues/568#issuecomment-469600294\n",
        "    # \"It’s OK for the predicted box to be wider and/or taller than the original image, but\n",
        "    # it does not make sense for the box to have a negative width or height. That’s why\n",
        "    # we take the exponent of the predicted number.\"\n",
        "    b_wh = tf.exp(t_wh) * valid_anchors_wh\n",
        "\n",
        "    y_box = tf.concat([b_xy, b_wh], axis=-1)\n",
        "    return y_box, objectness, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGS_sS5kg2ao"
      },
      "source": [
        "def get_relative_yolo_box(y_true, valid_anchors_wh):\n",
        "    \"\"\"\n",
        "    This is the inverse of `get_absolute_yolo_box` above. It's turning (bx, by, bw, bh) into\n",
        "    (tx, ty, tw, th) that is relative to cell location.\n",
        "    \"\"\"\n",
        "    grid_size = tf.shape(y_true)[1]\n",
        "    C_xy = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
        "    C_xy = tf.expand_dims(tf.stack(C_xy, axis=-1), axis=2)\n",
        "\n",
        "    b_xy = y_true[..., 0:2]\n",
        "    b_wh = y_true[..., 2:4]\n",
        "    t_xy = b_xy * tf.cast(grid_size, tf.float32) - tf.cast(C_xy, tf.float32)\n",
        "\n",
        "    t_wh = tf.math.log(b_wh / valid_anchors_wh)\n",
        "    # b_wh could have some cells are 0, divided by anchor could result in inf or nan\n",
        "    t_wh = tf.where(\n",
        "        tf.logical_or(tf.math.is_inf(t_wh), tf.math.is_nan(t_wh)),\n",
        "        tf.zeros_like(t_wh), t_wh)\n",
        "\n",
        "    y_box = tf.concat([t_xy, t_wh], axis=-1)\n",
        "    return y_box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qfE6H-Hqfx"
      },
      "source": [
        "#**Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9HulSylgyJp"
      },
      "source": [
        "class YoloLoss(object):\n",
        "    def __init__(self, num_classes, valid_anchors_wh):\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_thresh = 0.5 ###\n",
        "        self.valid_anchors_wh = valid_anchors_wh\n",
        "        self.lambda_coord = 5.0\n",
        "        self.lambda_noobj = 0.5  ###\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        calculate the loss of model prediction for one scale\n",
        "        \"\"\"\n",
        "        # for xy and wh, I seperated them into two groups with different suffix\n",
        "        # suffix rel (relative) means that its coordinates are relative to cells\n",
        "        # basically (tx, ty, tw, th) format from the paper\n",
        "        # _rel is used to calcuate the loss\n",
        "        # suffix abs (absolute) means that its coordinates are absolute with in whole image\n",
        "        # basically (bx, by, bw, bh) format from the paper\n",
        "        # _abs is used to calcuate iou and ignore mask\n",
        "\n",
        "        # split y_pred into xy, wh, objectness and one-hot classes\n",
        "        # pred_xy_rel: (batch, grid, grid, anchor, 2)\n",
        "        # pred_wh_rel: (batch, grid, grid, anchor, 2)\n",
        "        # TODO: Add comment for the sigmoid here\n",
        "        pred_xy_rel = tf.sigmoid(y_pred[..., 0:2])\n",
        "        pred_wh_rel = y_pred[..., 2:4]\n",
        "\n",
        "        # this box is used to calculate iou, NOT loss. so we can't use\n",
        "        # cell offset anymore and have to transform it into true values\n",
        "        # both pred_obj and pred_class has been sigmoid'ed here\n",
        "        # pred_xy_abs: (batch, grid, grid, anchor, 2)\n",
        "        # pred_wh_abs: (batch, grid, grid, anchor, 2)\n",
        "        # pred_obj: (batch, grid, grid, anchor, 1)\n",
        "        # pred_class: (batch, grid, grid, anchor, num_classes)\n",
        "        pred_box_abs, pred_obj, pred_class = get_absolute_yolo_box(\n",
        "            y_pred, self.valid_anchors_wh, self.num_classes)\n",
        "        pred_box_abs = xywh_to_x1y1x2y2(pred_box_abs)\n",
        "\n",
        "        # split y_true into xy, wh, objectness and one-hot classes\n",
        "        # pred_xy_abs: (batch, grid, grid, anchor, 2)\n",
        "        # pred_wh_abs: (batch, grid, grid, anchor, 2)\n",
        "        # pred_obj: (batch, grid, grid, anchor, 1)\n",
        "        # pred_class: (batch, grid, grid, anchor, num_classes)\n",
        "        true_xy_abs, true_wh_abs, true_obj, true_class = tf.split(\n",
        "            y_true, (2, 2, 1, self.num_classes), axis=-1)\n",
        "        true_box_abs = tf.concat([true_xy_abs, true_wh_abs], axis=-1)\n",
        "        true_box_abs = xywh_to_x1y1x2y2(true_box_abs)\n",
        "\n",
        "        # true_box_rel: (batch, grid, grid, anchor, 4)\n",
        "        true_box_rel = get_relative_yolo_box(y_true, self.valid_anchors_wh)\n",
        "        true_xy_rel = true_box_rel[..., 0:2]\n",
        "        true_wh_rel = true_box_rel[..., 2:4]\n",
        "\n",
        "        # some adjustment to improve small box detection, note the (2-truth.w*truth.h) below\n",
        "        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L190\n",
        "        weight = 2 - true_wh_abs[..., 0] * true_wh_abs[..., 1]\n",
        "\n",
        "        # YoloV2:\n",
        "        # \"If the cell is offset from the top left corner of the image by (cx , cy)\n",
        "        # and the bounding box prior has width and height pw , ph , then the predictions correspond to:\"\n",
        "        #\n",
        "        # to calculate the iou and determine the ignore mask, we need to first transform\n",
        "        # prediction into real coordinates (bx, by, bw, bh)\n",
        "\n",
        "        # YoloV2:\n",
        "        # \"This ground truth value can be easily computed by inverting the equations above.\"\n",
        "        #\n",
        "        # to calculate loss and differentiation, we need to transform ground truth into\n",
        "        # cell offset first like demonstrated here:\n",
        "        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L93\n",
        "        xy_loss = self.calc_xy_loss(true_obj, true_xy_rel, pred_xy_rel, weight)\n",
        "        wh_loss = self.calc_wh_loss(true_obj, true_wh_rel, pred_wh_rel, weight)\n",
        "        class_loss = self.calc_class_loss(true_obj, true_class, pred_class)\n",
        "\n",
        "        # use the absolute yolo box to calculate iou and ignore mask\n",
        "        ignore_mask = self.calc_ignore_mask(true_obj, true_box_abs,\n",
        "                                            pred_box_abs)\n",
        "        obj_loss = self.calc_obj_loss(true_obj, pred_obj, ignore_mask)\n",
        "\n",
        "        # YoloV1: Function (3)\n",
        "        return xy_loss + wh_loss + class_loss + obj_loss, (xy_loss, wh_loss,\n",
        "                                                           class_loss,\n",
        "                                                           obj_loss)\n",
        "\n",
        "    def calc_ignore_mask(self, true_obj, true_box, pred_box):\n",
        "        # eg. true_obj (1, 13, 13, 3, 1)\n",
        "        true_obj = tf.squeeze(true_obj, axis=-1)\n",
        "        # eg. true_obj (1, 13, 13, 3)\n",
        "        # eg. true_box (1, 13, 13, 3, 4)\n",
        "        # eg. pred_box (1, 13, 13, 2, 4)\n",
        "        # eg. true_box_filtered (2, 4) it was (3, 4) but one element got filtered out\n",
        "        true_box_filtered = tf.boolean_mask(true_box, tf.cast(\n",
        "            true_obj, tf.bool))\n",
        "\n",
        "        # YOLOv3:\n",
        "        # \"If the bounding box prior is not the best but does overlap a ground\n",
        "        # truth object by more than some threshold we ignore the prediction,\n",
        "        # following [17]. We use the threshold of .5.\"\n",
        "        # calculate the iou for each pair of pred bbox and true bbox, then find the best among them\n",
        "        # eg. best_iou (1, 1, 1, 2)\n",
        "        best_iou = tf.reduce_max(\n",
        "            broadcast_iou(pred_box, true_box_filtered), axis=-1)\n",
        "\n",
        "        # if best iou is higher than threshold, set the box to be ignored for noobj loss\n",
        "        # eg. ignore_mask(1, 1, 1, 2)\n",
        "        ignore_mask = tf.cast(best_iou < self.ignore_thresh, tf.float32)\n",
        "        ignore_mask = tf.expand_dims(ignore_mask, axis=-1)\n",
        "        return ignore_mask\n",
        "\n",
        "    def calc_obj_loss(self, true_obj, pred_obj, ignore_mask):\n",
        "        \"\"\"\n",
        "        calculate loss of objectness: sum of L2 distances\n",
        "        inputs:\n",
        "        true_obj: objectness from ground truth in shape of (batch, grid, grid, anchor, num_classes)\n",
        "        pred_obj: objectness from model prediction in shape of (batch, grid, grid, anchor, num_classes)\n",
        "        outputs:\n",
        "        obj_loss: objectness loss\n",
        "        \"\"\"\n",
        "        obj_entropy = binary_cross_entropy(pred_obj, true_obj)\n",
        "\n",
        "        obj_loss = true_obj * obj_entropy\n",
        "        noobj_loss = (1 - true_obj) * obj_entropy * ignore_mask\n",
        "\n",
        "        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3, 4)) \n",
        "        noobj_loss = tf.reduce_sum(\n",
        "            noobj_loss, axis=(1, 2, 3, 4)) * self.lambda_noobj\n",
        "\n",
        "        return obj_loss + noobj_loss\n",
        "\n",
        "    def calc_class_loss(self, true_obj, true_class, pred_class):\n",
        "        \"\"\"\n",
        "        calculate loss of class prediction\n",
        "        inputs:\n",
        "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
        "        true_class: one-hot class from ground truth in shape of (batch, grid, grid, anchor, num_classes)\n",
        "        pred_class: one-hot class from model prediction in shape of (batch, grid, grid, anchor, num_classes)\n",
        "        outputs:\n",
        "        class_loss: class loss\n",
        "        \"\"\"\n",
        "        # Yolov1:\n",
        "        # \"Note that the loss function only penalizes classiﬁcation error\n",
        "        # if an object is present in that grid cell (hence the conditional\n",
        "        # class probability discussed earlier).\n",
        "        class_loss = binary_cross_entropy(pred_class, true_class)\n",
        "        class_loss = true_obj * class_loss\n",
        "        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3, 4))\n",
        "        return class_loss\n",
        "\n",
        "    def calc_xy_loss(self, true_obj, true_xy, pred_xy, weight):\n",
        "        \"\"\"\n",
        "        calculate loss of the centroid coordinate: sum of L2 distances\n",
        "        inputs:\n",
        "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
        "        true_xy: centroid x and y from ground truth in shape of (batch, grid, grid, anchor, 2)\n",
        "        pred_xy: centroid x and y from model prediction in shape of (batch, grid, grid, anchor, 2)\n",
        "        weight: weight adjustment, reward smaller bounding box\n",
        "        outputs:\n",
        "        xy_loss: centroid loss\n",
        "        \"\"\"\n",
        "        # shape (batch, grid, grid, anchor), eg. (32, 13, 13, 3)\n",
        "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n",
        "\n",
        "        # in order to element-wise multiply the result from tf.reduce_sum\n",
        "        # we need to squeeze one dimension for objectness here\n",
        "        true_obj = tf.squeeze(true_obj, axis=-1)\n",
        "\n",
        "        # YoloV1:\n",
        "        # \"It also only penalizes bounding box coordinate error if that\n",
        "        # predictor is \"responsible\" for the ground truth box (i.e. has the\n",
        "        # highest IOU of any predictor in that grid cell).\"\n",
        "        xy_loss = true_obj * xy_loss * weight\n",
        "\n",
        "        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3)) * self.lambda_coord\n",
        "\n",
        "        return xy_loss\n",
        "\n",
        "    def calc_wh_loss(self, true_obj, true_wh, pred_wh, weight):\n",
        "        \"\"\"\n",
        "        calculate loss of the width and height: sum of L2 distances\n",
        "        inputs:\n",
        "        true_obj: if the object present from ground truth in shape of (batch, grid, grid, anchor, 1)\n",
        "        true_wh: width and height from ground truth in shape of (batch, grid, grid, anchor, 2)\n",
        "        pred_wh: width and height from model prediction in shape of (batch, grid, grid, anchor, 2)\n",
        "        weight: weight adjustment, reward smaller bounding box\n",
        "        outputs:\n",
        "        wh_loss: width and height loss\n",
        "        \"\"\"\n",
        "        # shape (batch, grid, grid, anchor), eg. (32, 13, 13, 3)\n",
        "        wh_loss = tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n",
        "        true_obj = tf.squeeze(true_obj, axis=-1)\n",
        "        wh_loss = true_obj * wh_loss * weight\n",
        "        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3)) * self.lambda_coord\n",
        "        return wh_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXsw8CpUg84j"
      },
      "source": [
        "class Preprocessor(object):\n",
        "    def __init__(self, is_train, num_classes, output_shape=(416, 416)):\n",
        "        self.is_train = is_train\n",
        "        self.num_classes = num_classes\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def __call__(self, example):\n",
        "        features = self.parse_tfexample(example)\n",
        "        encoded = features['image/encoded']\n",
        "        #tf.print(features['image/filename'])\n",
        "        image = tf.io.decode_jpeg(encoded)\n",
        "        \n",
        "\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        classes, bboxes = self.parse_y_features(features)\n",
        "        image         = self.change_intensity(image)\n",
        "        image         = self.change_contrast(image)\n",
        "        #image         = self.add_noise_to_image(image)\n",
        "        \n",
        "        image, bboxes = self.random_flip_image_and_label(image, bboxes)\n",
        "        image, bboxes = self.random_crop_image_and_label(image, bboxes)\n",
        "        \n",
        "        image = tf.image.resize(image, self.output_shape)\n",
        "        image = tf.cast(image, tf.float32) / 127.5 - 1 # 127.5 = 255/2 , map to (-1,1) range \n",
        "\n",
        "        label = (\n",
        "            self.preprocess_label_for_one_scale(classes, bboxes, 26,\n",
        "                                                np.array([0, 1, 2])),\n",
        "            self.preprocess_label_for_one_scale(classes, bboxes, 13,\n",
        "                                                np.array([3, 4, 5])),\n",
        "        )\n",
        "        return image, label\n",
        "\n",
        "    def change_intensity(self, image):\n",
        "        image = tf.cast(image, tf.int32)\n",
        "        r = tf.cast(tf.random.uniform(shape=[1], minval=-50, maxval=50),tf.int32)\n",
        "        image = tf.math.maximum(tf.math.minimum(image + r,255),0) ##adjust brightness\n",
        "        return image\n",
        "    \n",
        "    def change_contrast(self, image):\n",
        "        r = tf.cast(tf.random.uniform(shape=[1], minval=0.7, maxval=1.0),tf.float32)\n",
        "        image = tf.image.adjust_contrast(image, r[0]) ## adjust contrast\n",
        "        return image\n",
        "\n",
        "    def add_noise_to_image(self, image):\n",
        "        r = tf.random.uniform([1])\n",
        "        if r < 0.3:\n",
        "            image = tf.cast(image, tf.float32)\n",
        "            mean = 0\n",
        "            sigma = 100*r # max sigma = 30 \n",
        "            gauss = tf.random.normal(tf.shape(image),mean,sigma)\n",
        "            image = tf.math.maximum(tf.math.minimum(image + gauss,255),0)\n",
        "            image = tf.cast(image, tf.int32)\n",
        "        return image\n",
        "\n",
        "    def random_flip_image_and_label(self, image, bboxes):\n",
        "        \"\"\"\n",
        "        flip left and right for 50% of images\n",
        "        \"\"\"\n",
        "        r = tf.random.uniform([1])\n",
        "        if r < 0.5:\n",
        "            image = tf.image.flip_left_right(image)\n",
        "            xmin, ymin, xmax, ymax = tf.split(bboxes, [1, 1, 1, 1], -1)\n",
        "            # note that we need to switch here\n",
        "            xmin, xmax = 1 - xmax, 1 - xmin\n",
        "            bboxes = tf.squeeze(\n",
        "                tf.stack([xmin, ymin, xmax, ymax], axis=1), axis=-1)\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def get_random_crop_delta(self, bboxes):\n",
        "        \"\"\"\n",
        "        get a random crop which includes all bounding boxes. Since all bboxes here belong to one image,\n",
        "        we can calcualte the minimum of all xmin and ymin, and the maximum of all xmax and ymax to get\n",
        "        the an area that can include all boxes. the crop will be randomly picked between this area boundary and\n",
        "        the boundary of the whole image.\n",
        "        \"\"\"\n",
        "        min_xmin = tf.math.reduce_min(bboxes[..., 0])\n",
        "        min_ymin = tf.math.reduce_min(bboxes[..., 1])\n",
        "        max_xmax = tf.math.reduce_max(bboxes[..., 2])\n",
        "        max_ymax = tf.math.reduce_max(bboxes[..., 3])\n",
        "\n",
        "        # delta is the normalized margin from bboxes boundary the crop boundary\n",
        "        # ____________________________________\n",
        "        # |         ________________         |\n",
        "        # |image    |crop ______   |         |\n",
        "        # |<-DELTA->|     |bbox|   |<-DELTA->|\n",
        "        # |         |     |____|   |         |\n",
        "        # |         |______________|         |\n",
        "        # |__________________________________|\n",
        "        xmin_delta = tf.random.uniform([1], 0, min_xmin)\n",
        "        ymin_delta = tf.random.uniform([1], 0, min_ymin)\n",
        "        xmax_delta = tf.random.uniform([1], 0, 1 - max_xmax)\n",
        "        ymax_delta = tf.random.uniform([1], 0, 1 - max_ymax)\n",
        "\n",
        "        return xmin_delta, ymin_delta, xmax_delta, ymax_delta\n",
        "\n",
        "    def random_crop_image_and_label(self, image, bboxes):\n",
        "        \"\"\"\n",
        "        crop images randomly at 50% chance but preserve all bounding boxes. the crop is guaranteed to include\n",
        "        all bounding boxes. \n",
        "        \"\"\"\n",
        "        \n",
        "        r = tf.random.uniform([1])\n",
        "        if r < 0.5:\n",
        "            xmin_delta, ymin_delta, xmax_delta, ymax_delta = self.get_random_crop_delta(\n",
        "                bboxes)\n",
        "\n",
        "            xmin, ymin, xmax, ymax = tf.split(bboxes, [1, 1, 1, 1], -1)\n",
        "            # before crop: |_0.1_|_0.1_|____________0.5___________|_0.1_|___0.2___|\n",
        "            # after crop:  |_0.1_|____________0.5___________|_0.1_|\n",
        "            # imagine old xmin is 0.2 (0.1+0.1), old xmax is 0.8 (0.1+0.1+0.5+0.1)\n",
        "            # if we cut both left 0.1 (xmin_delta) and right 0.2 (xmax_delta)\n",
        "            # the new xmin will be (0.2 - 0.1) / (1 - 0.1 - 0.2) = 1/7\n",
        "            # the new xmax will be (0.8 - 0.1) / (1 - 0.1 - 0.2) = 6/7\n",
        "            # same thing for y\n",
        "            xmin = (xmin - xmin_delta) / (1 - xmin_delta - xmax_delta)\n",
        "            ymin = (ymin - ymin_delta) / (1 - ymin_delta - ymax_delta)\n",
        "            xmax = (xmax - xmin_delta) / (1 - xmin_delta - xmax_delta)\n",
        "            ymax = (ymax - ymin_delta) / (1 - ymin_delta - ymax_delta)\n",
        "\n",
        "            bboxes = tf.squeeze(\n",
        "                tf.stack([xmin, ymin, xmax, ymax], axis=1), axis=-1)\n",
        "            h = tf.cast(tf.shape(image)[0], dtype=tf.float32)\n",
        "            w = tf.cast(tf.shape(image)[1], dtype=tf.float32)\n",
        "\n",
        "            offset_height = tf.cast(ymin_delta[0] * h, dtype=tf.int32)\n",
        "            offset_width = tf.cast(xmin_delta[0] * w, dtype=tf.int32)\n",
        "            target_height = tf.cast(\n",
        "                tf.math.ceil((1 - ymax_delta - ymin_delta)[0] * h),\n",
        "                dtype=tf.int32)\n",
        "            target_width = tf.cast(\n",
        "                tf.math.ceil((1 - xmax_delta - xmin_delta)[0] * w),\n",
        "                dtype=tf.int32)\n",
        "\n",
        "            image = image[offset_height:offset_height +\n",
        "                          target_height, offset_width:offset_width +\n",
        "                          target_width, :]\n",
        "        return image, bboxes\n",
        "\n",
        "    def parse_y_features(self, features):\n",
        "        classes = tf.sparse.to_dense(features['image/object/class/label'])\n",
        "        classes = tf.one_hot(classes, self.num_classes)\n",
        "\n",
        "        # tf.pad(classes, [[0, 100 - tf.shape(classes)[0]], []], 'CONSTANT')\n",
        "\n",
        "        # bboxes shape (None, 4)\n",
        "        bboxes = tf.stack([\n",
        "            tf.sparse.to_dense(features['image/object/bbox/xmin']),\n",
        "            tf.sparse.to_dense(features['image/object/bbox/ymin']),\n",
        "            tf.sparse.to_dense(features['image/object/bbox/xmax']),\n",
        "            tf.sparse.to_dense(features['image/object/bbox/ymax']),\n",
        "        ],\n",
        "                          axis=1)\n",
        "        return classes, bboxes\n",
        "\n",
        "    def preprocess_label_for_one_scale(self,\n",
        "                                       classes,\n",
        "                                       bboxes,\n",
        "                                       grid_size=13,\n",
        "                                       valid_anchors=None):\n",
        "        \"\"\"\n",
        "        preprocess the class and bounding boxes annotations into model desired format for one scale\n",
        "        (grid, grid, anchor, (centroid x, centroid y, width, height, objectness, ...one-hot classes...))\n",
        "        inputs:\n",
        "        grid_size: a scalar grid size to use\n",
        "        outputs:\n",
        "        y: the desired label format to calcualte loss\n",
        "        \"\"\"\n",
        "        # construct an empty placeholder for the final output y first\n",
        "        y = tf.zeros((grid_size, grid_size, 3, 5 + self.num_classes))\n",
        "\n",
        "        # find the best anchor indices for each ground truth box\n",
        "        anchor_indices = self.find_best_anchor(bboxes)\n",
        "\n",
        "        # necessary assertion, otherwise the steps later would fail\n",
        "        tf.Assert(classes.shape[0] == bboxes.shape[0], [classes])\n",
        "        tf.Assert(anchor_indices.shape[0] == bboxes.shape[0], [anchor_indices])\n",
        "\n",
        "        # this has to be tf.shape instead of classes.shape, otherwise would be None\n",
        "        num_boxes = tf.shape(classes)[0]\n",
        "\n",
        "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
        "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
        "\n",
        "        valid_count = 0\n",
        "        for i in tf.range(num_boxes):\n",
        "            curr_class = tf.cast(classes[i], tf.float32)\n",
        "            curr_box = bboxes[i]\n",
        "            curr_anchor = anchor_indices[i]\n",
        "\n",
        "            # only use the anchor when it belongs to current scale (grid_size)\n",
        "            # for example, when grid size is 13, only anchor 6, 7, 8 (big anchors) are valid\n",
        "            # because the reception field of this grid size is the biggest\n",
        "            # however, if grid size is 52, the finest grained grid, we can only use anchor\n",
        "            # 0, 1, 2 (small anchors)\n",
        "            anchor_found = tf.reduce_any(curr_anchor == valid_anchors)\n",
        "            if anchor_found:\n",
        "                # now that we found the anchor, we need to set it in our final output y\n",
        "                # we only have three anchor boxes in y, so we need to mod by 3 first to get\n",
        "                # adjusted index. eg. anchor 7 will have index 1\n",
        "                # we need to reshape here so that adjusted_anchor_index is a vector\n",
        "                adjusted_anchor_index = tf.math.floormod(curr_anchor, 3)\n",
        "\n",
        "                # we need to turn (xmin, ymin, xmax, ymax) box format into\n",
        "                # (centeroid x, centroid y, width, height) to be able to\n",
        "                # calculate yolo loss later\n",
        "                curr_box_xy = (curr_box[..., 0:2] + curr_box[..., 2:4]) / 2\n",
        "                curr_box_wh = curr_box[..., 2:4] - curr_box[..., 0:2]\n",
        "\n",
        "                # calculate which grid cell should we use\n",
        "                # eg. when curr_box_xy = [0.25, 0.25], and grid size = 26, which is a quarter of the image\n",
        "                # the index of grid cell is floor(0.25 * 26) = 6\n",
        "                grid_cell_xy = tf.cast(\n",
        "                    curr_box_xy // tf.cast((1 / grid_size), dtype=tf.float32),\n",
        "                    tf.int32)\n",
        "\n",
        "                # for this box, we need to update y at location (grid_size, grid_size, adjusted_anchor_index)\n",
        "                # eg. shape in (13, 13, 1)\n",
        "                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n",
        "                # note that it's not grid[x][y]\n",
        "                index = tf.stack(\n",
        "                    [grid_cell_xy[1], grid_cell_xy[0], adjusted_anchor_index])\n",
        "\n",
        "                # this is the value we use to update the above location\n",
        "                # eg. shape in (7)\n",
        "                # note that we need to make this one-hot classes in order to use categorical crossentropy later\n",
        "                update = tf.concat(\n",
        "                    values=[\n",
        "                        curr_box_xy, curr_box_wh,\n",
        "                        tf.constant([1.0]), curr_class\n",
        "                    ],\n",
        "                    axis=0)\n",
        "                # add to final indices and updates to be written into y\n",
        "                indices = indices.write(valid_count, index)\n",
        "                updates = updates.write(valid_count, update)\n",
        "                # tf.print(indices.stack())\n",
        "                # tf.print(updates.stack())\n",
        "                valid_count = 1 + valid_count\n",
        "\n",
        "        y = tf.tensor_scatter_nd_update(y, indices.stack(), updates.stack())\n",
        "        return y\n",
        "\n",
        "    def find_best_anchor(self, y_box):\n",
        "        \"\"\"\n",
        "        find the best anchor for num_boxes ground truth boxes in y_box. Return a tensor in shape\n",
        "        of (num_boxes) that indicates the indices of best anchor for each box\n",
        "        inputs:\n",
        "        y_box: ground truth boxes in shape of (num_boxes, 4)\n",
        "        outputs:\n",
        "        anchor_idx: anchor indices in shape of (num_boxes)\n",
        "        \"\"\"\n",
        "        box_wh = y_box[..., 2:4] - y_box[..., 0:2]\n",
        "\n",
        "        # since box_wh is (num_boxes, 2) and anchor_wh is (9, 2), we need to tile box_wh\n",
        "        # first to match number to anchor in order to apply tf.minimum later\n",
        "        # eg. box_wh -> (2, 9, 2)\n",
        "        box_wh = tf.tile(\n",
        "            tf.expand_dims(box_wh, -2), (1, tf.shape(anchors_wh)[0], 1))\n",
        "\n",
        "        # the intersection here is not calculated based on real coordinates\n",
        "        # but assuming anchor and box share same centroid to help us decide\n",
        "        # which is the best fit anchor for this box\n",
        "        # so we just take the product of minimum width and height as intersection\n",
        "        # eg. intersection -> (2, 9)\n",
        "        intersection = tf.minimum(box_wh[..., 0],\n",
        "                                  anchors_wh[..., 0]) * tf.minimum(\n",
        "                                      box_wh[..., 1], anchors_wh[..., 1])\n",
        "\n",
        "        # box_area is the width*height for each box\n",
        "        # eg box_area -> (2, 9)\n",
        "        box_area = box_wh[..., 0] * box_wh[..., 1]\n",
        "\n",
        "        # anchor area is the width*height for each anchor\n",
        "        # eg anchor_area -> (9)\n",
        "        anchor_area = anchors_wh[..., 0] * anchors_wh[..., 1]\n",
        "\n",
        "        # eg. iou -> (2, 9)\n",
        "        iou = intersection / (box_area + anchor_area - intersection)\n",
        "\n",
        "        # find the best anchor for each box, there should be num_boxes indices\n",
        "        # in the result\n",
        "        # eg. anchor_idx -> (2)\n",
        "        anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.int32)\n",
        "        return anchor_idx\n",
        "\n",
        "    def parse_tfexample(self, example_proto):\n",
        "        image_feature_description = {\n",
        "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
        "            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
        "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "        }\n",
        "        return tf.io.parse_single_example(example_proto,\n",
        "                                          image_feature_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4VeOw8uILeZ"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXTAYsJtJo_-"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 initial_epoch,\n",
        "                 epochs,\n",
        "                 global_batch_size,\n",
        "                 strategy,\n",
        "                 initial_learning_rate=0.01):\n",
        "        self.model = model\n",
        "        self.initial_epoch = initial_epoch\n",
        "        self.epochs = epochs\n",
        "        self.strategy = strategy\n",
        "        self.global_batch_size = global_batch_size\n",
        "        self.loss_objects = [\n",
        "            YoloLoss(\n",
        "                num_classes=TOTAL_CLASSES,\n",
        "                valid_anchors_wh=anchors_wh[0:3]),  # small scale 26x26\n",
        "            YoloLoss(\n",
        "                num_classes=TOTAL_CLASSES,\n",
        "                valid_anchors_wh=anchors_wh[3:6]),  # large scale 13x13\n",
        "        ]\n",
        "        self.optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=initial_learning_rate)\n",
        "\n",
        "        # for learning rate schedule\n",
        "        self.current_learning_rate = initial_learning_rate\n",
        "        self.last_val_loss = math.inf \n",
        "        self.lowest_val_loss = math.inf\n",
        "        ###\n",
        "        self.last_train_loss = math.inf \n",
        "        self.lowest_train_loss = math.inf\n",
        "        ###\n",
        "        self.patience_count = 0\n",
        "        self.max_patience = 10 ##\n",
        "\n",
        "    def lr_decay(self):\n",
        "        \"\"\"\n",
        "        This effectively simulate ReduceOnPlateau learning rate schedule. Learning rate\n",
        "        will be reduced by a factor of 10 if there's no improvement over [max_patience] epochs\n",
        "        \"\"\"\n",
        "        if self.patience_count == self.max_patience:\n",
        "            self.current_learning_rate /= 10.0\n",
        "            self.patience_count = 0\n",
        "        elif self.last_val_loss == self.lowest_val_loss:\n",
        "            self.patience_count = 0\n",
        "        self.patience_count += 1\n",
        "\n",
        "        self.optimizer.learning_rate = self.current_learning_rate\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        images, labels = inputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = self.model(images, training=True)\n",
        "            total_losses = []\n",
        "            xy_losses = []\n",
        "            wh_losses = []\n",
        "            class_losses = []\n",
        "            obj_losses = []\n",
        "            # iterate over all three scales\n",
        "            for loss_object, y_pred, y_true in zip(self.loss_objects, outputs,\n",
        "                                                   labels):\n",
        "                total_loss, loss_breakdown = loss_object(y_true, y_pred)\n",
        "                xy_loss, wh_loss, class_loss, obj_loss = loss_breakdown\n",
        "                total_losses.append(total_loss * (1. / self.global_batch_size))\n",
        "                xy_losses.append(xy_loss * (1. / self.global_batch_size))\n",
        "                wh_losses.append(wh_loss * (1. / self.global_batch_size))\n",
        "                class_losses.append(class_loss * (1. / self.global_batch_size))\n",
        "                obj_losses.append(obj_loss * (1. / self.global_batch_size))\n",
        "            \n",
        "            total_loss = tf.reduce_sum(total_losses)\n",
        "            total_xy_loss = tf.reduce_sum(xy_losses)\n",
        "            total_wh_loss = tf.reduce_sum(wh_losses)\n",
        "            total_class_loss = tf.reduce_sum(class_losses)\n",
        "            total_obj_loss = tf.reduce_sum(obj_losses)\n",
        "\n",
        "\n",
        "        grads = tape.gradient(\n",
        "            target=total_loss, sources=self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        return total_loss, (total_xy_loss, total_wh_loss, total_class_loss,\n",
        "                            total_obj_loss)\n",
        "\n",
        "    def val_step(self, inputs):\n",
        "        images, labels = inputs\n",
        "\n",
        "        outputs = self.model(images, training=False)\n",
        "        losses = []\n",
        "        # iterate over all three scales\n",
        "        for loss_object, y_pred, y_true in zip(self.loss_objects, outputs,\n",
        "                                               labels):\n",
        "            loss, _ = loss_object(y_true, y_pred)\n",
        "            losses.append(loss * (1. / self.global_batch_size))\n",
        "        total_loss = tf.reduce_sum(losses)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def get_current_time(self):\n",
        "        return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    def run(self, train_dist_dataset, val_dist_dataset):\n",
        "        total_steps = tf.constant(0, dtype=tf.int64)\n",
        "\n",
        "        @tf.function\n",
        "        def distributed_train_epoch(dataset, train_summary_writer,\n",
        "                                    total_steps):\n",
        "            total_loss = 0.0\n",
        "            xy_hist = 0.0\n",
        "            wh_hist = 0.0\n",
        "            classes_hist = 0.0\n",
        "            obj_hist = 0.0\n",
        "            num_train_batches = tf.constant(0, dtype=tf.int64)\n",
        "            for one_batch in dataset:\n",
        "                per_replica_losses, per_replica_losses_breakdown = self.strategy.experimental_run_v2(\n",
        "                    self.train_step, args=(one_batch, ))\n",
        "                per_replica_xy_losses, per_replica_wh_losses, per_replica_class_losses, per_replica_obj_losses = per_replica_losses_breakdown\n",
        "                batch_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "                batch_xy_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM,\n",
        "                    per_replica_xy_losses,\n",
        "                    axis=None)\n",
        "                batch_wh_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM,\n",
        "                    per_replica_wh_losses,\n",
        "                    axis=None)\n",
        "                batch_class_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM,\n",
        "                    per_replica_class_losses,\n",
        "                    axis=None)\n",
        "                batch_obj_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM,\n",
        "                    per_replica_obj_losses,\n",
        "                    axis=None)\n",
        "                total_loss += batch_loss\n",
        "                xy_hist += batch_xy_loss\n",
        "                wh_hist += batch_wh_loss\n",
        "                classes_hist += batch_class_loss\n",
        "                obj_hist += batch_obj_loss\n",
        "                num_train_batches += 1\n",
        "                tf.print('Trained batch:', num_train_batches, 'batch loss:',\n",
        "                         batch_loss, 'batch xy loss', batch_xy_loss,\n",
        "                         'batch wh loss', batch_wh_loss, 'batch obj loss',\n",
        "                         batch_obj_loss, 'batch_class_loss', batch_class_loss,\n",
        "                         'epoch total loss:', total_loss)\n",
        "                with train_summary_writer.as_default():\n",
        "                    tf.summary.scalar(\n",
        "                        'batch train loss',\n",
        "                        batch_loss,\n",
        "                        step=total_steps + num_train_batches)\n",
        "                    tf.summary.scalar(\n",
        "                        'batch xy loss',\n",
        "                        batch_xy_loss,\n",
        "                        step=total_steps + num_train_batches)\n",
        "                    tf.summary.scalar(\n",
        "                        'batch wh loss',\n",
        "                        batch_wh_loss,\n",
        "                        step=total_steps + num_train_batches)\n",
        "                    tf.summary.scalar(\n",
        "                        'batch obj loss',\n",
        "                        batch_obj_loss,\n",
        "                        step=total_steps + num_train_batches)\n",
        "                    tf.summary.scalar(\n",
        "                        'batch class loss',\n",
        "                        batch_class_loss,\n",
        "                        step=total_steps + num_train_batches)\n",
        "            return total_loss, num_train_batches, (xy_hist,wh_hist,classes_hist,obj_hist)\n",
        "\n",
        "        @tf.function\n",
        "        def distributed_val_epoch(dataset):\n",
        "            total_loss = 0.0\n",
        "            num_val_batches = tf.constant(0, dtype=tf.int64)\n",
        "            for one_batch in dataset:\n",
        "                per_replica_losses = self.strategy.experimental_run_v2(\n",
        "                    self.val_step, args=(one_batch, ))\n",
        "                batch_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "                total_loss += batch_loss\n",
        "                num_val_batches += 1\n",
        "            return total_loss, num_val_batches\n",
        "\n",
        "        current_time = self.get_current_time()\n",
        "        train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "        val_log_dir = 'logs/gradient_tape/' + current_time + '/val'\n",
        "        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "        val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
        "\n",
        "        tf.print('{} Start training...'.format(current_time))\n",
        "        train_history = []\n",
        "        xy_history = []\n",
        "        wh_history = []\n",
        "        obj_history = []\n",
        "        class_history = []\n",
        "        val_history = []\n",
        "        loss_file = open('/content/gdrive/My Drive/loss.txt','a+')\n",
        "        for epoch in range(self.initial_epoch, self.epochs + 1):\n",
        "            t0 = time.time()\n",
        "            self.lr_decay()\n",
        "\n",
        "            tf.print(\n",
        "                '{} Started epoch {} with learning rate {}. Current LR patience count is {} epochs. Last lowest train loss is {}. Last lowest val loss is {}.'\n",
        "                .format(self.get_current_time(), epoch,\n",
        "                        self.current_learning_rate, self.patience_count,\n",
        "                        self.lowest_train_loss , self.lowest_val_loss))\n",
        "            train_total_loss, num_train_batches, (xyLOSS, whLOSS, classLOSS, objLOSS) = distributed_train_epoch(\n",
        "                train_dist_dataset, train_summary_writer, total_steps)\n",
        "            t1 = time.time()\n",
        "            train_loss = train_total_loss / tf.cast(\n",
        "                num_train_batches, dtype=tf.float32)\n",
        "            xy_loss_hist = xyLOSS / tf.cast(\n",
        "                num_train_batches, dtype=tf.float32)\n",
        "            wh_loss_hist = whLOSS / tf.cast(\n",
        "                num_train_batches, dtype=tf.float32)\n",
        "            class_loss_hist = classLOSS / tf.cast(\n",
        "                num_train_batches, dtype=tf.float32)\n",
        "            obj_loss_hist = objLOSS / tf.cast(\n",
        "                num_train_batches, dtype=tf.float32)\n",
        "            train_history.append(train_loss)\n",
        "            xy_history.append(xy_loss_hist)\n",
        "            wh_history.append(wh_loss_hist)\n",
        "            class_history.append(class_loss_hist)\n",
        "            obj_history.append(obj_loss_hist)\n",
        "            tf.print(\n",
        "                '{} Epoch {} train loss {}, total train batches {}, {} examples per second'\n",
        "                .format(\n",
        "                    self.get_current_time(), epoch, train_loss,\n",
        "                    num_train_batches,\n",
        "                    tf.cast(num_train_batches, dtype=tf.float32) *\n",
        "                    self.global_batch_size / (t1 - t0)))\n",
        "            with train_summary_writer.as_default():\n",
        "                tf.summary.scalar('epoch train loss', train_loss, step=epoch)\n",
        "            total_steps += num_train_batches\n",
        "\n",
        "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
        "                val_dist_dataset)\n",
        "\n",
        "            t2 = time.time()\n",
        "            val_loss = val_total_loss / tf.cast(\n",
        "                num_val_batches, dtype=tf.float32)\n",
        "            val_history.append(val_loss)\n",
        "            loss_line = \"%d , %f , %f , %f , %f , %f , %f\\r\\n\"%(epoch,xy_loss_hist,wh_loss_hist,\n",
        "                                                       class_loss_hist,obj_loss_hist,\n",
        "                                                       train_loss,val_loss)\n",
        "            loss_file.write(loss_line)\n",
        "            tf.print(loss_line)\n",
        "            tf.print(\n",
        "                '{} Epoch {} val loss {}, total val batches {}, {} examples per second'\n",
        "                .format(\n",
        "                    self.get_current_time(), epoch, val_loss, num_val_batches,\n",
        "                    tf.cast(num_val_batches, dtype=tf.float32) *\n",
        "                    self.global_batch_size / (t2 - t1)))\n",
        "            with val_summary_writer.as_default():\n",
        "                tf.summary.scalar('epoch val loss', val_loss, step=epoch)\n",
        "\n",
        "            ##\n",
        "            if train_loss < self.lowest_train_loss:\n",
        "                self.lowest_train_loss = train_loss\n",
        "            self.last_train_loss = train_loss\n",
        "            # save model when reach a new lowest validation loss\n",
        "            if val_loss < self.lowest_val_loss:\n",
        "                self.lowest_val_loss = val_loss\n",
        "                ##\n",
        "                if val_loss < 70.0: # save when loss is less than 70\n",
        "                    self.save_model(epoch, val_loss)\n",
        "            self.last_val_loss = val_loss\n",
        "\n",
        "            ##\n",
        "            if (epoch % 10 == 0): ## save model after every 10 epochs\n",
        "                self.save_model(epoch, val_loss)\n",
        "\n",
        "        self.save_model(self.epochs, self.last_val_loss)\n",
        "        print('{} Finished.'.format(self.get_current_time()))\n",
        "        return train_history, xy_history, wh_history, class_history, obj_history, val_history\n",
        "\n",
        "\n",
        "    def save_model(self, epoch, loss):\n",
        "        # https://github.com/tensorflow/tensorflow/issues/33565\n",
        "        #model_name = './models/model-v1.0.1-epoch-{}-loss-{:.4f}.tf'.format(epoch, loss)\n",
        "        model_name = '/content/gdrive/My Drive/results/epoch-{}-loss-{:.3f}.h5'.format(epoch, loss)\n",
        "        #self.model.save_weights(model_name)\n",
        "        self.model.save(model_name)\n",
        "        print(\"Model {} saved.\".format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPsBTaInhOYr"
      },
      "source": [
        "def create_dataset(tfrecords, batch_size, is_train):\n",
        "    preprocess = Preprocessor(is_train, TOTAL_CLASSES, OUTPUT_SHAPE)\n",
        "\n",
        "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
        "    dataset = tf.data.TFRecordDataset(dataset)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(512)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9dyauPSgsmX"
      },
      "source": [
        "# predefined anchors:\n",
        "anchors_wh = np.array([ [10,14],  [23,27],  [37,58] , [81,82],  [135,169],  [344,319] ] ,\n",
        "                      np.float32) / 416"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQVq5CCZhTNg"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "TOTAL_CLASSES = 6\n",
        "TOTAL_EPOCHS = 10\n",
        "OUTPUT_SHAPE = (416, 416)\n",
        "TF_RECORDS = '/content/gdrive/My Drive/tfrecords_2014'\n",
        "start_LR = 0.01 # starting learning rate\n",
        "tf.random.set_seed(1)\n",
        "first_time = True\n",
        "pre_weights = '/content/gdrive/My Drive/result/epoch-20-loss-22.862.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ZsFZxhhVPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5262a0b9-c77d-4796-d206-fbb4f50e0802"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "global_batch_size = strategy.num_replicas_in_sync * BATCH_SIZE\n",
        "train_dataset = create_dataset('{}/train*'.format(TF_RECORDS), global_batch_size, is_train=True)\n",
        "#train_dataset = create_dataset('{}/train/train*'.format(TF_RECORDS), global_batch_size, is_train=True)\n",
        "val_dataset = create_dataset('{}/val*'.format(TF_RECORDS), global_batch_size, is_train=False)\n",
        "#val_dataset = create_dataset('{}/val/val*'.format(TF_RECORDS), global_batch_size, is_train=False)\n",
        "if not os.path.exists(os.path.join('./models')):\n",
        "    os.makedirs(os.path.join('./models/'))\n",
        " \n",
        "with strategy.scope():\n",
        " \n",
        "  train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "  val_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)\n",
        "  #tiny_model = TL_tiny_model(TOTAL_CLASSES)\n",
        "  tiny_model = our_tiny_model(TOTAL_CLASSES, first_time)\n",
        "  #model.summary()\n",
        "  if not(first_time):\n",
        "      tiny_model.load_weights(pre_weights)\n",
        "  initial_epoch = 1  \n",
        "  trainer = Trainer(\n",
        "            model=tiny_model,\n",
        "            initial_epoch=initial_epoch,\n",
        "            epochs=TOTAL_EPOCHS,\n",
        "            global_batch_size=global_batch_size,\n",
        "            strategy=strategy,\n",
        "            initial_learning_rate = start_LR\n",
        "        )\n",
        "  \n",
        "  train_history, xy_history, wh_history, class_history, obj_history, val_history = trainer.run(train_dist_dataset, val_dist_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "20201008-153737 Start training...\n",
            "20201008-153737 Started epoch 1 with learning rate 0.01. Current LR patience count is 1 epochs. Last lowest train loss is inf. Last lowest val loss is inf.\n",
            "Trained batch: 1 batch loss: 919.356812 batch xy loss 11.7694225 batch wh loss 139.037415 batch obj loss 753.231628 batch_class_loss 15.3183699 epoch total loss: 919.356812\n",
            "Trained batch: 2 batch loss: 14724.9824 batch xy loss 13.2165489 batch wh loss 13541.1143 batch obj loss 1155.82227 batch_class_loss 14.8286819 epoch total loss: 15644.3389\n",
            "Trained batch: 3 batch loss: 1617.07422 batch xy loss 9.59143066 batch wh loss 905.828369 batch obj loss 687.465393 batch_class_loss 14.1890011 epoch total loss: 17261.4141\n",
            "Trained batch: 4 batch loss: 784.943176 batch xy loss 7.87267303 batch wh loss 355.545227 batch obj loss 406.75412 batch_class_loss 14.7711515 epoch total loss: 18046.3574\n",
            "Trained batch: 5 batch loss: 532.567749 batch xy loss 10.0769749 batch wh loss 263.034576 batch obj loss 240.104767 batch_class_loss 19.3514328 epoch total loss: 18578.9258\n",
            "Trained batch: 6 batch loss: 322.329712 batch xy loss 10.0112476 batch wh loss 156.407532 batch obj loss 135.687561 batch_class_loss 20.2233772 epoch total loss: 18901.2559\n",
            "Trained batch: 7 batch loss: 238.800842 batch xy loss 8.82129192 batch wh loss 121.400551 batch obj loss 89.6186905 batch_class_loss 18.960289 epoch total loss: 19140.0566\n",
            "Trained batch: 8 batch loss: 260.048828 batch xy loss 8.723382 batch wh loss 162.610138 batch obj loss 69.6298065 batch_class_loss 19.0855083 epoch total loss: 19400.1055\n",
            "Trained batch: 9 batch loss: 233.436478 batch xy loss 9.57973385 batch wh loss 142.691971 batch obj loss 57.7444458 batch_class_loss 23.4203224 epoch total loss: 19633.541\n",
            "Trained batch: 10 batch loss: 272.261963 batch xy loss 13.1341267 batch wh loss 163.226395 batch obj loss 60.9188576 batch_class_loss 34.9825821 epoch total loss: 19905.8027\n",
            "Trained batch: 11 batch loss: 190.938293 batch xy loss 8.43694305 batch wh loss 110.662521 batch obj loss 48.3740387 batch_class_loss 23.464798 epoch total loss: 20096.7402\n",
            "Trained batch: 12 batch loss: 193.890305 batch xy loss 11.1712837 batch wh loss 101.123993 batch obj loss 49.8302841 batch_class_loss 31.7647495 epoch total loss: 20290.6309\n",
            "Trained batch: 13 batch loss: 144.329971 batch xy loss 10.0527153 batch wh loss 62.7974396 batch obj loss 44.421627 batch_class_loss 27.0582 epoch total loss: 20434.9609\n",
            "Trained batch: 14 batch loss: 133.79097 batch xy loss 9.34755611 batch wh loss 61.408 batch obj loss 39.5786858 batch_class_loss 23.4567299 epoch total loss: 20568.752\n",
            "Trained batch: 15 batch loss: 138.939407 batch xy loss 10.3755178 batch wh loss 56.1739197 batch obj loss 45.3282928 batch_class_loss 27.0616817 epoch total loss: 20707.6914\n",
            "Trained batch: 16 batch loss: 131.652176 batch xy loss 8.62098598 batch wh loss 59.9664154 batch obj loss 38.9967728 batch_class_loss 24.0680084 epoch total loss: 20839.3438\n",
            "Trained batch: 17 batch loss: 145.677 batch xy loss 10.534565 batch wh loss 62.7122154 batch obj loss 43.955574 batch_class_loss 28.4746532 epoch total loss: 20985.0215\n",
            "Trained batch: 18 batch loss: 119.439598 batch xy loss 8.92900181 batch wh loss 48.7982712 batch obj loss 36.9674 batch_class_loss 24.7449207 epoch total loss: 21104.4609\n",
            "Trained batch: 19 batch loss: 100.502739 batch xy loss 7.59604311 batch wh loss 33.4497528 batch obj loss 36.8444939 batch_class_loss 22.6124516 epoch total loss: 21204.9629\n",
            "Trained batch: 20 batch loss: 136.980728 batch xy loss 11.503582 batch wh loss 48.6029434 batch obj loss 45.4506454 batch_class_loss 31.4235516 epoch total loss: 21341.9434\n",
            "Trained batch: 21 batch loss: 118.397827 batch xy loss 9.46061516 batch wh loss 40.1142235 batch obj loss 40.7989311 batch_class_loss 28.0240688 epoch total loss: 21460.3418\n",
            "Trained batch: 22 batch loss: 91.6784363 batch xy loss 6.84335804 batch wh loss 32.3656425 batch obj loss 32.1384468 batch_class_loss 20.3309879 epoch total loss: 21552.0195\n",
            "Trained batch: 23 batch loss: 113.142822 batch xy loss 8.4143095 batch wh loss 43.9198189 batch obj loss 34.6941 batch_class_loss 26.1145935 epoch total loss: 21665.1621\n",
            "Trained batch: 24 batch loss: 91.6663055 batch xy loss 7.4028616 batch wh loss 31.5908318 batch obj loss 31.4723969 batch_class_loss 21.2002144 epoch total loss: 21756.8281\n",
            "Trained batch: 25 batch loss: 83.0951233 batch xy loss 6.73340511 batch wh loss 24.2289162 batch obj loss 31.5849 batch_class_loss 20.5478954 epoch total loss: 21839.9238\n",
            "Trained batch: 26 batch loss: 98.937973 batch xy loss 8.46391487 batch wh loss 30.8732948 batch obj loss 33.9845924 batch_class_loss 25.6161766 epoch total loss: 21938.8613\n",
            "Trained batch: 27 batch loss: 94.0806656 batch xy loss 7.69423866 batch wh loss 31.0026894 batch obj loss 32.3329 batch_class_loss 23.0508366 epoch total loss: 22032.9414\n",
            "Trained batch: 28 batch loss: 107.445618 batch xy loss 10.0491562 batch wh loss 33.5126534 batch obj loss 36.3177643 batch_class_loss 27.5660477 epoch total loss: 22140.3867\n",
            "Trained batch: 29 batch loss: 96.8849487 batch xy loss 7.91032 batch wh loss 34.1591644 batch obj loss 32.4534454 batch_class_loss 22.3620148 epoch total loss: 22237.2715\n",
            "Trained batch: 30 batch loss: 86.3076935 batch xy loss 8.31012 batch wh loss 24.0281544 batch obj loss 31.2677803 batch_class_loss 22.7016258 epoch total loss: 22323.5801\n",
            "Trained batch: 31 batch loss: 87.4675598 batch xy loss 7.58707714 batch wh loss 23.5670414 batch obj loss 32.0622482 batch_class_loss 24.251194 epoch total loss: 22411.0469\n",
            "Trained batch: 32 batch loss: 79.2682877 batch xy loss 7.59892845 batch wh loss 23.7277603 batch obj loss 27.2870522 batch_class_loss 20.654541 epoch total loss: 22490.3145\n",
            "Trained batch: 33 batch loss: 74.5991669 batch xy loss 7.48003149 batch wh loss 17.25741 batch obj loss 29.1770458 batch_class_loss 20.6846771 epoch total loss: 22564.9141\n",
            "Trained batch: 34 batch loss: 94.6723 batch xy loss 9.07968807 batch wh loss 22.548933 batch obj loss 35.900898 batch_class_loss 27.1427879 epoch total loss: 22659.5859\n",
            "Trained batch: 35 batch loss: 87.6805878 batch xy loss 8.86655903 batch wh loss 23.3520584 batch obj loss 32.0400543 batch_class_loss 23.4219131 epoch total loss: 22747.2656\n",
            "Trained batch: 36 batch loss: 92.6700287 batch xy loss 9.87260151 batch wh loss 23.4945011 batch obj loss 34.0436592 batch_class_loss 25.2592678 epoch total loss: 22839.9355\n",
            "Trained batch: 37 batch loss: 80.5936203 batch xy loss 8.61151791 batch wh loss 17.9155293 batch obj loss 30.9639015 batch_class_loss 23.1026783 epoch total loss: 22920.5293\n",
            "Trained batch: 38 batch loss: 94.6510468 batch xy loss 8.49639893 batch wh loss 27.3132725 batch obj loss 32.7249184 batch_class_loss 26.1164513 epoch total loss: 23015.1797\n",
            "Trained batch: 39 batch loss: 89.7940674 batch xy loss 8.19568825 batch wh loss 23.8171272 batch obj loss 32.4210129 batch_class_loss 25.3602505 epoch total loss: 23104.9746\n",
            "Trained batch: 40 batch loss: 75.9453354 batch xy loss 7.41931534 batch wh loss 18.9474831 batch obj loss 28.9078732 batch_class_loss 20.6706638 epoch total loss: 23180.9199\n",
            "Trained batch: 41 batch loss: 87.3795471 batch xy loss 9.62254 batch wh loss 19.2987137 batch obj loss 33.6521721 batch_class_loss 24.8061295 epoch total loss: 23268.2988\n",
            "Trained batch: 42 batch loss: 72.6895142 batch xy loss 7.57878208 batch wh loss 20.1145058 batch obj loss 26.706522 batch_class_loss 18.2897072 epoch total loss: 23340.9883\n",
            "Trained batch: 43 batch loss: 88.0794678 batch xy loss 9.04723 batch wh loss 21.6433277 batch obj loss 33.3627 batch_class_loss 24.0262127 epoch total loss: 23429.0684\n",
            "Trained batch: 44 batch loss: 85.2300568 batch xy loss 9.339221 batch wh loss 21.0084267 batch obj loss 31.7362061 batch_class_loss 23.1461983 epoch total loss: 23514.2988\n",
            "Trained batch: 45 batch loss: 99.5913391 batch xy loss 9.73181343 batch wh loss 27.7081528 batch obj loss 35.3632812 batch_class_loss 26.7880821 epoch total loss: 23613.8906\n",
            "Trained batch: 46 batch loss: 78.2447357 batch xy loss 8.65788937 batch wh loss 21.7469883 batch obj loss 27.5543499 batch_class_loss 20.2855053 epoch total loss: 23692.1348\n",
            "Trained batch: 47 batch loss: 73.7204361 batch xy loss 7.89465857 batch wh loss 19.8347397 batch obj loss 27.7701626 batch_class_loss 18.220871 epoch total loss: 23765.8555\n",
            "Trained batch: 48 batch loss: 81.3566132 batch xy loss 9.12988758 batch wh loss 17.3163719 batch obj loss 32.1639481 batch_class_loss 22.7464104 epoch total loss: 23847.2129\n",
            "Trained batch: 49 batch loss: 86.8737106 batch xy loss 8.88974285 batch wh loss 20.614851 batch obj loss 32.0159378 batch_class_loss 25.3531857 epoch total loss: 23934.0859\n",
            "Trained batch: 50 batch loss: 67.36586 batch xy loss 6.91329241 batch wh loss 16.3486481 batch obj loss 26.6357651 batch_class_loss 17.468153 epoch total loss: 24001.4512\n",
            "Trained batch: 51 batch loss: 74.9277191 batch xy loss 8.782897 batch wh loss 15.538991 batch obj loss 29.9460697 batch_class_loss 20.6597519 epoch total loss: 24076.3789\n",
            "Trained batch: 52 batch loss: 63.4187851 batch xy loss 7.22228432 batch wh loss 12.0398731 batch obj loss 26.7671967 batch_class_loss 17.3894348 epoch total loss: 24139.7969\n",
            "Trained batch: 53 batch loss: 63.6151199 batch xy loss 6.77169037 batch wh loss 14.1682501 batch obj loss 25.1305714 batch_class_loss 17.5446072 epoch total loss: 24203.4121\n",
            "Trained batch: 54 batch loss: 69.9545441 batch xy loss 8.32942 batch wh loss 14.9785013 batch obj loss 27.5059757 batch_class_loss 19.1406422 epoch total loss: 24273.3672\n",
            "Trained batch: 55 batch loss: 73.0146561 batch xy loss 7.45817184 batch wh loss 18.634861 batch obj loss 27.9796124 batch_class_loss 18.9420109 epoch total loss: 24346.3828\n",
            "Trained batch: 56 batch loss: 79.0182877 batch xy loss 8.99301338 batch wh loss 15.3148136 batch obj loss 32.1306305 batch_class_loss 22.5798302 epoch total loss: 24425.4\n",
            "Trained batch: 57 batch loss: 69.6587906 batch xy loss 8.58363 batch wh loss 15.3030815 batch obj loss 26.883007 batch_class_loss 18.8890686 epoch total loss: 24495.0586\n",
            "Trained batch: 58 batch loss: 62.6418457 batch xy loss 7.92353582 batch wh loss 13.2551327 batch obj loss 25.1766281 batch_class_loss 16.2865429 epoch total loss: 24557.7012\n",
            "Trained batch: 59 batch loss: 64.3870392 batch xy loss 7.5945425 batch wh loss 13.2550354 batch obj loss 26.1479549 batch_class_loss 17.3895111 epoch total loss: 24622.0879\n",
            "Trained batch: 60 batch loss: 62.1401749 batch xy loss 6.46930361 batch wh loss 13.1809444 batch obj loss 26.3323631 batch_class_loss 16.1575565 epoch total loss: 24684.2285\n",
            "Trained batch: 61 batch loss: 60.6768723 batch xy loss 7.15639305 batch wh loss 12.5259914 batch obj loss 23.9181862 batch_class_loss 17.0763 epoch total loss: 24744.9062\n",
            "Trained batch: 62 batch loss: 71.7103729 batch xy loss 8.45597935 batch wh loss 15.2664547 batch obj loss 27.775013 batch_class_loss 20.212925 epoch total loss: 24816.6172\n",
            "Trained batch: 63 batch loss: 79.1373291 batch xy loss 9.02583694 batch wh loss 18.7107143 batch obj loss 29.4855442 batch_class_loss 21.9152317 epoch total loss: 24895.7539\n",
            "Trained batch: 64 batch loss: 71.9492722 batch xy loss 7.60337067 batch wh loss 17.8249283 batch obj loss 26.5115089 batch_class_loss 20.0094719 epoch total loss: 24967.7031\n",
            "Trained batch: 65 batch loss: 78.3792801 batch xy loss 8.88648415 batch wh loss 19.4678879 batch obj loss 29.7393684 batch_class_loss 20.2855358 epoch total loss: 25046.082\n",
            "Trained batch: 66 batch loss: 69.1487 batch xy loss 8.1894989 batch wh loss 13.4423513 batch obj loss 27.1286106 batch_class_loss 20.3882351 epoch total loss: 25115.2305\n",
            "Trained batch: 67 batch loss: 70.3025436 batch xy loss 9.56898594 batch wh loss 15.9378071 batch obj loss 25.6751194 batch_class_loss 19.120636 epoch total loss: 25185.5332\n",
            "Trained batch: 68 batch loss: 70.4324799 batch xy loss 8.49045753 batch wh loss 13.7962484 batch obj loss 27.6910172 batch_class_loss 20.4547501 epoch total loss: 25255.9648\n",
            "Trained batch: 69 batch loss: 85.4944153 batch xy loss 9.98106861 batch wh loss 17.9094067 batch obj loss 32.7405281 batch_class_loss 24.8634033 epoch total loss: 25341.459\n",
            "Trained batch: 70 batch loss: 65.3262939 batch xy loss 7.31603336 batch wh loss 13.3886595 batch obj loss 25.5637836 batch_class_loss 19.0578213 epoch total loss: 25406.7852\n",
            "Trained batch: 71 batch loss: 59.5504608 batch xy loss 6.92836714 batch wh loss 11.2919693 batch obj loss 24.4526653 batch_class_loss 16.8774586 epoch total loss: 25466.3359\n",
            "Trained batch: 72 batch loss: 75.4508209 batch xy loss 8.25417137 batch wh loss 19.8933411 batch obj loss 27.308485 batch_class_loss 19.9948235 epoch total loss: 25541.7871\n",
            "Trained batch: 73 batch loss: 64.8380127 batch xy loss 7.33256626 batch wh loss 14.3549986 batch obj loss 26.0853 batch_class_loss 17.0651512 epoch total loss: 25606.625\n",
            "Trained batch: 74 batch loss: 57.6051369 batch xy loss 7.50085354 batch wh loss 13.0056419 batch obj loss 22.487608 batch_class_loss 14.6110334 epoch total loss: 25664.2305\n",
            "Trained batch: 75 batch loss: 66.9406815 batch xy loss 8.89113426 batch wh loss 13.2615309 batch obj loss 26.9603119 batch_class_loss 17.8277 epoch total loss: 25731.1719\n",
            "Trained batch: 76 batch loss: 59.6362839 batch xy loss 7.62950373 batch wh loss 10.3592834 batch obj loss 24.165556 batch_class_loss 17.4819431 epoch total loss: 25790.8086\n",
            "Trained batch: 77 batch loss: 69.7912827 batch xy loss 8.44932938 batch wh loss 14.5388432 batch obj loss 28.1499596 batch_class_loss 18.6531487 epoch total loss: 25860.6\n",
            "Trained batch: 78 batch loss: 64.0511 batch xy loss 7.30202579 batch wh loss 13.5912476 batch obj loss 24.8524094 batch_class_loss 18.3054199 epoch total loss: 25924.6504\n",
            "Trained batch: 79 batch loss: 69.3954086 batch xy loss 9.16585 batch wh loss 12.479022 batch obj loss 27.6228256 batch_class_loss 20.1277122 epoch total loss: 25994.0449\n",
            "Trained batch: 80 batch loss: 68.5863953 batch xy loss 8.13178253 batch wh loss 12.3280849 batch obj loss 27.4947052 batch_class_loss 20.6318188 epoch total loss: 26062.6309\n",
            "Trained batch: 81 batch loss: 67.3146439 batch xy loss 7.43029928 batch wh loss 13.3814249 batch obj loss 27.0400772 batch_class_loss 19.462841 epoch total loss: 26129.9453\n",
            "Trained batch: 82 batch loss: 62.8384552 batch xy loss 8.3839283 batch wh loss 12.9495296 batch obj loss 24.2970181 batch_class_loss 17.2079811 epoch total loss: 26192.7832\n",
            "Trained batch: 83 batch loss: 62.8127213 batch xy loss 7.26060438 batch wh loss 14.0234299 batch obj loss 24.7018642 batch_class_loss 16.8268223 epoch total loss: 26255.5957\n",
            "Trained batch: 84 batch loss: 69.7626343 batch xy loss 8.95902824 batch wh loss 14.6661911 batch obj loss 26.6558857 batch_class_loss 19.4815273 epoch total loss: 26325.3574\n",
            "Trained batch: 85 batch loss: 67.1084213 batch xy loss 7.43571663 batch wh loss 15.0388317 batch obj loss 25.8090019 batch_class_loss 18.8248692 epoch total loss: 26392.4668\n",
            "Trained batch: 86 batch loss: 61.4575958 batch xy loss 7.55262756 batch wh loss 12.6107368 batch obj loss 24.380825 batch_class_loss 16.9134102 epoch total loss: 26453.9238\n",
            "Trained batch: 87 batch loss: 67.1520538 batch xy loss 7.87677908 batch wh loss 14.9475527 batch obj loss 25.9624615 batch_class_loss 18.3652554 epoch total loss: 26521.0762\n",
            "Trained batch: 88 batch loss: 76.3133 batch xy loss 9.42305 batch wh loss 17.3735886 batch obj loss 28.0945358 batch_class_loss 21.422123 epoch total loss: 26597.3887\n",
            "Trained batch: 89 batch loss: 56.6770325 batch xy loss 7.29917955 batch wh loss 10.7841301 batch obj loss 22.7320194 batch_class_loss 15.8617039 epoch total loss: 26654.0664\n",
            "Trained batch: 90 batch loss: 60.3457718 batch xy loss 7.03101254 batch wh loss 13.2682171 batch obj loss 23.7818298 batch_class_loss 16.2647133 epoch total loss: 26714.4121\n",
            "Trained batch: 91 batch loss: 65.9266052 batch xy loss 8.98703671 batch wh loss 12.574132 batch obj loss 25.8365326 batch_class_loss 18.5289078 epoch total loss: 26780.3379\n",
            "Trained batch: 92 batch loss: 71.9838486 batch xy loss 9.25705338 batch wh loss 15.8978424 batch obj loss 27.3925323 batch_class_loss 19.4364204 epoch total loss: 26852.3223\n",
            "Trained batch: 93 batch loss: 56.7860947 batch xy loss 7.37001324 batch wh loss 11.0660143 batch obj loss 22.4917278 batch_class_loss 15.8583364 epoch total loss: 26909.1074\n",
            "Trained batch: 94 batch loss: 67.911232 batch xy loss 8.47201252 batch wh loss 14.1048326 batch obj loss 27.1522961 batch_class_loss 18.1820927 epoch total loss: 26977.0195\n",
            "Trained batch: 95 batch loss: 57.9400024 batch xy loss 7.21274471 batch wh loss 12.518878 batch obj loss 22.4711723 batch_class_loss 15.7372074 epoch total loss: 27034.959\n",
            "Trained batch: 96 batch loss: 56.762291 batch xy loss 7.34820271 batch wh loss 11.4426107 batch obj loss 22.9872303 batch_class_loss 14.9842424 epoch total loss: 27091.7207\n",
            "Trained batch: 97 batch loss: 54.057 batch xy loss 6.93829489 batch wh loss 10.014636 batch obj loss 22.1186314 batch_class_loss 14.9854355 epoch total loss: 27145.7773\n",
            "Trained batch: 98 batch loss: 56.0866241 batch xy loss 7.38657331 batch wh loss 11.5935307 batch obj loss 22.0572186 batch_class_loss 15.0492992 epoch total loss: 27201.8633\n",
            "Trained batch: 99 batch loss: 60.2056732 batch xy loss 7.63770962 batch wh loss 13.1615028 batch obj loss 23.7327728 batch_class_loss 15.6736832 epoch total loss: 27262.0684\n",
            "Trained batch: 100 batch loss: 63.3252831 batch xy loss 7.45982742 batch wh loss 14.8807011 batch obj loss 24.4069805 batch_class_loss 16.577776 epoch total loss: 27325.3945\n",
            "Trained batch: 101 batch loss: 66.6803513 batch xy loss 8.67479229 batch wh loss 15.8105946 batch obj loss 24.3384075 batch_class_loss 17.8565636 epoch total loss: 27392.0742\n",
            "Trained batch: 102 batch loss: 49.4814911 batch xy loss 6.79305649 batch wh loss 9.5826931 batch obj loss 19.9625816 batch_class_loss 13.1431599 epoch total loss: 27441.5566\n",
            "Trained batch: 103 batch loss: 45.4496 batch xy loss 5.57399368 batch wh loss 8.98638 batch obj loss 18.9899158 batch_class_loss 11.8993149 epoch total loss: 27487.0059\n",
            "Trained batch: 104 batch loss: 55.0755234 batch xy loss 6.31180048 batch wh loss 10.0337343 batch obj loss 23.6617184 batch_class_loss 15.0682735 epoch total loss: 27542.082\n",
            "Trained batch: 105 batch loss: 56.0787735 batch xy loss 7.43142128 batch wh loss 10.8305101 batch obj loss 22.6279755 batch_class_loss 15.1888704 epoch total loss: 27598.1602\n",
            "Trained batch: 106 batch loss: 67.8502502 batch xy loss 9.08131599 batch wh loss 13.677352 batch obj loss 25.9401321 batch_class_loss 19.1514492 epoch total loss: 27666.0098\n",
            "Trained batch: 107 batch loss: 55.3037033 batch xy loss 7.03863859 batch wh loss 9.53906822 batch obj loss 23.1119289 batch_class_loss 15.6140709 epoch total loss: 27721.3125\n",
            "Trained batch: 108 batch loss: 72.20578 batch xy loss 9.90445423 batch wh loss 15.4714699 batch obj loss 26.6161785 batch_class_loss 20.2136841 epoch total loss: 27793.5176\n",
            "Trained batch: 109 batch loss: 68.2539215 batch xy loss 9.30916405 batch wh loss 13.6582994 batch obj loss 26.1098938 batch_class_loss 19.1765671 epoch total loss: 27861.7715\n",
            "Trained batch: 110 batch loss: 67.1715851 batch xy loss 9.02787876 batch wh loss 13.6319342 batch obj loss 25.8924484 batch_class_loss 18.6193199 epoch total loss: 27928.9434\n",
            "Trained batch: 111 batch loss: 69.4355 batch xy loss 8.28700733 batch wh loss 14.8841333 batch obj loss 26.4684734 batch_class_loss 19.7958851 epoch total loss: 27998.3789\n",
            "Trained batch: 112 batch loss: 64.1138153 batch xy loss 8.41561699 batch wh loss 13.9136333 batch obj loss 24.4151878 batch_class_loss 17.3693848 epoch total loss: 28062.4922\n",
            "Trained batch: 113 batch loss: 71.3782654 batch xy loss 9.53779411 batch wh loss 15.9394741 batch obj loss 26.2536201 batch_class_loss 19.647377 epoch total loss: 28133.8711\n",
            "Trained batch: 114 batch loss: 64.437 batch xy loss 8.66034126 batch wh loss 16.5812073 batch obj loss 23.3698235 batch_class_loss 15.8256292 epoch total loss: 28198.3086\n",
            "Trained batch: 115 batch loss: 61.8229218 batch xy loss 7.94679546 batch wh loss 13.4530373 batch obj loss 23.7273 batch_class_loss 16.6957893 epoch total loss: 28260.1309\n",
            "Trained batch: 116 batch loss: 54.8300858 batch xy loss 7.65465736 batch wh loss 9.96182919 batch obj loss 22.2006626 batch_class_loss 15.0129337 epoch total loss: 28314.9609\n",
            "Trained batch: 117 batch loss: 62.8429489 batch xy loss 8.37546062 batch wh loss 15.5620575 batch obj loss 23.1935272 batch_class_loss 15.7119083 epoch total loss: 28377.8047\n",
            "Trained batch: 118 batch loss: 61.1448364 batch xy loss 7.75745487 batch wh loss 12.2796059 batch obj loss 24.7520351 batch_class_loss 16.3557377 epoch total loss: 28438.9492\n",
            "Trained batch: 119 batch loss: 44.6075 batch xy loss 5.71904516 batch wh loss 7.3412075 batch obj loss 20.0764866 batch_class_loss 11.4707584 epoch total loss: 28483.5566\n",
            "Trained batch: 120 batch loss: 58.5855293 batch xy loss 7.39834404 batch wh loss 12.1584845 batch obj loss 23.3706608 batch_class_loss 15.6580448 epoch total loss: 28542.1426\n",
            "Trained batch: 121 batch loss: 64.2759323 batch xy loss 8.26314 batch wh loss 15.9012032 batch obj loss 24.0116425 batch_class_loss 16.0999451 epoch total loss: 28606.418\n",
            "Trained batch: 122 batch loss: 71.8056946 batch xy loss 9.19942379 batch wh loss 13.9935093 batch obj loss 28.6749344 batch_class_loss 19.9378262 epoch total loss: 28678.2246\n",
            "Trained batch: 123 batch loss: 59.1523857 batch xy loss 7.74314547 batch wh loss 11.6033421 batch obj loss 23.4109554 batch_class_loss 16.3949432 epoch total loss: 28737.377\n",
            "Trained batch: 124 batch loss: 48.5611954 batch xy loss 5.99738598 batch wh loss 11.5934963 batch obj loss 18.8104877 batch_class_loss 12.1598263 epoch total loss: 28785.9375\n",
            "Trained batch: 125 batch loss: 48.9385452 batch xy loss 5.93888092 batch wh loss 10.5887613 batch obj loss 19.9047318 batch_class_loss 12.5061722 epoch total loss: 28834.877\n",
            "Trained batch: 126 batch loss: 52.2974663 batch xy loss 6.50967741 batch wh loss 10.2749424 batch obj loss 20.726059 batch_class_loss 14.786787 epoch total loss: 28887.1738\n",
            "Trained batch: 127 batch loss: 42.6625214 batch xy loss 5.30441332 batch wh loss 8.36382389 batch obj loss 17.3873405 batch_class_loss 11.606946 epoch total loss: 28929.8359\n",
            "Trained batch: 128 batch loss: 48.786293 batch xy loss 6.59211111 batch wh loss 9.6675415 batch obj loss 19.4818573 batch_class_loss 13.0447826 epoch total loss: 28978.623\n",
            "Trained batch: 129 batch loss: 52.2236786 batch xy loss 6.93280792 batch wh loss 9.75312901 batch obj loss 20.7380447 batch_class_loss 14.7997007 epoch total loss: 29030.8477\n",
            "Trained batch: 130 batch loss: 60.0762558 batch xy loss 7.60960197 batch wh loss 11.4999647 batch obj loss 23.246109 batch_class_loss 17.7205791 epoch total loss: 29090.9238\n",
            "Trained batch: 131 batch loss: 60.9327888 batch xy loss 8.51635742 batch wh loss 12.1151609 batch obj loss 23.0795155 batch_class_loss 17.2217522 epoch total loss: 29151.8574\n",
            "Trained batch: 132 batch loss: 52.9994087 batch xy loss 7.55439854 batch wh loss 9.35134506 batch obj loss 20.9439926 batch_class_loss 15.1496744 epoch total loss: 29204.8574\n",
            "Trained batch: 133 batch loss: 51.3976212 batch xy loss 7.03034687 batch wh loss 10.9740162 batch obj loss 19.9706268 batch_class_loss 13.4226322 epoch total loss: 29256.2559\n",
            "Trained batch: 134 batch loss: 61.4076576 batch xy loss 8.48596478 batch wh loss 11.5292397 batch obj loss 24.2870102 batch_class_loss 17.1054401 epoch total loss: 29317.6641\n",
            "Trained batch: 135 batch loss: 67.6746216 batch xy loss 9.64406 batch wh loss 14.1633272 batch obj loss 25.1201038 batch_class_loss 18.7471256 epoch total loss: 29385.3379\n",
            "Trained batch: 136 batch loss: 50.6748161 batch xy loss 7.37338066 batch wh loss 9.18335533 batch obj loss 20.2923908 batch_class_loss 13.8256941 epoch total loss: 29436.0137\n",
            "Trained batch: 137 batch loss: 68.4589462 batch xy loss 9.36905479 batch wh loss 12.3902893 batch obj loss 27.2975979 batch_class_loss 19.402 epoch total loss: 29504.4727\n",
            "Trained batch: 138 batch loss: 53.5453796 batch xy loss 7.15133953 batch wh loss 9.34653664 batch obj loss 22.2447605 batch_class_loss 14.802743 epoch total loss: 29558.0176\n",
            "Trained batch: 139 batch loss: 48.1844 batch xy loss 6.82709599 batch wh loss 9.38896465 batch obj loss 19.5265484 batch_class_loss 12.4417915 epoch total loss: 29606.2012\n",
            "Trained batch: 140 batch loss: 71.7580261 batch xy loss 9.33681107 batch wh loss 17.5415325 batch obj loss 26.8184776 batch_class_loss 18.0612144 epoch total loss: 29677.959\n",
            "Trained batch: 141 batch loss: 67.6111755 batch xy loss 8.45114708 batch wh loss 14.7831583 batch obj loss 26.6040306 batch_class_loss 17.7728367 epoch total loss: 29745.5703\n",
            "Trained batch: 142 batch loss: 55.0706711 batch xy loss 7.50836468 batch wh loss 11.557642 batch obj loss 22.3351479 batch_class_loss 13.6695185 epoch total loss: 29800.6406\n",
            "Trained batch: 143 batch loss: 60.4480896 batch xy loss 8.63707352 batch wh loss 11.0319624 batch obj loss 24.0615845 batch_class_loss 16.7174644 epoch total loss: 29861.0879\n",
            "Trained batch: 144 batch loss: 49.4608269 batch xy loss 6.65132046 batch wh loss 10.0748615 batch obj loss 20.7226048 batch_class_loss 12.0120411 epoch total loss: 29910.5488\n",
            "Trained batch: 145 batch loss: 64.3724213 batch xy loss 8.50919056 batch wh loss 13.6495485 batch obj loss 25.2362289 batch_class_loss 16.977459 epoch total loss: 29974.9219\n",
            "Trained batch: 146 batch loss: 56.9902191 batch xy loss 7.38550472 batch wh loss 12.2309895 batch obj loss 22.9465122 batch_class_loss 14.4272137 epoch total loss: 30031.9121\n",
            "Trained batch: 147 batch loss: 55.037365 batch xy loss 7.19793224 batch wh loss 10.8720074 batch obj loss 22.7602139 batch_class_loss 14.2072077 epoch total loss: 30086.9492\n",
            "Trained batch: 148 batch loss: 67.4439316 batch xy loss 8.83361721 batch wh loss 14.0763111 batch obj loss 25.9837341 batch_class_loss 18.5502701 epoch total loss: 30154.3926\n",
            "Trained batch: 149 batch loss: 51.6907349 batch xy loss 7.81497669 batch wh loss 7.94350815 batch obj loss 21.7560577 batch_class_loss 14.1761923 epoch total loss: 30206.084\n",
            "Trained batch: 150 batch loss: 55.2619247 batch xy loss 7.41027403 batch wh loss 10.29286 batch obj loss 21.8699074 batch_class_loss 15.6888866 epoch total loss: 30261.3457\n",
            "Trained batch: 151 batch loss: 44.0556259 batch xy loss 6.37993336 batch wh loss 8.13400269 batch obj loss 17.8388329 batch_class_loss 11.702858 epoch total loss: 30305.4\n",
            "Trained batch: 152 batch loss: 52.3671112 batch xy loss 7.32640171 batch wh loss 8.57854843 batch obj loss 21.2438526 batch_class_loss 15.2183094 epoch total loss: 30357.7676\n",
            "Trained batch: 153 batch loss: 56.6023483 batch xy loss 7.6562438 batch wh loss 8.96576 batch obj loss 23.7743645 batch_class_loss 16.2059765 epoch total loss: 30414.3691\n",
            "Trained batch: 154 batch loss: 54.7293968 batch xy loss 8.04705524 batch wh loss 11.9345741 batch obj loss 20.4026184 batch_class_loss 14.3451452 epoch total loss: 30469.0977\n",
            "Trained batch: 155 batch loss: 66.6822586 batch xy loss 9.24515724 batch wh loss 14.6084509 batch obj loss 24.3034401 batch_class_loss 18.5252075 epoch total loss: 30535.7793\n",
            "Trained batch: 156 batch loss: 46.0498657 batch xy loss 6.9557848 batch wh loss 9.19758 batch obj loss 18.0713234 batch_class_loss 11.8251724 epoch total loss: 30581.8301\n",
            "Trained batch: 157 batch loss: 66.6472626 batch xy loss 10.0173025 batch wh loss 13.2465019 batch obj loss 25.2537422 batch_class_loss 18.1297169 epoch total loss: 30648.4766\n",
            "Trained batch: 158 batch loss: 45.611763 batch xy loss 6.04014158 batch wh loss 9.25544548 batch obj loss 18.553709 batch_class_loss 11.7624626 epoch total loss: 30694.0879\n",
            "Trained batch: 159 batch loss: 63.0094 batch xy loss 8.99164677 batch wh loss 12.1251211 batch obj loss 24.1848488 batch_class_loss 17.7077808 epoch total loss: 30757.0977\n",
            "Trained batch: 160 batch loss: 55.340992 batch xy loss 7.95987415 batch wh loss 10.2076159 batch obj loss 21.6755295 batch_class_loss 15.4979792 epoch total loss: 30812.4395\n",
            "Trained batch: 161 batch loss: 64.4265 batch xy loss 8.72361851 batch wh loss 14.4798183 batch obj loss 23.7749023 batch_class_loss 17.4481564 epoch total loss: 30876.8652\n",
            "Trained batch: 162 batch loss: 50.4936295 batch xy loss 7.16516304 batch wh loss 9.89600563 batch obj loss 20.7988873 batch_class_loss 12.6335745 epoch total loss: 30927.3594\n",
            "Trained batch: 163 batch loss: 50.4253311 batch xy loss 7.04079437 batch wh loss 10.7710972 batch obj loss 20.3517876 batch_class_loss 12.2616558 epoch total loss: 30977.7852\n",
            "Trained batch: 164 batch loss: 52.3829346 batch xy loss 6.33931637 batch wh loss 11.4673462 batch obj loss 20.6752815 batch_class_loss 13.9009876 epoch total loss: 31030.168\n",
            "Trained batch: 165 batch loss: 54.3510284 batch xy loss 7.21908283 batch wh loss 10.0260611 batch obj loss 22.4242249 batch_class_loss 14.6816568 epoch total loss: 31084.5195\n",
            "Trained batch: 166 batch loss: 43.2910233 batch xy loss 5.64754725 batch wh loss 8.70616 batch obj loss 17.8400059 batch_class_loss 11.097311 epoch total loss: 31127.8105\n",
            "Trained batch: 167 batch loss: 43.4382095 batch xy loss 6.05379295 batch wh loss 7.90356827 batch obj loss 17.8910236 batch_class_loss 11.5898228 epoch total loss: 31171.248\n",
            "Trained batch: 168 batch loss: 57.0483284 batch xy loss 8.56504631 batch wh loss 11.8096828 batch obj loss 21.5629826 batch_class_loss 15.1106176 epoch total loss: 31228.2969\n",
            "Trained batch: 169 batch loss: 51.3912773 batch xy loss 6.75665712 batch wh loss 10.775341 batch obj loss 20.535923 batch_class_loss 13.3233566 epoch total loss: 31279.6875\n",
            "Trained batch: 170 batch loss: 45.422451 batch xy loss 6.79229259 batch wh loss 8.26485825 batch obj loss 18.3726463 batch_class_loss 11.9926567 epoch total loss: 31325.1094\n",
            "Trained batch: 171 batch loss: 56.534935 batch xy loss 7.73088884 batch wh loss 12.3161354 batch obj loss 21.4739952 batch_class_loss 15.0139198 epoch total loss: 31381.6445\n",
            "Trained batch: 172 batch loss: 56.5087738 batch xy loss 7.13289833 batch wh loss 13.1781387 batch obj loss 21.4858932 batch_class_loss 14.7118425 epoch total loss: 31438.1523\n",
            "Trained batch: 173 batch loss: 68.0188751 batch xy loss 9.40548706 batch wh loss 14.9144783 batch obj loss 24.6897659 batch_class_loss 19.0091438 epoch total loss: 31506.1719\n",
            "Trained batch: 174 batch loss: 47.4272156 batch xy loss 7.41087 batch wh loss 8.00710869 batch obj loss 18.8916683 batch_class_loss 13.1175737 epoch total loss: 31553.6\n",
            "Trained batch: 175 batch loss: 45.0520897 batch xy loss 6.30899906 batch wh loss 9.94632435 batch obj loss 17.511097 batch_class_loss 11.2856693 epoch total loss: 31598.6523\n",
            "Trained batch: 176 batch loss: 55.9920387 batch xy loss 7.44886732 batch wh loss 12.2972507 batch obj loss 20.9318428 batch_class_loss 15.3140764 epoch total loss: 31654.6445\n",
            "Trained batch: 177 batch loss: 45.7678757 batch xy loss 5.96809626 batch wh loss 8.54196262 batch obj loss 18.6283092 batch_class_loss 12.6295071 epoch total loss: 31700.4121\n",
            "Trained batch: 178 batch loss: 58.7594757 batch xy loss 7.9227643 batch wh loss 12.0457926 batch obj loss 22.801897 batch_class_loss 15.9890184 epoch total loss: 31759.1719\n",
            "Trained batch: 179 batch loss: 49.8450203 batch xy loss 6.68075466 batch wh loss 9.71820354 batch obj loss 19.9473362 batch_class_loss 13.498724 epoch total loss: 31809.0176\n",
            "Trained batch: 180 batch loss: 55.7458038 batch xy loss 7.15771103 batch wh loss 11.1817369 batch obj loss 21.951067 batch_class_loss 15.4552841 epoch total loss: 31864.7637\n",
            "Trained batch: 181 batch loss: 49.9850845 batch xy loss 7.90367699 batch wh loss 10.6806526 batch obj loss 18.7511215 batch_class_loss 12.6496344 epoch total loss: 31914.748\n",
            "Trained batch: 182 batch loss: 47.7122383 batch xy loss 6.59336948 batch wh loss 7.58244848 batch obj loss 20.7153015 batch_class_loss 12.8211193 epoch total loss: 31962.4609\n",
            "Trained batch: 183 batch loss: 57.8705 batch xy loss 8.26827335 batch wh loss 12.6501598 batch obj loss 22.1170769 batch_class_loss 14.8349924 epoch total loss: 32020.332\n",
            "Trained batch: 184 batch loss: 63.4153519 batch xy loss 8.85000801 batch wh loss 13.777976 batch obj loss 23.8915939 batch_class_loss 16.8957691 epoch total loss: 32083.748\n",
            "Trained batch: 185 batch loss: 46.4895172 batch xy loss 6.34274673 batch wh loss 8.9817915 batch obj loss 18.4247952 batch_class_loss 12.7401838 epoch total loss: 32130.2383\n",
            "Trained batch: 186 batch loss: 41.1705933 batch xy loss 5.62725592 batch wh loss 8.21982384 batch obj loss 16.6402435 batch_class_loss 10.6832733 epoch total loss: 32171.4082\n",
            "Trained batch: 187 batch loss: 56.6634521 batch xy loss 8.02772141 batch wh loss 11.4715166 batch obj loss 22.1557217 batch_class_loss 15.0084953 epoch total loss: 32228.0723\n",
            "Trained batch: 188 batch loss: 62.5389938 batch xy loss 8.40497 batch wh loss 12.6417322 batch obj loss 24.3490906 batch_class_loss 17.1432018 epoch total loss: 32290.6113\n",
            "Trained batch: 189 batch loss: 60.2237 batch xy loss 8.59289265 batch wh loss 11.0209255 batch obj loss 23.7871094 batch_class_loss 16.8227749 epoch total loss: 32350.8359\n",
            "Trained batch: 190 batch loss: 58.6054382 batch xy loss 7.73352718 batch wh loss 13.1566629 batch obj loss 21.9090137 batch_class_loss 15.8062372 epoch total loss: 32409.4414\n",
            "Trained batch: 191 batch loss: 57.9243965 batch xy loss 8.46219254 batch wh loss 11.8759 batch obj loss 22.012537 batch_class_loss 15.5737677 epoch total loss: 32467.3652\n",
            "Trained batch: 192 batch loss: 66.3458 batch xy loss 9.06972313 batch wh loss 15.6105652 batch obj loss 23.8213463 batch_class_loss 17.8441677 epoch total loss: 32533.7109\n",
            "Trained batch: 193 batch loss: 55.5028496 batch xy loss 7.36083078 batch wh loss 14.0445204 batch obj loss 21.2509499 batch_class_loss 12.8465443 epoch total loss: 32589.2129\n",
            "Trained batch: 194 batch loss: 54.5981674 batch xy loss 7.68910122 batch wh loss 12.3213072 batch obj loss 20.7343845 batch_class_loss 13.8533716 epoch total loss: 32643.8105\n",
            "Trained batch: 195 batch loss: 62.7787476 batch xy loss 8.497262 batch wh loss 11.5630217 batch obj loss 24.9364147 batch_class_loss 17.7820473 epoch total loss: 32706.5898\n",
            "Trained batch: 196 batch loss: 48.3909302 batch xy loss 7.25736141 batch wh loss 9.08026791 batch obj loss 19.946722 batch_class_loss 12.1065769 epoch total loss: 32754.9805\n",
            "Trained batch: 197 batch loss: 46.0136223 batch xy loss 7.37604237 batch wh loss 8.84025097 batch obj loss 18.6037312 batch_class_loss 11.1935978 epoch total loss: 32800.9922\n",
            "Trained batch: 198 batch loss: 64.9757843 batch xy loss 9.29436493 batch wh loss 13.8180428 batch obj loss 24.5746212 batch_class_loss 17.2887554 epoch total loss: 32865.9688\n",
            "Trained batch: 199 batch loss: 50.0680809 batch xy loss 6.35970926 batch wh loss 8.78176498 batch obj loss 21.5552044 batch_class_loss 13.3714066 epoch total loss: 32916.0352\n",
            "Trained batch: 200 batch loss: 54.3660049 batch xy loss 7.74664164 batch wh loss 10.8511734 batch obj loss 21.1947174 batch_class_loss 14.5734758 epoch total loss: 32970.4023\n",
            "Trained batch: 201 batch loss: 60.0974579 batch xy loss 7.72197771 batch wh loss 14.7054 batch obj loss 22.4413662 batch_class_loss 15.2287102 epoch total loss: 33030.5\n",
            "Trained batch: 202 batch loss: 45.4946938 batch xy loss 6.64014673 batch wh loss 8.06706524 batch obj loss 18.8615971 batch_class_loss 11.9258804 epoch total loss: 33075.9961\n",
            "Trained batch: 203 batch loss: 48.4055023 batch xy loss 7.14879799 batch wh loss 9.21913528 batch obj loss 19.4172249 batch_class_loss 12.620348 epoch total loss: 33124.4023\n",
            "Trained batch: 204 batch loss: 56.5023117 batch xy loss 7.90734673 batch wh loss 11.5365639 batch obj loss 21.7836571 batch_class_loss 15.2747421 epoch total loss: 33180.9062\n",
            "Trained batch: 205 batch loss: 51.47892 batch xy loss 7.41018343 batch wh loss 11.2066574 batch obj loss 20.2751427 batch_class_loss 12.5869322 epoch total loss: 33232.3867\n",
            "Trained batch: 206 batch loss: 44.1579971 batch xy loss 6.38360405 batch wh loss 8.18352604 batch obj loss 17.9402561 batch_class_loss 11.6506071 epoch total loss: 33276.543\n",
            "Trained batch: 207 batch loss: 60.9943581 batch xy loss 8.96023941 batch wh loss 14.8572388 batch obj loss 21.3685017 batch_class_loss 15.8083763 epoch total loss: 33337.5391\n",
            "Trained batch: 208 batch loss: 54.4993973 batch xy loss 6.9780283 batch wh loss 10.6471252 batch obj loss 21.5511246 batch_class_loss 15.3231192 epoch total loss: 33392.0391\n",
            "Trained batch: 209 batch loss: 48.3342857 batch xy loss 7.11867714 batch wh loss 7.94036293 batch obj loss 19.70084 batch_class_loss 13.5744009 epoch total loss: 33440.375\n",
            "Trained batch: 210 batch loss: 43.7385864 batch xy loss 6.43201685 batch wh loss 7.29292488 batch obj loss 18.2979927 batch_class_loss 11.7156506 epoch total loss: 33484.1133\n",
            "Trained batch: 211 batch loss: 45.178894 batch xy loss 6.48638439 batch wh loss 8.39078236 batch obj loss 18.7795486 batch_class_loss 11.5221825 epoch total loss: 33529.293\n",
            "Trained batch: 212 batch loss: 55.4139137 batch xy loss 8.39580727 batch wh loss 12.5725784 batch obj loss 20.5963535 batch_class_loss 13.8491764 epoch total loss: 33584.707\n",
            "Trained batch: 213 batch loss: 57.9934311 batch xy loss 8.72359848 batch wh loss 10.1969757 batch obj loss 22.2742882 batch_class_loss 16.7985668 epoch total loss: 33642.7\n",
            "Trained batch: 214 batch loss: 44.2767601 batch xy loss 6.01819038 batch wh loss 10.2522945 batch obj loss 17.4385281 batch_class_loss 10.5677481 epoch total loss: 33686.9766\n",
            "Trained batch: 215 batch loss: 43.9804077 batch xy loss 5.88426161 batch wh loss 8.97917557 batch obj loss 17.7338943 batch_class_loss 11.3830748 epoch total loss: 33730.957\n",
            "Trained batch: 216 batch loss: 66.7997589 batch xy loss 9.23467255 batch wh loss 16.22332 batch obj loss 24.0988216 batch_class_loss 17.2429447 epoch total loss: 33797.7578\n",
            "Trained batch: 217 batch loss: 52.9930916 batch xy loss 7.90279484 batch wh loss 10.1675262 batch obj loss 20.1985226 batch_class_loss 14.7242489 epoch total loss: 33850.75\n",
            "Trained batch: 218 batch loss: 52.9822922 batch xy loss 7.18552542 batch wh loss 10.7502823 batch obj loss 21.3448982 batch_class_loss 13.7015867 epoch total loss: 33903.7305\n",
            "Trained batch: 219 batch loss: 57.6666527 batch xy loss 7.70626545 batch wh loss 14.8473682 batch obj loss 20.4246025 batch_class_loss 14.6884136 epoch total loss: 33961.4\n",
            "Trained batch: 220 batch loss: 41.0574837 batch xy loss 6.12438488 batch wh loss 8.64454269 batch obj loss 16.101265 batch_class_loss 10.1872902 epoch total loss: 34002.457\n",
            "Trained batch: 221 batch loss: 62.9018059 batch xy loss 8.90906906 batch wh loss 13.7813091 batch obj loss 22.9266434 batch_class_loss 17.2847824 epoch total loss: 34065.3594\n",
            "Trained batch: 222 batch loss: 51.5818939 batch xy loss 8.05253315 batch wh loss 9.48959732 batch obj loss 20.7390289 batch_class_loss 13.3007326 epoch total loss: 34116.9414\n",
            "Trained batch: 223 batch loss: 62.368103 batch xy loss 8.69659233 batch wh loss 12.8843403 batch obj loss 23.8482628 batch_class_loss 16.9389076 epoch total loss: 34179.3086\n",
            "Trained batch: 224 batch loss: 43.4853249 batch xy loss 6.43266773 batch wh loss 7.81652308 batch obj loss 18.4586678 batch_class_loss 10.7774696 epoch total loss: 34222.793\n",
            "Trained batch: 225 batch loss: 51.9361343 batch xy loss 7.16698408 batch wh loss 9.15150833 batch obj loss 21.5286922 batch_class_loss 14.0889492 epoch total loss: 34274.7305\n",
            "Trained batch: 226 batch loss: 50.9061661 batch xy loss 6.68254614 batch wh loss 10.4524155 batch obj loss 20.096159 batch_class_loss 13.6750469 epoch total loss: 34325.6367\n",
            "Trained batch: 227 batch loss: 62.2292709 batch xy loss 9.28900528 batch wh loss 12.7171955 batch obj loss 23.7859745 batch_class_loss 16.4371014 epoch total loss: 34387.8672\n",
            "Trained batch: 228 batch loss: 50.6625328 batch xy loss 7.21688557 batch wh loss 9.91809845 batch obj loss 20.6672382 batch_class_loss 12.8603086 epoch total loss: 34438.5312\n",
            "Trained batch: 229 batch loss: 50.6692963 batch xy loss 7.23982191 batch wh loss 10.6396 batch obj loss 20.0373306 batch_class_loss 12.7525444 epoch total loss: 34489.2\n",
            "Trained batch: 230 batch loss: 42.587944 batch xy loss 5.34873486 batch wh loss 9.20900917 batch obj loss 17.5465374 batch_class_loss 10.4836655 epoch total loss: 34531.7891\n",
            "Trained batch: 231 batch loss: 43.5292511 batch xy loss 5.66695881 batch wh loss 9.45724487 batch obj loss 17.6137428 batch_class_loss 10.7913094 epoch total loss: 34575.3164\n",
            "Trained batch: 232 batch loss: 51.4722977 batch xy loss 7.40121889 batch wh loss 10.6112137 batch obj loss 19.5989304 batch_class_loss 13.8609304 epoch total loss: 34626.7891\n",
            "Trained batch: 233 batch loss: 42.7692833 batch xy loss 5.93561316 batch wh loss 8.72500706 batch obj loss 17.2474079 batch_class_loss 10.8612537 epoch total loss: 34669.5586\n",
            "Trained batch: 234 batch loss: 46.851223 batch xy loss 6.81737137 batch wh loss 9.92518806 batch obj loss 17.4814377 batch_class_loss 12.6272306 epoch total loss: 34716.4102\n",
            "Trained batch: 235 batch loss: 48.3570366 batch xy loss 6.9088645 batch wh loss 9.35746384 batch obj loss 18.9809608 batch_class_loss 13.1097469 epoch total loss: 34764.7656\n",
            "Trained batch: 236 batch loss: 47.868576 batch xy loss 6.78034544 batch wh loss 10.9758806 batch obj loss 17.8425217 batch_class_loss 12.2698278 epoch total loss: 34812.6328\n",
            "Trained batch: 237 batch loss: 41.7339325 batch xy loss 5.44858742 batch wh loss 7.5253582 batch obj loss 17.4878387 batch_class_loss 11.27215 epoch total loss: 34854.3672\n",
            "Trained batch: 238 batch loss: 51.8556938 batch xy loss 7.12652969 batch wh loss 11.4496365 batch obj loss 18.8053474 batch_class_loss 14.4741745 epoch total loss: 34906.2227\n",
            "Trained batch: 239 batch loss: 48.8279724 batch xy loss 7.22113037 batch wh loss 8.19706 batch obj loss 19.0836658 batch_class_loss 14.3261166 epoch total loss: 34955.0508\n",
            "Trained batch: 240 batch loss: 52.5752106 batch xy loss 7.9179306 batch wh loss 8.84222698 batch obj loss 20.5957146 batch_class_loss 15.2193432 epoch total loss: 35007.625\n",
            "Trained batch: 241 batch loss: 46.0782585 batch xy loss 6.52017117 batch wh loss 9.28508 batch obj loss 18.1520138 batch_class_loss 12.1209946 epoch total loss: 35053.7031\n",
            "Trained batch: 242 batch loss: 48.0818062 batch xy loss 6.9452033 batch wh loss 8.41147804 batch obj loss 19.4309731 batch_class_loss 13.2941475 epoch total loss: 35101.7852\n",
            "Trained batch: 243 batch loss: 58.6556168 batch xy loss 8.55264 batch wh loss 11.7823372 batch obj loss 21.8692989 batch_class_loss 16.4513397 epoch total loss: 35160.4414\n",
            "Trained batch: 244 batch loss: 46.0943031 batch xy loss 6.28689671 batch wh loss 9.64047623 batch obj loss 18.1424828 batch_class_loss 12.0244484 epoch total loss: 35206.5352\n",
            "Trained batch: 245 batch loss: 52.6917114 batch xy loss 8.06179905 batch wh loss 10.5705528 batch obj loss 20.1988831 batch_class_loss 13.8604755 epoch total loss: 35259.2266\n",
            "Trained batch: 246 batch loss: 55.741806 batch xy loss 7.35851765 batch wh loss 13.0002298 batch obj loss 21.0413475 batch_class_loss 14.341712 epoch total loss: 35314.9688\n",
            "Trained batch: 247 batch loss: 46.7153511 batch xy loss 6.51892567 batch wh loss 8.42937 batch obj loss 19.0550861 batch_class_loss 12.7119703 epoch total loss: 35361.6836\n",
            "Trained batch: 248 batch loss: 47.593792 batch xy loss 6.80198383 batch wh loss 10.1444855 batch obj loss 18.4382858 batch_class_loss 12.2090349 epoch total loss: 35409.2773\n",
            "Trained batch: 249 batch loss: 50.5830688 batch xy loss 7.24429607 batch wh loss 9.85283089 batch obj loss 20.596489 batch_class_loss 12.889452 epoch total loss: 35459.8594\n",
            "Trained batch: 250 batch loss: 52.1854591 batch xy loss 7.34422588 batch wh loss 10.6940985 batch obj loss 20.3486328 batch_class_loss 13.798501 epoch total loss: 35512.043\n",
            "Trained batch: 251 batch loss: 49.3987465 batch xy loss 6.74137402 batch wh loss 9.472085 batch obj loss 20.5186 batch_class_loss 12.6666899 epoch total loss: 35561.4414\n",
            "Trained batch: 252 batch loss: 46.4764633 batch xy loss 6.76480818 batch wh loss 7.74863958 batch obj loss 19.0283546 batch_class_loss 12.934659 epoch total loss: 35607.918\n",
            "Trained batch: 253 batch loss: 51.8562431 batch xy loss 7.37976265 batch wh loss 11.3501339 batch obj loss 19.2387028 batch_class_loss 13.8876448 epoch total loss: 35659.7734\n",
            "Trained batch: 254 batch loss: 43.2758 batch xy loss 5.96398258 batch wh loss 7.41161346 batch obj loss 18.230032 batch_class_loss 11.6701698 epoch total loss: 35703.0508\n",
            "Trained batch: 255 batch loss: 55.770916 batch xy loss 7.71306038 batch wh loss 14.2223711 batch obj loss 19.9686394 batch_class_loss 13.866848 epoch total loss: 35758.8203\n",
            "Trained batch: 256 batch loss: 50.6847801 batch xy loss 7.35837936 batch wh loss 10.0423422 batch obj loss 19.8265686 batch_class_loss 13.4574938 epoch total loss: 35809.5039\n",
            "Trained batch: 257 batch loss: 50.6436653 batch xy loss 7.86861706 batch wh loss 9.47897911 batch obj loss 19.5223408 batch_class_loss 13.7737322 epoch total loss: 35860.1484\n",
            "Trained batch: 258 batch loss: 43.0988693 batch xy loss 7.24308 batch wh loss 7.45840788 batch obj loss 17.144062 batch_class_loss 11.2533216 epoch total loss: 35903.2461\n",
            "Trained batch: 259 batch loss: 55.9333878 batch xy loss 7.80291748 batch wh loss 13.7139158 batch obj loss 20.4066048 batch_class_loss 14.0099497 epoch total loss: 35959.1797\n",
            "Trained batch: 260 batch loss: 47.3883553 batch xy loss 7.4585886 batch wh loss 8.80587673 batch obj loss 18.5506363 batch_class_loss 12.5732536 epoch total loss: 36006.5664\n",
            "Trained batch: 261 batch loss: 54.4025917 batch xy loss 8.43125248 batch wh loss 11.2436981 batch obj loss 20.3834152 batch_class_loss 14.3442211 epoch total loss: 36060.9688\n",
            "Trained batch: 262 batch loss: 47.2898865 batch xy loss 6.79435921 batch wh loss 9.62203407 batch obj loss 19.2302265 batch_class_loss 11.6432667 epoch total loss: 36108.2578\n",
            "Trained batch: 263 batch loss: 44.6147423 batch xy loss 6.66548252 batch wh loss 6.9210062 batch obj loss 18.6162949 batch_class_loss 12.4119616 epoch total loss: 36152.8711\n",
            "Trained batch: 264 batch loss: 45.3164902 batch xy loss 7.21589088 batch wh loss 7.76006556 batch obj loss 18.3654804 batch_class_loss 11.9750538 epoch total loss: 36198.1875\n",
            "Trained batch: 265 batch loss: 46.7499046 batch xy loss 6.32461452 batch wh loss 9.47481728 batch obj loss 19.4931736 batch_class_loss 11.4572973 epoch total loss: 36244.9375\n",
            "Trained batch: 266 batch loss: 48.874424 batch xy loss 7.61044073 batch wh loss 9.52919 batch obj loss 19.0229301 batch_class_loss 12.7118597 epoch total loss: 36293.8125\n",
            "Trained batch: 267 batch loss: 48.4147186 batch xy loss 6.88041592 batch wh loss 10.6885433 batch obj loss 18.9012547 batch_class_loss 11.9445019 epoch total loss: 36342.2266\n",
            "Trained batch: 268 batch loss: 44.4312363 batch xy loss 6.3052206 batch wh loss 8.45883369 batch obj loss 17.5389118 batch_class_loss 12.128274 epoch total loss: 36386.6562\n",
            "Trained batch: 269 batch loss: 46.9514694 batch xy loss 7.41856098 batch wh loss 8.54440308 batch obj loss 18.5319386 batch_class_loss 12.4565716 epoch total loss: 36433.6094\n",
            "Trained batch: 270 batch loss: 47.4449921 batch xy loss 6.39364433 batch wh loss 9.15611458 batch obj loss 18.7896576 batch_class_loss 13.1055746 epoch total loss: 36481.0547\n",
            "Trained batch: 271 batch loss: 50.4595871 batch xy loss 7.15669918 batch wh loss 11.6551294 batch obj loss 19.0332642 batch_class_loss 12.6144934 epoch total loss: 36531.5156\n",
            "Trained batch: 272 batch loss: 48.9982758 batch xy loss 7.76532173 batch wh loss 9.90903 batch obj loss 18.7787514 batch_class_loss 12.5451717 epoch total loss: 36580.5156\n",
            "Trained batch: 273 batch loss: 42.8874168 batch xy loss 6.54764366 batch wh loss 7.47877693 batch obj loss 17.3204708 batch_class_loss 11.5405264 epoch total loss: 36623.4023\n",
            "Trained batch: 274 batch loss: 61.8909836 batch xy loss 8.71415 batch wh loss 11.4542522 batch obj loss 23.6598282 batch_class_loss 18.0627499 epoch total loss: 36685.293\n",
            "Trained batch: 275 batch loss: 42.9188 batch xy loss 5.83227921 batch wh loss 8.28649521 batch obj loss 17.2274551 batch_class_loss 11.5725727 epoch total loss: 36728.2109\n",
            "Trained batch: 276 batch loss: 52.4612389 batch xy loss 7.53844357 batch wh loss 9.20353794 batch obj loss 21.3914433 batch_class_loss 14.327816 epoch total loss: 36780.6719\n",
            "Trained batch: 277 batch loss: 50.8788719 batch xy loss 6.69205236 batch wh loss 12.4465256 batch obj loss 19.4317226 batch_class_loss 12.3085709 epoch total loss: 36831.5508\n",
            "Trained batch: 278 batch loss: 40.2122574 batch xy loss 6.09861279 batch wh loss 7.27819252 batch obj loss 16.8891888 batch_class_loss 9.94626236 epoch total loss: 36871.7617\n",
            "Trained batch: 279 batch loss: 54.3495064 batch xy loss 8.82150269 batch wh loss 10.3184481 batch obj loss 20.3960228 batch_class_loss 14.8135319 epoch total loss: 36926.1094\n",
            "Trained batch: 280 batch loss: 52.9748268 batch xy loss 8.15679741 batch wh loss 9.81519127 batch obj loss 20.7452259 batch_class_loss 14.2576141 epoch total loss: 36979.0859\n",
            "Trained batch: 281 batch loss: 53.0223427 batch xy loss 7.95824051 batch wh loss 9.95384 batch obj loss 20.7694073 batch_class_loss 14.3408518 epoch total loss: 37032.1094\n",
            "Trained batch: 282 batch loss: 45.3337479 batch xy loss 7.25322914 batch wh loss 8.51766872 batch obj loss 17.6363525 batch_class_loss 11.9265 epoch total loss: 37077.4414\n",
            "Trained batch: 283 batch loss: 52.3418274 batch xy loss 8.08928871 batch wh loss 10.239152 batch obj loss 20.1181374 batch_class_loss 13.8952503 epoch total loss: 37129.7852\n",
            "Trained batch: 284 batch loss: 55.5756836 batch xy loss 7.70312691 batch wh loss 10.8094606 batch obj loss 21.7033749 batch_class_loss 15.3597193 epoch total loss: 37185.3594\n",
            "Trained batch: 285 batch loss: 55.4608383 batch xy loss 8.28333282 batch wh loss 12.9604788 batch obj loss 20.4107914 batch_class_loss 13.8062325 epoch total loss: 37240.8203\n",
            "Trained batch: 286 batch loss: 52.1735268 batch xy loss 7.2824316 batch wh loss 12.0651846 batch obj loss 20.0510712 batch_class_loss 12.7748384 epoch total loss: 37292.9922\n",
            "Trained batch: 287 batch loss: 54.8191948 batch xy loss 7.91148281 batch wh loss 10.067872 batch obj loss 21.7009449 batch_class_loss 15.1388931 epoch total loss: 37347.8125\n",
            "Trained batch: 288 batch loss: 46.9656296 batch xy loss 7.12023735 batch wh loss 8.35385704 batch obj loss 18.9076023 batch_class_loss 12.583931 epoch total loss: 37394.7773\n",
            "Trained batch: 289 batch loss: 45.0796089 batch xy loss 6.28952217 batch wh loss 9.28912544 batch obj loss 18.1316509 batch_class_loss 11.3693094 epoch total loss: 37439.8555\n",
            "Trained batch: 290 batch loss: 46.7280655 batch xy loss 7.08135891 batch wh loss 9.04420853 batch obj loss 18.4036026 batch_class_loss 12.1988974 epoch total loss: 37486.582\n",
            "Trained batch: 291 batch loss: 38.4167137 batch xy loss 4.93156242 batch wh loss 7.56631517 batch obj loss 15.7652864 batch_class_loss 10.1535511 epoch total loss: 37525\n",
            "Trained batch: 292 batch loss: 47.3088837 batch xy loss 7.4331913 batch wh loss 9.54181194 batch obj loss 18.2164364 batch_class_loss 12.117444 epoch total loss: 37572.3086\n",
            "Trained batch: 293 batch loss: 35.4375954 batch xy loss 4.99880743 batch wh loss 5.93202877 batch obj loss 14.9107399 batch_class_loss 9.59601879 epoch total loss: 37607.7461\n",
            "Trained batch: 294 batch loss: 42.8823433 batch xy loss 6.68544626 batch wh loss 7.10428047 batch obj loss 17.1091366 batch_class_loss 11.9834824 epoch total loss: 37650.6289\n",
            "Trained batch: 295 batch loss: 48.7959938 batch xy loss 7.66529226 batch wh loss 9.11315823 batch obj loss 19.0532932 batch_class_loss 12.9642458 epoch total loss: 37699.4258\n",
            "Trained batch: 296 batch loss: 42.2452316 batch xy loss 5.96903133 batch wh loss 7.37393379 batch obj loss 17.2442322 batch_class_loss 11.6580391 epoch total loss: 37741.6719\n",
            "Trained batch: 297 batch loss: 55.1469116 batch xy loss 8.47100067 batch wh loss 9.80209541 batch obj loss 21.0965576 batch_class_loss 15.7772617 epoch total loss: 37796.8203\n",
            "Trained batch: 298 batch loss: 45.3999367 batch xy loss 6.87546539 batch wh loss 7.75688171 batch obj loss 18.3842049 batch_class_loss 12.3833838 epoch total loss: 37842.2188\n",
            "Trained batch: 299 batch loss: 45.4061203 batch xy loss 7.33846951 batch wh loss 9.15691757 batch obj loss 17.4926 batch_class_loss 11.4181356 epoch total loss: 37887.625\n",
            "Trained batch: 300 batch loss: 45.8603058 batch xy loss 6.87119 batch wh loss 8.19705391 batch obj loss 18.0536919 batch_class_loss 12.7383728 epoch total loss: 37933.4844\n",
            "Trained batch: 301 batch loss: 46.9730721 batch xy loss 6.77029514 batch wh loss 8.62238407 batch obj loss 18.8667 batch_class_loss 12.7136917 epoch total loss: 37980.457\n",
            "Trained batch: 302 batch loss: 49.5727577 batch xy loss 7.29593182 batch wh loss 9.79279137 batch obj loss 19.654171 batch_class_loss 12.8298626 epoch total loss: 38030.0312\n",
            "Trained batch: 303 batch loss: 46.5001183 batch xy loss 6.80594349 batch wh loss 9.55554771 batch obj loss 18.2517796 batch_class_loss 11.8868465 epoch total loss: 38076.5312\n",
            "Trained batch: 304 batch loss: 46.070549 batch xy loss 6.92422438 batch wh loss 8.26668167 batch obj loss 18.4254379 batch_class_loss 12.4542084 epoch total loss: 38122.6\n",
            "Trained batch: 305 batch loss: 43.1991653 batch xy loss 6.0087595 batch wh loss 7.8295188 batch obj loss 18.1564407 batch_class_loss 11.2044468 epoch total loss: 38165.8\n",
            "Trained batch: 306 batch loss: 55.8504448 batch xy loss 8.77088451 batch wh loss 11.8813152 batch obj loss 20.7742195 batch_class_loss 14.4240246 epoch total loss: 38221.6523\n",
            "Trained batch: 307 batch loss: 40.2757 batch xy loss 5.83034515 batch wh loss 9.64954376 batch obj loss 15.322917 batch_class_loss 9.47289276 epoch total loss: 38261.9297\n",
            "Trained batch: 308 batch loss: 53.0733757 batch xy loss 8.72546 batch wh loss 11.5549335 batch obj loss 20.2101288 batch_class_loss 12.5828476 epoch total loss: 38315.0039\n",
            "Trained batch: 309 batch loss: 53.6318092 batch xy loss 7.11163521 batch wh loss 11.1407499 batch obj loss 20.7689457 batch_class_loss 14.6104803 epoch total loss: 38368.6367\n",
            "Trained batch: 310 batch loss: 45.8688622 batch xy loss 6.21164465 batch wh loss 8.45508 batch obj loss 18.9229965 batch_class_loss 12.2791452 epoch total loss: 38414.5039\n",
            "Trained batch: 311 batch loss: 57.4429474 batch xy loss 8.53378391 batch wh loss 11.1357021 batch obj loss 22.4049397 batch_class_loss 15.3685188 epoch total loss: 38471.9453\n",
            "Trained batch: 312 batch loss: 43.6513786 batch xy loss 6.8895092 batch wh loss 7.74859333 batch obj loss 17.544075 batch_class_loss 11.469203 epoch total loss: 38515.5977\n",
            "Trained batch: 313 batch loss: 41.9513092 batch xy loss 5.98241425 batch wh loss 9.06480217 batch obj loss 16.5843754 batch_class_loss 10.3197184 epoch total loss: 38557.5508\n",
            "Trained batch: 314 batch loss: 56.2576637 batch xy loss 7.51069355 batch wh loss 12.8587856 batch obj loss 20.6752853 batch_class_loss 15.2128992 epoch total loss: 38613.8086\n",
            "Trained batch: 315 batch loss: 53.5564384 batch xy loss 7.86228657 batch wh loss 12.2398739 batch obj loss 19.5359783 batch_class_loss 13.9182949 epoch total loss: 38667.3633\n",
            "Trained batch: 316 batch loss: 38.4770088 batch xy loss 5.19900322 batch wh loss 8.96108532 batch obj loss 14.9995146 batch_class_loss 9.3174057 epoch total loss: 38705.8398\n",
            "Trained batch: 317 batch loss: 42.932312 batch xy loss 6.41707182 batch wh loss 9.15641 batch obj loss 17.0086136 batch_class_loss 10.3502169 epoch total loss: 38748.7734\n",
            "Trained batch: 318 batch loss: 49.2371788 batch xy loss 7.37123203 batch wh loss 9.381 batch obj loss 19.0841274 batch_class_loss 13.4008198 epoch total loss: 38798.0117\n",
            "Trained batch: 319 batch loss: 45.5141487 batch xy loss 6.78273392 batch wh loss 7.6863327 batch obj loss 18.282608 batch_class_loss 12.7624722 epoch total loss: 38843.5273\n",
            "Trained batch: 320 batch loss: 51.0457497 batch xy loss 7.4799509 batch wh loss 12.4460716 batch obj loss 18.4727726 batch_class_loss 12.6469555 epoch total loss: 38894.5742\n",
            "Trained batch: 321 batch loss: 34.4494476 batch xy loss 4.73950577 batch wh loss 5.64314747 batch obj loss 15.0327663 batch_class_loss 9.0340271 epoch total loss: 38929.0234\n",
            "Trained batch: 322 batch loss: 47.9150429 batch xy loss 6.51071644 batch wh loss 12.2479563 batch obj loss 17.2998505 batch_class_loss 11.8565178 epoch total loss: 38976.9375\n",
            "Trained batch: 323 batch loss: 44.7602158 batch xy loss 6.62369967 batch wh loss 9.08787251 batch obj loss 17.8811874 batch_class_loss 11.1674528 epoch total loss: 39021.7\n",
            "Trained batch: 324 batch loss: 40.1383972 batch xy loss 5.78011274 batch wh loss 8.27160358 batch obj loss 15.4387188 batch_class_loss 10.6479616 epoch total loss: 39061.8359\n",
            "Trained batch: 325 batch loss: 46.7409477 batch xy loss 6.49367809 batch wh loss 8.92367935 batch obj loss 18.2416325 batch_class_loss 13.0819588 epoch total loss: 39108.5781\n",
            "Trained batch: 326 batch loss: 48.8155594 batch xy loss 6.87988281 batch wh loss 11.1225939 batch obj loss 18.3864193 batch_class_loss 12.4266672 epoch total loss: 39157.3945\n",
            "Trained batch: 327 batch loss: 48.1400146 batch xy loss 6.30567646 batch wh loss 8.83375454 batch obj loss 18.9440231 batch_class_loss 14.0565586 epoch total loss: 39205.5352\n",
            "Trained batch: 328 batch loss: 48.0325851 batch xy loss 8.19258118 batch wh loss 8.21684074 batch obj loss 17.9386139 batch_class_loss 13.6845484 epoch total loss: 39253.5664\n",
            "Trained batch: 329 batch loss: 42.0954208 batch xy loss 6.14513779 batch wh loss 6.68056822 batch obj loss 17.57938 batch_class_loss 11.6903372 epoch total loss: 39295.6602\n",
            "Trained batch: 330 batch loss: 52.6062851 batch xy loss 7.73307896 batch wh loss 10.5001554 batch obj loss 19.7115479 batch_class_loss 14.661499 epoch total loss: 39348.2656\n",
            "Trained batch: 331 batch loss: 41.7021217 batch xy loss 5.78211 batch wh loss 7.69942188 batch obj loss 17.1007557 batch_class_loss 11.119832 epoch total loss: 39389.9688\n",
            "Trained batch: 332 batch loss: 42.7250099 batch xy loss 6.13387299 batch wh loss 8.3258934 batch obj loss 17.4770031 batch_class_loss 10.7882385 epoch total loss: 39432.6953\n",
            "Trained batch: 333 batch loss: 48.6252441 batch xy loss 7.03518295 batch wh loss 9.02675915 batch obj loss 19.1172447 batch_class_loss 13.4460583 epoch total loss: 39481.3203\n",
            "Trained batch: 334 batch loss: 54.3975258 batch xy loss 7.86958218 batch wh loss 11.7584457 batch obj loss 20.934063 batch_class_loss 13.8354387 epoch total loss: 39535.7188\n",
            "Trained batch: 335 batch loss: 43.0360222 batch xy loss 6.33909321 batch wh loss 9.88938713 batch obj loss 16.1174622 batch_class_loss 10.6900835 epoch total loss: 39578.7539\n",
            "Trained batch: 336 batch loss: 40.4625 batch xy loss 6.01242208 batch wh loss 7.68505669 batch obj loss 16.4423828 batch_class_loss 10.3226414 epoch total loss: 39619.2148\n",
            "Trained batch: 337 batch loss: 42.5371628 batch xy loss 6.54315662 batch wh loss 7.23433352 batch obj loss 17.7103767 batch_class_loss 11.0492973 epoch total loss: 39661.7539\n",
            "Trained batch: 338 batch loss: 53.47929 batch xy loss 7.75764513 batch wh loss 11.4936228 batch obj loss 20.2550755 batch_class_loss 13.9729462 epoch total loss: 39715.2344\n",
            "Trained batch: 339 batch loss: 44.7902565 batch xy loss 6.27823925 batch wh loss 8.93037128 batch obj loss 17.882206 batch_class_loss 11.6994381 epoch total loss: 39760.0234\n",
            "Trained batch: 340 batch loss: 44.9211845 batch xy loss 6.52466488 batch wh loss 8.45497608 batch obj loss 18.091095 batch_class_loss 11.8504524 epoch total loss: 39804.9453\n",
            "Trained batch: 341 batch loss: 41.2111359 batch xy loss 5.68381453 batch wh loss 7.43009901 batch obj loss 16.7136841 batch_class_loss 11.3835411 epoch total loss: 39846.1562\n",
            "Trained batch: 342 batch loss: 57.9907494 batch xy loss 7.74339867 batch wh loss 13.1523361 batch obj loss 21.4612198 batch_class_loss 15.6337948 epoch total loss: 39904.1484\n",
            "Trained batch: 343 batch loss: 44.4747467 batch xy loss 7.04021263 batch wh loss 7.88603497 batch obj loss 17.3314247 batch_class_loss 12.2170763 epoch total loss: 39948.625\n",
            "Trained batch: 344 batch loss: 43.1229477 batch xy loss 6.31009912 batch wh loss 7.57954216 batch obj loss 17.2515926 batch_class_loss 11.9817095 epoch total loss: 39991.7461\n",
            "Trained batch: 345 batch loss: 39.8138733 batch xy loss 6.09347057 batch wh loss 7.13112402 batch obj loss 16.0220699 batch_class_loss 10.5672083 epoch total loss: 40031.5586\n",
            "Trained batch: 346 batch loss: 52.6704865 batch xy loss 7.62025261 batch wh loss 10.6155224 batch obj loss 19.9591 batch_class_loss 14.4756098 epoch total loss: 40084.2305\n",
            "Trained batch: 347 batch loss: 44.7043686 batch xy loss 6.41251469 batch wh loss 9.41627407 batch obj loss 16.8838787 batch_class_loss 11.9917 epoch total loss: 40128.9336\n",
            "Trained batch: 348 batch loss: 44.041626 batch xy loss 6.50320625 batch wh loss 9.29917908 batch obj loss 17.0978718 batch_class_loss 11.1413717 epoch total loss: 40172.9766\n",
            "Trained batch: 349 batch loss: 43.627739 batch xy loss 6.4039278 batch wh loss 8.16575623 batch obj loss 17.5307503 batch_class_loss 11.5273066 epoch total loss: 40216.6055\n",
            "Trained batch: 350 batch loss: 48.1604 batch xy loss 7.16338348 batch wh loss 9.47012234 batch obj loss 18.2873592 batch_class_loss 13.2395325 epoch total loss: 40264.7656\n",
            "Trained batch: 351 batch loss: 49.5706482 batch xy loss 6.93081522 batch wh loss 10.2329102 batch obj loss 19.3242378 batch_class_loss 13.0826807 epoch total loss: 40314.3359\n",
            "Trained batch: 352 batch loss: 52.2800598 batch xy loss 7.71580696 batch wh loss 10.3972168 batch obj loss 20.3551311 batch_class_loss 13.8119049 epoch total loss: 40366.6172\n",
            "Trained batch: 353 batch loss: 52.4738235 batch xy loss 7.06411362 batch wh loss 10.264431 batch obj loss 20.8864975 batch_class_loss 14.2587814 epoch total loss: 40419.0898\n",
            "Trained batch: 354 batch loss: 44.6620178 batch xy loss 6.43675041 batch wh loss 8.06812286 batch obj loss 18.7615967 batch_class_loss 11.3955488 epoch total loss: 40463.75\n",
            "Trained batch: 355 batch loss: 43.6044502 batch xy loss 7.21647835 batch wh loss 7.42081213 batch obj loss 17.8409462 batch_class_loss 11.1262159 epoch total loss: 40507.3555\n",
            "Trained batch: 356 batch loss: 46.5327 batch xy loss 6.21489668 batch wh loss 10.813921 batch obj loss 18.1561069 batch_class_loss 11.3477764 epoch total loss: 40553.8867\n",
            "Trained batch: 357 batch loss: 51.1027679 batch xy loss 7.54668427 batch wh loss 10.9689474 batch obj loss 19.2792187 batch_class_loss 13.3079176 epoch total loss: 40604.9883\n",
            "Trained batch: 358 batch loss: 43.9620094 batch xy loss 7.24917 batch wh loss 7.70407391 batch obj loss 17.0565414 batch_class_loss 11.9522266 epoch total loss: 40648.9492\n",
            "Trained batch: 359 batch loss: 42.5437927 batch xy loss 6.67788076 batch wh loss 8.23066902 batch obj loss 16.5881348 batch_class_loss 11.0471096 epoch total loss: 40691.4922\n",
            "Trained batch: 360 batch loss: 37.1882324 batch xy loss 5.37924433 batch wh loss 7.22539568 batch obj loss 15.0290966 batch_class_loss 9.55449295 epoch total loss: 40728.6797\n",
            "Trained batch: 361 batch loss: 46.9073601 batch xy loss 7.68213892 batch wh loss 8.54487324 batch obj loss 18.4995289 batch_class_loss 12.1808186 epoch total loss: 40775.5859\n",
            "Trained batch: 362 batch loss: 56.2869148 batch xy loss 7.84771824 batch wh loss 12.6414032 batch obj loss 21.2427959 batch_class_loss 14.5549974 epoch total loss: 40831.8711\n",
            "Trained batch: 363 batch loss: 49.7925301 batch xy loss 7.43850946 batch wh loss 9.18090248 batch obj loss 18.9358978 batch_class_loss 14.2372198 epoch total loss: 40881.6641\n",
            "Trained batch: 364 batch loss: 39.2928276 batch xy loss 6.0793 batch wh loss 8.20814323 batch obj loss 15.5042524 batch_class_loss 9.50113106 epoch total loss: 40920.957\n",
            "Trained batch: 365 batch loss: 62.9015121 batch xy loss 9.20307 batch wh loss 13.5069103 batch obj loss 23.1892338 batch_class_loss 17.0022964 epoch total loss: 40983.8594\n",
            "Trained batch: 366 batch loss: 50.3405 batch xy loss 7.26432896 batch wh loss 10.8027945 batch obj loss 19.0219612 batch_class_loss 13.2514172 epoch total loss: 41034.2\n",
            "Trained batch: 367 batch loss: 47.9162178 batch xy loss 6.55209255 batch wh loss 10.778616 batch obj loss 18.7478676 batch_class_loss 11.8376474 epoch total loss: 41082.1172\n",
            "Trained batch: 368 batch loss: 51.4288635 batch xy loss 7.13575697 batch wh loss 11.9125013 batch obj loss 19.3764362 batch_class_loss 13.0041666 epoch total loss: 41133.5469\n",
            "Trained batch: 369 batch loss: 57.7461 batch xy loss 7.92850208 batch wh loss 11.1253672 batch obj loss 22.3241634 batch_class_loss 16.3680706 epoch total loss: 41191.293\n",
            "Trained batch: 370 batch loss: 45.1059418 batch xy loss 6.59370327 batch wh loss 9.52694798 batch obj loss 17.3032761 batch_class_loss 11.6820164 epoch total loss: 41236.4\n",
            "Trained batch: 371 batch loss: 42.5022469 batch xy loss 6.3660984 batch wh loss 8.45261383 batch obj loss 16.7211056 batch_class_loss 10.962431 epoch total loss: 41278.9023\n",
            "Trained batch: 372 batch loss: 43.6726685 batch xy loss 6.40317774 batch wh loss 8.61894226 batch obj loss 17.4474392 batch_class_loss 11.2031126 epoch total loss: 41322.5742\n",
            "Trained batch: 373 batch loss: 48.5727081 batch xy loss 7.70717573 batch wh loss 10.0491228 batch obj loss 18.5600777 batch_class_loss 12.2563334 epoch total loss: 41371.1484\n",
            "Trained batch: 374 batch loss: 57.8981056 batch xy loss 8.56934929 batch wh loss 13.2477951 batch obj loss 21.223053 batch_class_loss 14.8579054 epoch total loss: 41429.0469\n",
            "Trained batch: 375 batch loss: 57.6935959 batch xy loss 8.31217194 batch wh loss 11.16012 batch obj loss 22.5154934 batch_class_loss 15.7058125 epoch total loss: 41486.7422\n",
            "Trained batch: 376 batch loss: 48.7509384 batch xy loss 7.07928276 batch wh loss 10.1200838 batch obj loss 19.1733551 batch_class_loss 12.378212 epoch total loss: 41535.4922\n",
            "Trained batch: 377 batch loss: 47.4266548 batch xy loss 7.1937747 batch wh loss 10.8278866 batch obj loss 18.19104 batch_class_loss 11.2139587 epoch total loss: 41582.918\n",
            "Trained batch: 378 batch loss: 41.1943855 batch xy loss 6.18052912 batch wh loss 8.68230343 batch obj loss 16.2059784 batch_class_loss 10.1255703 epoch total loss: 41624.1133\n",
            "Trained batch: 379 batch loss: 37.8346214 batch xy loss 5.91929245 batch wh loss 7.28171062 batch obj loss 14.744751 batch_class_loss 9.88887 epoch total loss: 41661.9492\n",
            "Trained batch: 380 batch loss: 44.2702026 batch xy loss 6.49316788 batch wh loss 9.64529419 batch obj loss 16.9960461 batch_class_loss 11.1356993 epoch total loss: 41706.2188\n",
            "Trained batch: 381 batch loss: 43.0592728 batch xy loss 5.99090528 batch wh loss 7.71821356 batch obj loss 17.037487 batch_class_loss 12.312664 epoch total loss: 41749.2773\n",
            "Trained batch: 382 batch loss: 46.3491173 batch xy loss 6.86178303 batch wh loss 10.590827 batch obj loss 17.5616951 batch_class_loss 11.3348122 epoch total loss: 41795.625\n",
            "Trained batch: 383 batch loss: 47.5290451 batch xy loss 6.97781801 batch wh loss 11.3676109 batch obj loss 17.314045 batch_class_loss 11.8695679 epoch total loss: 41843.1523\n",
            "Trained batch: 384 batch loss: 48.7833977 batch xy loss 7.66765547 batch wh loss 9.39687729 batch obj loss 18.6284256 batch_class_loss 13.0904436 epoch total loss: 41891.9375\n",
            "Trained batch: 385 batch loss: 48.9377213 batch xy loss 7.52709866 batch wh loss 11.2418242 batch obj loss 17.4530792 batch_class_loss 12.7157154 epoch total loss: 41940.875\n",
            "Trained batch: 386 batch loss: 54.4069595 batch xy loss 7.50260115 batch wh loss 12.6945667 batch obj loss 20.2995605 batch_class_loss 13.9102297 epoch total loss: 41995.2812\n",
            "Trained batch: 387 batch loss: 44.4762573 batch xy loss 6.8066206 batch wh loss 9.4022131 batch obj loss 17.4556713 batch_class_loss 10.8117533 epoch total loss: 42039.7578\n",
            "Trained batch: 388 batch loss: 48.937767 batch xy loss 7.45332813 batch wh loss 9.02145 batch obj loss 19.7152157 batch_class_loss 12.7477713 epoch total loss: 42088.6953\n",
            "Trained batch: 389 batch loss: 58.6423836 batch xy loss 8.45925331 batch wh loss 11.7019501 batch obj loss 22.6278477 batch_class_loss 15.8533335 epoch total loss: 42147.3359\n",
            "Trained batch: 390 batch loss: 42.9516716 batch xy loss 6.75325823 batch wh loss 8.67960167 batch obj loss 17.7221107 batch_class_loss 9.79669857 epoch total loss: 42190.2891\n",
            "Trained batch: 391 batch loss: 51.506073 batch xy loss 6.84366322 batch wh loss 10.8436728 batch obj loss 20.4970531 batch_class_loss 13.3216782 epoch total loss: 42241.7969\n",
            "Trained batch: 392 batch loss: 46.2220383 batch xy loss 6.75787926 batch wh loss 8.61221504 batch obj loss 18.5867577 batch_class_loss 12.2651844 epoch total loss: 42288.0195\n",
            "Trained batch: 393 batch loss: 58.3050194 batch xy loss 8.32148933 batch wh loss 12.8266344 batch obj loss 21.761652 batch_class_loss 15.3952446 epoch total loss: 42346.3242\n",
            "Trained batch: 394 batch loss: 50.5585747 batch xy loss 8.02508926 batch wh loss 9.8328867 batch obj loss 19.8092461 batch_class_loss 12.8913536 epoch total loss: 42396.8828\n",
            "Trained batch: 395 batch loss: 38.1341705 batch xy loss 5.9986515 batch wh loss 7.55008602 batch obj loss 14.9396086 batch_class_loss 9.64582253 epoch total loss: 42435.0156\n",
            "Trained batch: 396 batch loss: 46.8997841 batch xy loss 7.01431131 batch wh loss 8.29077339 batch obj loss 18.7023106 batch_class_loss 12.8923903 epoch total loss: 42481.9141\n",
            "Trained batch: 397 batch loss: 45.8989525 batch xy loss 6.08940363 batch wh loss 9.85251141 batch obj loss 18.2101955 batch_class_loss 11.7468395 epoch total loss: 42527.8125\n",
            "Trained batch: 398 batch loss: 52.2073784 batch xy loss 6.93584061 batch wh loss 11.7565556 batch obj loss 19.5231895 batch_class_loss 13.9917965 epoch total loss: 42580.0195\n",
            "Trained batch: 399 batch loss: 47.9799271 batch xy loss 6.90348911 batch wh loss 11.747283 batch obj loss 17.4215641 batch_class_loss 11.9075956 epoch total loss: 42628\n",
            "Trained batch: 400 batch loss: 46.6661339 batch xy loss 7.62670422 batch wh loss 7.81596375 batch obj loss 18.3054276 batch_class_loss 12.9180403 epoch total loss: 42674.668\n",
            "Trained batch: 401 batch loss: 51.3226624 batch xy loss 7.88554287 batch wh loss 9.84266758 batch obj loss 19.579155 batch_class_loss 14.015295 epoch total loss: 42725.9922\n",
            "Trained batch: 402 batch loss: 49.2155533 batch xy loss 7.76251125 batch wh loss 9.53739834 batch obj loss 18.7327652 batch_class_loss 13.1828785 epoch total loss: 42775.207\n",
            "Trained batch: 403 batch loss: 54.586586 batch xy loss 7.57019424 batch wh loss 11.4110508 batch obj loss 21.2963333 batch_class_loss 14.3090057 epoch total loss: 42829.793\n",
            "Trained batch: 404 batch loss: 54.3344765 batch xy loss 7.93348 batch wh loss 12.7395 batch obj loss 19.7659359 batch_class_loss 13.8955631 epoch total loss: 42884.1289\n",
            "Trained batch: 405 batch loss: 42.1031609 batch xy loss 6.35001087 batch wh loss 9.01024628 batch obj loss 16.426609 batch_class_loss 10.3162937 epoch total loss: 42926.2305\n",
            "Trained batch: 406 batch loss: 39.5197639 batch xy loss 5.94648647 batch wh loss 7.83446121 batch obj loss 15.5505476 batch_class_loss 10.1882668 epoch total loss: 42965.75\n",
            "Trained batch: 407 batch loss: 48.213974 batch xy loss 7.555758 batch wh loss 9.47217751 batch obj loss 18.8282185 batch_class_loss 12.3578186 epoch total loss: 43013.9648\n",
            "Trained batch: 408 batch loss: 50.3934 batch xy loss 6.61729717 batch wh loss 12.1017265 batch obj loss 19.4695187 batch_class_loss 12.2048559 epoch total loss: 43064.3594\n",
            "Trained batch: 409 batch loss: 46.9258537 batch xy loss 6.80188656 batch wh loss 10.5000525 batch obj loss 18.0607643 batch_class_loss 11.5631495 epoch total loss: 43111.2852\n",
            "Trained batch: 410 batch loss: 54.5621033 batch xy loss 7.88222694 batch wh loss 12.7146816 batch obj loss 20.1932526 batch_class_loss 13.771944 epoch total loss: 43165.8477\n",
            "Trained batch: 411 batch loss: 47.3735504 batch xy loss 7.90169621 batch wh loss 7.80240917 batch obj loss 18.9592628 batch_class_loss 12.7101822 epoch total loss: 43213.2227\n",
            "Trained batch: 412 batch loss: 43.610054 batch xy loss 7.0624342 batch wh loss 8.55808735 batch obj loss 17.0706177 batch_class_loss 10.9189148 epoch total loss: 43256.832\n",
            "Trained batch: 413 batch loss: 41.0334511 batch xy loss 6.21996355 batch wh loss 8.14462852 batch obj loss 16.451952 batch_class_loss 10.2169037 epoch total loss: 43297.8672\n",
            "Trained batch: 414 batch loss: 46.0947609 batch xy loss 7.32256603 batch wh loss 9.39787 batch obj loss 17.8749199 batch_class_loss 11.499403 epoch total loss: 43343.9609\n",
            "Trained batch: 415 batch loss: 47.1582947 batch xy loss 7.05390835 batch wh loss 9.42724705 batch obj loss 18.8872871 batch_class_loss 11.7898474 epoch total loss: 43391.1211\n",
            "Trained batch: 416 batch loss: 41.2533722 batch xy loss 5.94042206 batch wh loss 8.22110748 batch obj loss 16.4683857 batch_class_loss 10.623457 epoch total loss: 43432.375\n",
            "Trained batch: 417 batch loss: 43.6802711 batch xy loss 5.73711395 batch wh loss 9.90117931 batch obj loss 17.0926056 batch_class_loss 10.9493732 epoch total loss: 43476.0547\n",
            "Trained batch: 418 batch loss: 55.3287926 batch xy loss 8.27290249 batch wh loss 12.4583273 batch obj loss 19.9861336 batch_class_loss 14.6114292 epoch total loss: 43531.3828\n",
            "Trained batch: 419 batch loss: 39.6113091 batch xy loss 5.92464542 batch wh loss 7.84804535 batch obj loss 15.5402498 batch_class_loss 10.2983713 epoch total loss: 43570.9922\n",
            "Trained batch: 420 batch loss: 52.2388077 batch xy loss 7.18595409 batch wh loss 12.5301485 batch obj loss 18.5037041 batch_class_loss 14.0190048 epoch total loss: 43623.2305\n",
            "Trained batch: 421 batch loss: 42.2919846 batch xy loss 6.04090261 batch wh loss 8.6565876 batch obj loss 16.5835667 batch_class_loss 11.0109282 epoch total loss: 43665.5234\n",
            "Trained batch: 422 batch loss: 44.5415192 batch xy loss 6.27370739 batch wh loss 8.94110107 batch obj loss 17.6854 batch_class_loss 11.6413126 epoch total loss: 43710.0664\n",
            "Trained batch: 423 batch loss: 46.7044067 batch xy loss 7.51565 batch wh loss 9.61377048 batch obj loss 17.6336803 batch_class_loss 11.941309 epoch total loss: 43756.7695\n",
            "Trained batch: 424 batch loss: 42.0075874 batch xy loss 6.16551781 batch wh loss 7.9874053 batch obj loss 16.591795 batch_class_loss 11.2628746 epoch total loss: 43798.7773\n",
            "Trained batch: 425 batch loss: 48.3785172 batch xy loss 7.21512794 batch wh loss 10.4518147 batch obj loss 18.1516075 batch_class_loss 12.5599709 epoch total loss: 43847.1562\n",
            "Trained batch: 426 batch loss: 42.048336 batch xy loss 6.71349525 batch wh loss 7.76643229 batch obj loss 16.5977516 batch_class_loss 10.9706564 epoch total loss: 43889.2031\n",
            "Trained batch: 427 batch loss: 44.2626572 batch xy loss 6.59807396 batch wh loss 8.32863903 batch obj loss 18.0419559 batch_class_loss 11.2939835 epoch total loss: 43933.4648\n",
            "Trained batch: 428 batch loss: 35.6291313 batch xy loss 4.9255476 batch wh loss 5.15962362 batch obj loss 15.5756283 batch_class_loss 9.96833229 epoch total loss: 43969.0938\n",
            "Trained batch: 429 batch loss: 50.8857498 batch xy loss 7.45998669 batch wh loss 10.3240843 batch obj loss 19.0688763 batch_class_loss 14.0327988 epoch total loss: 44019.9805\n",
            "Trained batch: 430 batch loss: 50.6733665 batch xy loss 7.96103668 batch wh loss 9.97250938 batch obj loss 19.5112324 batch_class_loss 13.22859 epoch total loss: 44070.6523\n",
            "Trained batch: 431 batch loss: 48.3139648 batch xy loss 6.35430861 batch wh loss 13.0515995 batch obj loss 17.2611618 batch_class_loss 11.6468954 epoch total loss: 44118.9648\n",
            "Trained batch: 432 batch loss: 42.0083694 batch xy loss 6.08298206 batch wh loss 8.66438198 batch obj loss 16.2115898 batch_class_loss 11.0494146 epoch total loss: 44160.9727\n",
            "Trained batch: 433 batch loss: 14.6624947 batch xy loss 1.82114172 batch wh loss 2.46659851 batch obj loss 7.00306892 batch_class_loss 3.37168503 epoch total loss: 44175.6367\n",
            "20201008-154324 Epoch 1 train loss 102.02225494384766, total train batches 433, 79.87947845458984 examples per second\n",
            "1 , 7.506265 , 49.580204 , 15.070122 , 29.865644 , 102.022255 , 21.110283\n",
            "\n",
            "20201008-154540 Epoch 1 val loss 21.11028289794922, total val batches 191, 90.25227355957031 examples per second\n",
            "Model /content/gdrive/My Drive/results/epoch-1-loss-21.110.h5 saved.\n",
            "20201008-154540 Started epoch 2 with learning rate 0.01. Current LR patience count is 1 epochs. Last lowest train loss is 102.02225494384766. Last lowest val loss is 21.11028289794922.\n",
            "Trained batch: 1 batch loss: 50.6100159 batch xy loss 8.01719666 batch wh loss 11.5364943 batch obj loss 18.8316593 batch_class_loss 12.2246628 epoch total loss: 50.6100159\n",
            "Trained batch: 2 batch loss: 39.1736374 batch xy loss 5.6877594 batch wh loss 7.49681234 batch obj loss 15.7998543 batch_class_loss 10.1892099 epoch total loss: 89.7836533\n",
            "Trained batch: 3 batch loss: 61.4391289 batch xy loss 8.67645645 batch wh loss 13.6367416 batch obj loss 22.2716331 batch_class_loss 16.8543 epoch total loss: 151.222778\n",
            "Trained batch: 4 batch loss: 46.1894913 batch xy loss 6.89438915 batch wh loss 8.38082886 batch obj loss 18.5983429 batch_class_loss 12.3159342 epoch total loss: 197.412262\n",
            "Trained batch: 5 batch loss: 49.9757118 batch xy loss 6.92863226 batch wh loss 10.7298269 batch obj loss 19.2792187 batch_class_loss 13.0380344 epoch total loss: 247.38797\n",
            "Trained batch: 6 batch loss: 41.5256 batch xy loss 6.07823563 batch wh loss 8.34100723 batch obj loss 16.6287537 batch_class_loss 10.4776039 epoch total loss: 288.913574\n",
            "Trained batch: 7 batch loss: 45.2566261 batch xy loss 6.2630744 batch wh loss 9.76160431 batch obj loss 17.6037445 batch_class_loss 11.6281977 epoch total loss: 334.170197\n",
            "Trained batch: 8 batch loss: 41.9303741 batch xy loss 5.42806625 batch wh loss 10.6400919 batch obj loss 15.9908218 batch_class_loss 9.87139606 epoch total loss: 376.100586\n",
            "Trained batch: 9 batch loss: 47.2082672 batch xy loss 7.07547855 batch wh loss 9.03681469 batch obj loss 18.7181568 batch_class_loss 12.3778133 epoch total loss: 423.308838\n",
            "Trained batch: 10 batch loss: 43.4452171 batch xy loss 6.91379547 batch wh loss 7.61243582 batch obj loss 17.5349236 batch_class_loss 11.3840628 epoch total loss: 466.754059\n",
            "Trained batch: 11 batch loss: 47.3861504 batch xy loss 7.46208239 batch wh loss 8.74256134 batch obj loss 18.917284 batch_class_loss 12.2642212 epoch total loss: 514.140198\n",
            "Trained batch: 12 batch loss: 51.6694145 batch xy loss 7.03248072 batch wh loss 10.9164505 batch obj loss 19.592762 batch_class_loss 14.1277218 epoch total loss: 565.809631\n",
            "Trained batch: 13 batch loss: 46.4325104 batch xy loss 7.44447422 batch wh loss 8.44028091 batch obj loss 17.7820873 batch_class_loss 12.7656689 epoch total loss: 612.242126\n",
            "Trained batch: 14 batch loss: 42.7766113 batch xy loss 6.71946239 batch wh loss 8.15018177 batch obj loss 17.4904919 batch_class_loss 10.4164753 epoch total loss: 655.018738\n",
            "Trained batch: 15 batch loss: 42.7889786 batch xy loss 5.95796108 batch wh loss 8.87851143 batch obj loss 16.7852631 batch_class_loss 11.167244 epoch total loss: 697.807739\n",
            "Trained batch: 16 batch loss: 58.5442619 batch xy loss 8.72295952 batch wh loss 11.8109665 batch obj loss 22.4051628 batch_class_loss 15.6051674 epoch total loss: 756.352\n",
            "Trained batch: 17 batch loss: 42.3699 batch xy loss 5.59892797 batch wh loss 8.94455528 batch obj loss 17.3080616 batch_class_loss 10.5183563 epoch total loss: 798.721863\n",
            "Trained batch: 18 batch loss: 51.4054108 batch xy loss 7.64938974 batch wh loss 13.2771521 batch obj loss 17.838398 batch_class_loss 12.6404705 epoch total loss: 850.127258\n",
            "Trained batch: 19 batch loss: 41.1307297 batch xy loss 5.87393856 batch wh loss 7.91832161 batch obj loss 16.2511711 batch_class_loss 11.0872965 epoch total loss: 891.258\n",
            "Trained batch: 20 batch loss: 37.5892105 batch xy loss 5.76340199 batch wh loss 6.87856722 batch obj loss 15.3573399 batch_class_loss 9.58989906 epoch total loss: 928.847229\n",
            "Trained batch: 21 batch loss: 45.512104 batch xy loss 6.19046545 batch wh loss 11.023736 batch obj loss 16.5184841 batch_class_loss 11.7794209 epoch total loss: 974.359314\n",
            "Trained batch: 22 batch loss: 51.5607 batch xy loss 6.24947166 batch wh loss 12.2458677 batch obj loss 19.5990505 batch_class_loss 13.4663086 epoch total loss: 1025.92\n",
            "Trained batch: 23 batch loss: 58.7462234 batch xy loss 8.89042759 batch wh loss 12.1015959 batch obj loss 21.4971161 batch_class_loss 16.2570877 epoch total loss: 1084.66626\n",
            "Trained batch: 24 batch loss: 54.0946198 batch xy loss 7.18927908 batch wh loss 11.2753201 batch obj loss 20.5173664 batch_class_loss 15.1126556 epoch total loss: 1138.76086\n",
            "Trained batch: 25 batch loss: 46.8548546 batch xy loss 7.56036472 batch wh loss 8.182374 batch obj loss 18.7590866 batch_class_loss 12.3530331 epoch total loss: 1185.61572\n",
            "Trained batch: 26 batch loss: 42.2624512 batch xy loss 6.76971626 batch wh loss 6.73109531 batch obj loss 17.4231377 batch_class_loss 11.3384991 epoch total loss: 1227.87817\n",
            "Trained batch: 27 batch loss: 43.3378143 batch xy loss 6.0325346 batch wh loss 7.83109903 batch obj loss 17.7355404 batch_class_loss 11.7386417 epoch total loss: 1271.21594\n",
            "Trained batch: 28 batch loss: 33.6811943 batch xy loss 4.3985343 batch wh loss 5.29624748 batch obj loss 15.1981974 batch_class_loss 8.78821278 epoch total loss: 1304.89709\n",
            "Trained batch: 29 batch loss: 48.4990654 batch xy loss 7.90808296 batch wh loss 10.357872 batch obj loss 18.6991558 batch_class_loss 11.5339527 epoch total loss: 1353.39612\n",
            "Trained batch: 30 batch loss: 49.5066681 batch xy loss 7.82910633 batch wh loss 9.64282894 batch obj loss 19.4728661 batch_class_loss 12.5618658 epoch total loss: 1402.90283\n",
            "Trained batch: 31 batch loss: 53.7838516 batch xy loss 7.89555836 batch wh loss 12.2362213 batch obj loss 20.5821095 batch_class_loss 13.0699587 epoch total loss: 1456.68665\n",
            "Trained batch: 32 batch loss: 45.6417732 batch xy loss 6.43658161 batch wh loss 9.5566 batch obj loss 18.0955486 batch_class_loss 11.5530405 epoch total loss: 1502.32837\n",
            "Trained batch: 33 batch loss: 45.8080788 batch xy loss 7.51024199 batch wh loss 7.40342522 batch obj loss 18.8178539 batch_class_loss 12.07656 epoch total loss: 1548.13647\n",
            "Trained batch: 34 batch loss: 47.742157 batch xy loss 7.45000935 batch wh loss 8.70907593 batch obj loss 18.7869644 batch_class_loss 12.7961082 epoch total loss: 1595.87866\n",
            "Trained batch: 35 batch loss: 35.8142 batch xy loss 5.48125172 batch wh loss 5.74447632 batch obj loss 15.5731936 batch_class_loss 9.01527596 epoch total loss: 1631.69287\n",
            "Trained batch: 36 batch loss: 40.8648605 batch xy loss 6.1182394 batch wh loss 8.17475 batch obj loss 15.7631912 batch_class_loss 10.8086777 epoch total loss: 1672.55774\n",
            "Trained batch: 37 batch loss: 41.034523 batch xy loss 5.86890745 batch wh loss 8.34114361 batch obj loss 15.3851013 batch_class_loss 11.4393711 epoch total loss: 1713.59229\n",
            "Trained batch: 38 batch loss: 36.0919075 batch xy loss 5.40435696 batch wh loss 5.961411 batch obj loss 14.5171604 batch_class_loss 10.2089787 epoch total loss: 1749.6842\n",
            "Trained batch: 39 batch loss: 47.4687042 batch xy loss 7.04333115 batch wh loss 10.4111042 batch obj loss 17.6944332 batch_class_loss 12.3198338 epoch total loss: 1797.15295\n",
            "Trained batch: 40 batch loss: 42.1032219 batch xy loss 6.05024099 batch wh loss 8.95590591 batch obj loss 16.2049084 batch_class_loss 10.8921652 epoch total loss: 1839.25623\n",
            "Trained batch: 41 batch loss: 41.0620308 batch xy loss 6.4974165 batch wh loss 7.65938234 batch obj loss 15.7257957 batch_class_loss 11.1794357 epoch total loss: 1880.31824\n",
            "Trained batch: 42 batch loss: 43.6720123 batch xy loss 6.10275841 batch wh loss 7.93531275 batch obj loss 18.0159588 batch_class_loss 11.6179829 epoch total loss: 1923.99023\n",
            "Trained batch: 43 batch loss: 48.4490738 batch xy loss 7.90847301 batch wh loss 11.3217812 batch obj loss 17.2043457 batch_class_loss 12.0144749 epoch total loss: 1972.43933\n",
            "Trained batch: 44 batch loss: 41.1770134 batch xy loss 6.26258421 batch wh loss 8.70579529 batch obj loss 15.9006195 batch_class_loss 10.3080158 epoch total loss: 2013.61633\n",
            "Trained batch: 45 batch loss: 51.7137642 batch xy loss 7.44806099 batch wh loss 10.3188267 batch obj loss 19.7885799 batch_class_loss 14.1582975 epoch total loss: 2065.33\n",
            "Trained batch: 46 batch loss: 39.7613525 batch xy loss 5.88817024 batch wh loss 7.3473506 batch obj loss 16.1472054 batch_class_loss 10.3786211 epoch total loss: 2105.09131\n",
            "Trained batch: 47 batch loss: 41.0680389 batch xy loss 6.62471867 batch wh loss 7.77526045 batch obj loss 16.0156307 batch_class_loss 10.6524267 epoch total loss: 2146.15942\n",
            "Trained batch: 48 batch loss: 50.0814819 batch xy loss 7.72771311 batch wh loss 10.3026285 batch obj loss 19.4381 batch_class_loss 12.61304 epoch total loss: 2196.24097\n",
            "Trained batch: 49 batch loss: 47.8371658 batch xy loss 6.85535192 batch wh loss 9.50785828 batch obj loss 18.8007011 batch_class_loss 12.673255 epoch total loss: 2244.07812\n",
            "Trained batch: 50 batch loss: 43.7780724 batch xy loss 5.84539127 batch wh loss 9.89923286 batch obj loss 17.7795868 batch_class_loss 10.2538595 epoch total loss: 2287.8562\n",
            "Trained batch: 51 batch loss: 44.8162498 batch xy loss 6.53576 batch wh loss 9.20447731 batch obj loss 17.8460941 batch_class_loss 11.2299204 epoch total loss: 2332.67236\n",
            "Trained batch: 52 batch loss: 39.7579689 batch xy loss 5.77723455 batch wh loss 8.0226326 batch obj loss 15.6633921 batch_class_loss 10.2947121 epoch total loss: 2372.43042\n",
            "Trained batch: 53 batch loss: 45.9970169 batch xy loss 7.0180397 batch wh loss 7.91801786 batch obj loss 17.9883747 batch_class_loss 13.072587 epoch total loss: 2418.42749\n",
            "Trained batch: 54 batch loss: 47.3529816 batch xy loss 7.82352161 batch wh loss 7.79988718 batch obj loss 18.7155457 batch_class_loss 13.0140276 epoch total loss: 2465.78052\n",
            "Trained batch: 55 batch loss: 37.2571716 batch xy loss 5.87998772 batch wh loss 7.24308729 batch obj loss 14.6278629 batch_class_loss 9.50623417 epoch total loss: 2503.0376\n",
            "Trained batch: 56 batch loss: 44.6876755 batch xy loss 6.93381 batch wh loss 7.96779537 batch obj loss 17.9143162 batch_class_loss 11.8717556 epoch total loss: 2547.72534\n",
            "Trained batch: 57 batch loss: 44.2616882 batch xy loss 6.59542894 batch wh loss 9.68924618 batch obj loss 16.3714581 batch_class_loss 11.6055584 epoch total loss: 2591.98706\n",
            "Trained batch: 58 batch loss: 43.1295319 batch xy loss 5.52220917 batch wh loss 9.24736404 batch obj loss 17.3308697 batch_class_loss 11.029089 epoch total loss: 2635.1167\n",
            "Trained batch: 59 batch loss: 40.4471169 batch xy loss 5.67737 batch wh loss 9.98004913 batch obj loss 15.6848755 batch_class_loss 9.10482407 epoch total loss: 2675.56372\n",
            "Trained batch: 60 batch loss: 53.5993881 batch xy loss 7.69691086 batch wh loss 11.6962509 batch obj loss 19.9729595 batch_class_loss 14.233264 epoch total loss: 2729.16309\n",
            "Trained batch: 61 batch loss: 40.9018326 batch xy loss 5.92088604 batch wh loss 7.79449511 batch obj loss 16.6570835 batch_class_loss 10.5293674 epoch total loss: 2770.06494\n",
            "Trained batch: 62 batch loss: 40.1928978 batch xy loss 6.48439 batch wh loss 7.41017103 batch obj loss 15.763341 batch_class_loss 10.534996 epoch total loss: 2810.25781\n",
            "Trained batch: 63 batch loss: 46.6451492 batch xy loss 7.36449528 batch wh loss 9.38351 batch obj loss 17.9164543 batch_class_loss 11.980689 epoch total loss: 2856.90308\n",
            "Trained batch: 64 batch loss: 40.907341 batch xy loss 5.52320576 batch wh loss 10.4988194 batch obj loss 15.1873093 batch_class_loss 9.69800854 epoch total loss: 2897.8103\n",
            "Trained batch: 65 batch loss: 37.2359085 batch xy loss 5.0024147 batch wh loss 8.11735058 batch obj loss 14.5985031 batch_class_loss 9.51764107 epoch total loss: 2935.04614\n",
            "Trained batch: 66 batch loss: 40.4011765 batch xy loss 6.35842276 batch wh loss 7.77447367 batch obj loss 15.7375927 batch_class_loss 10.5306911 epoch total loss: 2975.44727\n",
            "Trained batch: 67 batch loss: 42.6644363 batch xy loss 6.15270901 batch wh loss 10.7732801 batch obj loss 15.5279608 batch_class_loss 10.2104855 epoch total loss: 3018.11182\n",
            "Trained batch: 68 batch loss: 44.2127609 batch xy loss 5.85033274 batch wh loss 9.21782112 batch obj loss 17.0040703 batch_class_loss 12.1405363 epoch total loss: 3062.32446\n",
            "Trained batch: 69 batch loss: 54.7980919 batch xy loss 9.06508541 batch wh loss 10.2063875 batch obj loss 20.493042 batch_class_loss 15.0335779 epoch total loss: 3117.12256\n",
            "Trained batch: 70 batch loss: 44.4086609 batch xy loss 6.96115923 batch wh loss 9.44553566 batch obj loss 16.8118172 batch_class_loss 11.1901503 epoch total loss: 3161.53125\n",
            "Trained batch: 71 batch loss: 45.2947884 batch xy loss 6.92272854 batch wh loss 7.88621378 batch obj loss 18.4997158 batch_class_loss 11.9861336 epoch total loss: 3206.82593\n",
            "Trained batch: 72 batch loss: 40.13134 batch xy loss 5.68787479 batch wh loss 7.9319911 batch obj loss 16.5679893 batch_class_loss 9.94348335 epoch total loss: 3246.95728\n",
            "Trained batch: 73 batch loss: 44.5308456 batch xy loss 6.56149292 batch wh loss 8.91251755 batch obj loss 17.535677 batch_class_loss 11.5211592 epoch total loss: 3291.48804\n",
            "Trained batch: 74 batch loss: 50.9394073 batch xy loss 7.35260582 batch wh loss 11.8243885 batch obj loss 19.3813477 batch_class_loss 12.3810625 epoch total loss: 3342.42749\n",
            "Trained batch: 75 batch loss: 46.7223358 batch xy loss 7.30372429 batch wh loss 8.93888283 batch obj loss 18.552496 batch_class_loss 11.9272346 epoch total loss: 3389.15\n",
            "Trained batch: 76 batch loss: 44.4103775 batch xy loss 7.24573326 batch wh loss 8.38144302 batch obj loss 17.7626419 batch_class_loss 11.0205574 epoch total loss: 3433.5603\n",
            "Trained batch: 77 batch loss: 46.2293739 batch xy loss 6.32302856 batch wh loss 11.560111 batch obj loss 17.2539558 batch_class_loss 11.0922823 epoch total loss: 3479.78979\n",
            "Trained batch: 78 batch loss: 40.7785568 batch xy loss 5.9178443 batch wh loss 7.88577461 batch obj loss 16.0773506 batch_class_loss 10.8975887 epoch total loss: 3520.56836\n",
            "Trained batch: 79 batch loss: 43.0229912 batch xy loss 6.50289583 batch wh loss 8.43401527 batch obj loss 16.6871471 batch_class_loss 11.3989344 epoch total loss: 3563.59131\n",
            "Trained batch: 80 batch loss: 59.6495094 batch xy loss 7.70348263 batch wh loss 13.1474676 batch obj loss 22.0036011 batch_class_loss 16.7949562 epoch total loss: 3623.24072\n",
            "Trained batch: 81 batch loss: 48.2132454 batch xy loss 7.52591 batch wh loss 9.18166 batch obj loss 18.8802299 batch_class_loss 12.6254416 epoch total loss: 3671.45386\n",
            "Trained batch: 82 batch loss: 52.5023232 batch xy loss 7.93512154 batch wh loss 10.1758709 batch obj loss 20.236908 batch_class_loss 14.1544228 epoch total loss: 3723.9563\n",
            "Trained batch: 83 batch loss: 44.9540367 batch xy loss 6.56782722 batch wh loss 9.0726223 batch obj loss 17.1496716 batch_class_loss 12.1639109 epoch total loss: 3768.9104\n",
            "Trained batch: 84 batch loss: 34.5674706 batch xy loss 4.44374323 batch wh loss 7.15058517 batch obj loss 14.2084799 batch_class_loss 8.76466 epoch total loss: 3803.47778\n",
            "Trained batch: 85 batch loss: 42.1775627 batch xy loss 6.10606718 batch wh loss 8.10478401 batch obj loss 17.0255718 batch_class_loss 10.9411421 epoch total loss: 3845.65527\n",
            "Trained batch: 86 batch loss: 35.6897278 batch xy loss 5.05796099 batch wh loss 7.45543385 batch obj loss 13.9625874 batch_class_loss 9.21374893 epoch total loss: 3881.34497\n",
            "Trained batch: 87 batch loss: 54.0019913 batch xy loss 7.8882966 batch wh loss 10.4292021 batch obj loss 21.0885773 batch_class_loss 14.5959148 epoch total loss: 3935.34692\n",
            "Trained batch: 88 batch loss: 49.2873344 batch xy loss 7.69343758 batch wh loss 10.6861038 batch obj loss 18.8206654 batch_class_loss 12.0871296 epoch total loss: 3984.63428\n",
            "Trained batch: 89 batch loss: 44.8996735 batch xy loss 6.37942028 batch wh loss 9.97509193 batch obj loss 17.1837082 batch_class_loss 11.3614531 epoch total loss: 4029.53394\n",
            "Trained batch: 90 batch loss: 43.78088 batch xy loss 7.09718275 batch wh loss 8.30265808 batch obj loss 17.5664806 batch_class_loss 10.814559 epoch total loss: 4073.3147\n",
            "Trained batch: 91 batch loss: 54.0672073 batch xy loss 7.51959419 batch wh loss 12.5333395 batch obj loss 19.9072533 batch_class_loss 14.1070251 epoch total loss: 4127.38184\n",
            "Trained batch: 92 batch loss: 37.4223 batch xy loss 5.88333797 batch wh loss 6.91975355 batch obj loss 15.9150229 batch_class_loss 8.70418167 epoch total loss: 4164.8042\n",
            "Trained batch: 93 batch loss: 47.9082794 batch xy loss 6.12934 batch wh loss 15.2581863 batch obj loss 16.3017616 batch_class_loss 10.2189913 epoch total loss: 4212.7124\n",
            "Trained batch: 94 batch loss: 54.5662422 batch xy loss 8.57739162 batch wh loss 10.7613754 batch obj loss 20.8243408 batch_class_loss 14.4031382 epoch total loss: 4267.27881\n",
            "Trained batch: 95 batch loss: 49.6696243 batch xy loss 8.23562431 batch wh loss 8.67762947 batch obj loss 19.4573135 batch_class_loss 13.2990532 epoch total loss: 4316.94824\n",
            "Trained batch: 96 batch loss: 45.428894 batch xy loss 7.28321266 batch wh loss 8.64980698 batch obj loss 17.5482349 batch_class_loss 11.9476366 epoch total loss: 4362.37695\n",
            "Trained batch: 97 batch loss: 44.0325089 batch xy loss 6.33728123 batch wh loss 7.50844908 batch obj loss 17.7957439 batch_class_loss 12.391036 epoch total loss: 4406.40967\n",
            "Trained batch: 98 batch loss: 39.341 batch xy loss 5.3903513 batch wh loss 8.07255363 batch obj loss 15.3846655 batch_class_loss 10.4934282 epoch total loss: 4445.75049\n",
            "Trained batch: 99 batch loss: 45.1680031 batch xy loss 6.22919941 batch wh loss 8.90708637 batch obj loss 18.1741638 batch_class_loss 11.8575506 epoch total loss: 4490.91846\n",
            "Trained batch: 100 batch loss: 36.9182167 batch xy loss 5.45717716 batch wh loss 7.94473267 batch obj loss 14.2829628 batch_class_loss 9.23334408 epoch total loss: 4527.83691\n",
            "Trained batch: 101 batch loss: 64.2521667 batch xy loss 10.0227985 batch wh loss 12.9450111 batch obj loss 22.9959106 batch_class_loss 18.2884521 epoch total loss: 4592.08887\n",
            "Trained batch: 102 batch loss: 44.4328461 batch xy loss 6.54809141 batch wh loss 9.22600555 batch obj loss 16.8822327 batch_class_loss 11.7765169 epoch total loss: 4636.52148\n",
            "Trained batch: 103 batch loss: 41.3959 batch xy loss 6.76892185 batch wh loss 6.97212458 batch obj loss 17.2613506 batch_class_loss 10.3935 epoch total loss: 4677.91748\n",
            "Trained batch: 104 batch loss: 43.9792252 batch xy loss 6.61369324 batch wh loss 8.59708691 batch obj loss 16.7345486 batch_class_loss 12.0338955 epoch total loss: 4721.89648\n",
            "Trained batch: 105 batch loss: 41.3979492 batch xy loss 5.45634842 batch wh loss 8.55641651 batch obj loss 17.0277634 batch_class_loss 10.3574247 epoch total loss: 4763.29443\n",
            "Trained batch: 106 batch loss: 56.3818359 batch xy loss 8.49221802 batch wh loss 11.8389244 batch obj loss 20.8700962 batch_class_loss 15.1805964 epoch total loss: 4819.67627\n",
            "Trained batch: 107 batch loss: 43.8302 batch xy loss 6.16399384 batch wh loss 8.88255692 batch obj loss 17.5016422 batch_class_loss 11.2820091 epoch total loss: 4863.50635\n",
            "Trained batch: 108 batch loss: 55.7729263 batch xy loss 8.20519543 batch wh loss 11.6081104 batch obj loss 20.9747391 batch_class_loss 14.9848766 epoch total loss: 4919.2793\n",
            "Trained batch: 109 batch loss: 38.3967972 batch xy loss 6.01957178 batch wh loss 7.04476881 batch obj loss 15.6786823 batch_class_loss 9.65377235 epoch total loss: 4957.67627\n",
            "Trained batch: 110 batch loss: 60.9516869 batch xy loss 8.92022705 batch wh loss 13.5688343 batch obj loss 22.3973484 batch_class_loss 16.0652733 epoch total loss: 5018.62793\n",
            "Trained batch: 111 batch loss: 44.5907745 batch xy loss 7.38526249 batch wh loss 9.00540161 batch obj loss 17.1670971 batch_class_loss 11.0330105 epoch total loss: 5063.21875\n",
            "Trained batch: 112 batch loss: 45.2580757 batch xy loss 6.1832304 batch wh loss 9.58726788 batch obj loss 18.063076 batch_class_loss 11.4245024 epoch total loss: 5108.47705\n",
            "Trained batch: 113 batch loss: 43.8321648 batch xy loss 6.93643951 batch wh loss 8.32142639 batch obj loss 17.3687229 batch_class_loss 11.205574 epoch total loss: 5152.30908\n",
            "Trained batch: 114 batch loss: 42.4204636 batch xy loss 6.27148294 batch wh loss 7.68780565 batch obj loss 17.2431927 batch_class_loss 11.2179852 epoch total loss: 5194.72949\n",
            "Trained batch: 115 batch loss: 37.1227608 batch xy loss 5.65836143 batch wh loss 6.79383135 batch obj loss 15.3786964 batch_class_loss 9.29187202 epoch total loss: 5231.85205\n",
            "Trained batch: 116 batch loss: 45.0514145 batch xy loss 7.064785 batch wh loss 8.17956352 batch obj loss 17.9277725 batch_class_loss 11.8792944 epoch total loss: 5276.90332\n",
            "Trained batch: 117 batch loss: 53.5650978 batch xy loss 8.19728565 batch wh loss 12.0522614 batch obj loss 19.7797699 batch_class_loss 13.53578 epoch total loss: 5330.46826\n",
            "Trained batch: 118 batch loss: 58.1770401 batch xy loss 7.43895292 batch wh loss 12.4758749 batch obj loss 22.049572 batch_class_loss 16.212635 epoch total loss: 5388.64551\n",
            "Trained batch: 119 batch loss: 49.4116554 batch xy loss 7.07114697 batch wh loss 11.0290775 batch obj loss 18.2425385 batch_class_loss 13.0688934 epoch total loss: 5438.05713\n",
            "Trained batch: 120 batch loss: 55.3500214 batch xy loss 8.01977348 batch wh loss 11.165453 batch obj loss 21.628479 batch_class_loss 14.536314 epoch total loss: 5493.40723\n",
            "Trained batch: 121 batch loss: 39.5960617 batch xy loss 5.68982458 batch wh loss 8.61433125 batch obj loss 15.7455654 batch_class_loss 9.54633617 epoch total loss: 5533.00342\n",
            "Trained batch: 122 batch loss: 54.200531 batch xy loss 8.41297 batch wh loss 10.6031017 batch obj loss 20.9753838 batch_class_loss 14.2090826 epoch total loss: 5587.2041\n",
            "Trained batch: 123 batch loss: 44.505619 batch xy loss 7.20377064 batch wh loss 8.23323 batch obj loss 17.9594364 batch_class_loss 11.1091862 epoch total loss: 5631.71\n",
            "Trained batch: 124 batch loss: 49.6152687 batch xy loss 6.54393291 batch wh loss 12.6543627 batch obj loss 18.7469597 batch_class_loss 11.6700182 epoch total loss: 5681.3252\n",
            "Trained batch: 125 batch loss: 49.3934441 batch xy loss 6.80160284 batch wh loss 9.77647114 batch obj loss 19.0878201 batch_class_loss 13.7275496 epoch total loss: 5730.71875\n",
            "Trained batch: 126 batch loss: 39.2512932 batch xy loss 5.59991741 batch wh loss 7.73120737 batch obj loss 16.3102188 batch_class_loss 9.60994911 epoch total loss: 5769.97\n",
            "Trained batch: 127 batch loss: 43.1693878 batch xy loss 6.42485189 batch wh loss 8.7169466 batch obj loss 17.5589104 batch_class_loss 10.4686756 epoch total loss: 5813.13965\n",
            "Trained batch: 128 batch loss: 30.0067749 batch xy loss 4.43660069 batch wh loss 5.44189739 batch obj loss 13.130372 batch_class_loss 6.99790382 epoch total loss: 5843.14648\n",
            "Trained batch: 129 batch loss: 39.4203529 batch xy loss 5.75564671 batch wh loss 9.19551659 batch obj loss 15.1905909 batch_class_loss 9.27859688 epoch total loss: 5882.56689\n",
            "Trained batch: 130 batch loss: 34.2468109 batch xy loss 4.84840441 batch wh loss 6.34596539 batch obj loss 13.810667 batch_class_loss 9.24177361 epoch total loss: 5916.81348\n",
            "Trained batch: 131 batch loss: 44.7629433 batch xy loss 6.70627832 batch wh loss 8.82044315 batch obj loss 17.5795898 batch_class_loss 11.6566286 epoch total loss: 5961.57666\n",
            "Trained batch: 132 batch loss: 46.1770477 batch xy loss 6.70798397 batch wh loss 9.91303 batch obj loss 17.3876457 batch_class_loss 12.1683884 epoch total loss: 6007.75391\n",
            "Trained batch: 133 batch loss: 41.0291634 batch xy loss 5.51979399 batch wh loss 8.01341152 batch obj loss 16.2960663 batch_class_loss 11.199893 epoch total loss: 6048.7832\n",
            "Trained batch: 134 batch loss: 34.9117 batch xy loss 4.54029846 batch wh loss 7.03504562 batch obj loss 14.0388308 batch_class_loss 9.2975235 epoch total loss: 6083.69482\n",
            "Trained batch: 135 batch loss: 58.1934319 batch xy loss 8.86618614 batch wh loss 13.0271378 batch obj loss 20.9080582 batch_class_loss 15.3920507 epoch total loss: 6141.88818\n",
            "Trained batch: 136 batch loss: 41.1522255 batch xy loss 5.39651299 batch wh loss 8.86267757 batch obj loss 16.0383396 batch_class_loss 10.8546972 epoch total loss: 6183.04053\n",
            "Trained batch: 137 batch loss: 46.9012871 batch xy loss 7.13075542 batch wh loss 9.21094894 batch obj loss 18.4817123 batch_class_loss 12.0778704 epoch total loss: 6229.94189\n",
            "Trained batch: 138 batch loss: 54.9660492 batch xy loss 7.39655638 batch wh loss 12.4209213 batch obj loss 20.7507172 batch_class_loss 14.3978558 epoch total loss: 6284.90771\n",
            "Trained batch: 139 batch loss: 44.8372459 batch xy loss 7.82715559 batch wh loss 7.64916611 batch obj loss 17.5632782 batch_class_loss 11.7976465 epoch total loss: 6329.74512\n",
            "Trained batch: 140 batch loss: 42.8310051 batch xy loss 5.98176956 batch wh loss 9.44157124 batch obj loss 17.4080486 batch_class_loss 9.99961662 epoch total loss: 6372.57617\n",
            "Trained batch: 141 batch loss: 41.3924446 batch xy loss 5.88605 batch wh loss 10.070075 batch obj loss 15.7592411 batch_class_loss 9.67707443 epoch total loss: 6413.96875\n",
            "Trained batch: 142 batch loss: 46.7782249 batch xy loss 7.24240971 batch wh loss 9.41536427 batch obj loss 18.1625481 batch_class_loss 11.9579048 epoch total loss: 6460.74707\n",
            "Trained batch: 143 batch loss: 44.8840218 batch xy loss 6.0226841 batch wh loss 11.147994 batch obj loss 16.9701748 batch_class_loss 10.7431669 epoch total loss: 6505.63086\n",
            "Trained batch: 144 batch loss: 32.9069252 batch xy loss 4.92386293 batch wh loss 5.15636349 batch obj loss 13.7876205 batch_class_loss 9.03908 epoch total loss: 6538.5376\n",
            "Trained batch: 145 batch loss: 53.1024628 batch xy loss 7.56361294 batch wh loss 9.66941833 batch obj loss 20.5332413 batch_class_loss 15.3361921 epoch total loss: 6591.64\n",
            "Trained batch: 146 batch loss: 34.8448372 batch xy loss 5.04075623 batch wh loss 6.75303268 batch obj loss 14.1150198 batch_class_loss 8.93602943 epoch total loss: 6626.48486\n",
            "Trained batch: 147 batch loss: 42.7386 batch xy loss 7.36705208 batch wh loss 7.2028532 batch obj loss 16.2408409 batch_class_loss 11.9278555 epoch total loss: 6669.22363\n",
            "Trained batch: 148 batch loss: 46.6721916 batch xy loss 6.3503418 batch wh loss 9.69463253 batch obj loss 17.3324928 batch_class_loss 13.2947245 epoch total loss: 6715.896\n",
            "Trained batch: 149 batch loss: 43.4651566 batch xy loss 6.28708076 batch wh loss 9.03640652 batch obj loss 16.9806366 batch_class_loss 11.1610327 epoch total loss: 6759.36133\n",
            "Trained batch: 150 batch loss: 36.182457 batch xy loss 5.38425493 batch wh loss 6.9508667 batch obj loss 14.9453955 batch_class_loss 8.90193844 epoch total loss: 6795.54395\n",
            "Trained batch: 151 batch loss: 44.0132446 batch xy loss 5.81431961 batch wh loss 10.7018318 batch obj loss 16.3598537 batch_class_loss 11.1372414 epoch total loss: 6839.55713\n",
            "Trained batch: 152 batch loss: 40.2645073 batch xy loss 6.16593504 batch wh loss 7.02830505 batch obj loss 16.5557976 batch_class_loss 10.5144682 epoch total loss: 6879.82178\n",
            "Trained batch: 153 batch loss: 38.0693703 batch xy loss 6.0605197 batch wh loss 8.04347897 batch obj loss 14.7693052 batch_class_loss 9.1960659 epoch total loss: 6917.89111\n",
            "Trained batch: 154 batch loss: 49.8486557 batch xy loss 7.01283741 batch wh loss 13.3712139 batch obj loss 17.6324825 batch_class_loss 11.8321209 epoch total loss: 6967.73975\n",
            "Trained batch: 155 batch loss: 44.359951 batch xy loss 7.1045022 batch wh loss 9.86732197 batch obj loss 16.7459908 batch_class_loss 10.6421328 epoch total loss: 7012.09961\n",
            "Trained batch: 156 batch loss: 36.9583702 batch xy loss 5.39680958 batch wh loss 8.11939 batch obj loss 14.3252726 batch_class_loss 9.1169014 epoch total loss: 7049.05811\n",
            "Trained batch: 157 batch loss: 54.042038 batch xy loss 8.73906422 batch wh loss 11.3532763 batch obj loss 20.0168324 batch_class_loss 13.9328671 epoch total loss: 7103.1\n",
            "Trained batch: 158 batch loss: 37.5379601 batch xy loss 5.84542131 batch wh loss 7.81518173 batch obj loss 14.6558838 batch_class_loss 9.22147274 epoch total loss: 7140.63818\n",
            "Trained batch: 159 batch loss: 43.8611641 batch xy loss 6.79439068 batch wh loss 8.72979736 batch obj loss 16.7493629 batch_class_loss 11.5876141 epoch total loss: 7184.49951\n",
            "Trained batch: 160 batch loss: 49.9338074 batch xy loss 6.81102 batch wh loss 11.4900055 batch obj loss 18.9003735 batch_class_loss 12.7324066 epoch total loss: 7234.43311\n",
            "Trained batch: 161 batch loss: 55.0461769 batch xy loss 8.85625839 batch wh loss 11.2605419 batch obj loss 21.122879 batch_class_loss 13.8064957 epoch total loss: 7289.47949\n",
            "Trained batch: 162 batch loss: 46.7804298 batch xy loss 6.34493256 batch wh loss 10.0539122 batch obj loss 18.6911545 batch_class_loss 11.6904335 epoch total loss: 7336.26\n",
            "Trained batch: 163 batch loss: 48.2089767 batch xy loss 7.28986263 batch wh loss 9.94898 batch obj loss 19.2695808 batch_class_loss 11.7005529 epoch total loss: 7384.46875\n",
            "Trained batch: 164 batch loss: 46.9513664 batch xy loss 6.79121208 batch wh loss 11.3952255 batch obj loss 17.4817715 batch_class_loss 11.2831593 epoch total loss: 7431.42\n",
            "Trained batch: 165 batch loss: 37.4866142 batch xy loss 5.80708313 batch wh loss 6.7225275 batch obj loss 15.3830147 batch_class_loss 9.57399 epoch total loss: 7468.90674\n",
            "Trained batch: 166 batch loss: 37.4057541 batch xy loss 5.78753185 batch wh loss 8.16937733 batch obj loss 14.8141537 batch_class_loss 8.63469124 epoch total loss: 7506.3125\n",
            "Trained batch: 167 batch loss: 46.4582062 batch xy loss 7.12551165 batch wh loss 10.6814575 batch obj loss 16.9842014 batch_class_loss 11.6670332 epoch total loss: 7552.77051\n",
            "Trained batch: 168 batch loss: 45.7014694 batch xy loss 6.2758832 batch wh loss 10.0680695 batch obj loss 17.2199516 batch_class_loss 12.1375637 epoch total loss: 7598.47217\n",
            "Trained batch: 169 batch loss: 40.2264977 batch xy loss 5.723279 batch wh loss 7.48857117 batch obj loss 15.9221439 batch_class_loss 11.0925083 epoch total loss: 7638.69873\n",
            "Trained batch: 170 batch loss: 34.586235 batch xy loss 5.10911322 batch wh loss 5.71457958 batch obj loss 14.2147665 batch_class_loss 9.54777145 epoch total loss: 7673.28516\n",
            "Trained batch: 171 batch loss: 46.8302269 batch xy loss 6.41897297 batch wh loss 11.6921864 batch obj loss 17.0148 batch_class_loss 11.7042608 epoch total loss: 7720.11523\n",
            "Trained batch: 172 batch loss: 41.1878 batch xy loss 6.80291462 batch wh loss 8.61691856 batch obj loss 15.2373018 batch_class_loss 10.5306664 epoch total loss: 7761.30322\n",
            "Trained batch: 173 batch loss: 55.74049 batch xy loss 8.29153919 batch wh loss 12.482934 batch obj loss 20.0576897 batch_class_loss 14.9083261 epoch total loss: 7817.04395\n",
            "Trained batch: 174 batch loss: 41.1653786 batch xy loss 6.47511673 batch wh loss 7.44807148 batch obj loss 16.5269375 batch_class_loss 10.7152519 epoch total loss: 7858.20947\n",
            "Trained batch: 175 batch loss: 41.0496864 batch xy loss 6.26150227 batch wh loss 9.23271751 batch obj loss 15.3941 batch_class_loss 10.1613655 epoch total loss: 7899.25928\n",
            "Trained batch: 176 batch loss: 47.5485306 batch xy loss 6.71277046 batch wh loss 10.144002 batch obj loss 18.191 batch_class_loss 12.5007601 epoch total loss: 7946.80762\n",
            "Trained batch: 177 batch loss: 42.8983536 batch xy loss 6.06195307 batch wh loss 8.38982105 batch obj loss 17.5091915 batch_class_loss 10.9373846 epoch total loss: 7989.70605\n",
            "Trained batch: 178 batch loss: 47.2462311 batch xy loss 7.17925406 batch wh loss 10.0766287 batch obj loss 17.9085274 batch_class_loss 12.0818233 epoch total loss: 8036.95215\n",
            "Trained batch: 179 batch loss: 48.0991364 batch xy loss 7.21296787 batch wh loss 10.6954575 batch obj loss 18.1940403 batch_class_loss 11.9966698 epoch total loss: 8085.05127\n",
            "Trained batch: 180 batch loss: 49.3411026 batch xy loss 7.53794289 batch wh loss 9.32955265 batch obj loss 19.8086853 batch_class_loss 12.6649199 epoch total loss: 8134.39258\n",
            "Trained batch: 181 batch loss: 36.0927353 batch xy loss 5.41227531 batch wh loss 6.34336329 batch obj loss 15.6509342 batch_class_loss 8.68616295 epoch total loss: 8170.48535\n",
            "Trained batch: 182 batch loss: 46.4574928 batch xy loss 7.06573057 batch wh loss 9.96203423 batch obj loss 17.5696106 batch_class_loss 11.8601179 epoch total loss: 8216.94238\n",
            "Trained batch: 183 batch loss: 47.9525604 batch xy loss 7.58806658 batch wh loss 9.83909798 batch obj loss 18.3346481 batch_class_loss 12.1907501 epoch total loss: 8264.89453\n",
            "Trained batch: 184 batch loss: 50.5293922 batch xy loss 6.89054728 batch wh loss 10.9526529 batch obj loss 18.9998512 batch_class_loss 13.6863356 epoch total loss: 8315.42383\n",
            "Trained batch: 185 batch loss: 45.8505898 batch xy loss 6.81007671 batch wh loss 8.5863533 batch obj loss 18.7140636 batch_class_loss 11.7400932 epoch total loss: 8361.27441\n",
            "Trained batch: 186 batch loss: 47.7608376 batch xy loss 7.33509731 batch wh loss 9.39104 batch obj loss 18.521225 batch_class_loss 12.5134726 epoch total loss: 8409.03516\n",
            "Trained batch: 187 batch loss: 33.6152191 batch xy loss 5.40303898 batch wh loss 6.40836334 batch obj loss 13.7774286 batch_class_loss 8.02638817 epoch total loss: 8442.65\n",
            "Trained batch: 188 batch loss: 47.841526 batch xy loss 7.8300004 batch wh loss 9.11134911 batch obj loss 17.746048 batch_class_loss 13.1541262 epoch total loss: 8490.49219\n",
            "Trained batch: 189 batch loss: 49.1166306 batch xy loss 6.98777819 batch wh loss 10.1118336 batch obj loss 18.5296326 batch_class_loss 13.4873848 epoch total loss: 8539.6084\n",
            "Trained batch: 190 batch loss: 49.8512535 batch xy loss 7.7131114 batch wh loss 9.55445766 batch obj loss 19.4373131 batch_class_loss 13.1463795 epoch total loss: 8589.46\n",
            "Trained batch: 191 batch loss: 45.7869797 batch xy loss 7.79600143 batch wh loss 8.82511139 batch obj loss 17.4433784 batch_class_loss 11.7224865 epoch total loss: 8635.24707\n",
            "Trained batch: 192 batch loss: 45.8902168 batch xy loss 6.5838027 batch wh loss 8.5158987 batch obj loss 18.1102867 batch_class_loss 12.6802273 epoch total loss: 8681.1377\n",
            "Trained batch: 193 batch loss: 42.4872322 batch xy loss 6.35928726 batch wh loss 8.71129227 batch obj loss 16.2901211 batch_class_loss 11.1265297 epoch total loss: 8723.625\n",
            "Trained batch: 194 batch loss: 45.0640717 batch xy loss 6.86336279 batch wh loss 9.97252083 batch obj loss 17.5911465 batch_class_loss 10.637042 epoch total loss: 8768.68945\n",
            "Trained batch: 195 batch loss: 50.788 batch xy loss 7.42434502 batch wh loss 12.8453627 batch obj loss 18.8095093 batch_class_loss 11.7087841 epoch total loss: 8819.47754\n",
            "Trained batch: 196 batch loss: 49.5754662 batch xy loss 7.93038893 batch wh loss 11.1644754 batch obj loss 18.0115128 batch_class_loss 12.4690933 epoch total loss: 8869.05273\n",
            "Trained batch: 197 batch loss: 43.7939 batch xy loss 6.68277693 batch wh loss 8.88870144 batch obj loss 16.6983242 batch_class_loss 11.5240946 epoch total loss: 8912.84668\n",
            "Trained batch: 198 batch loss: 45.7709961 batch xy loss 6.89820671 batch wh loss 9.59254265 batch obj loss 18.1096249 batch_class_loss 11.1706228 epoch total loss: 8958.61719\n",
            "Trained batch: 199 batch loss: 53.578846 batch xy loss 7.67640781 batch wh loss 11.0961332 batch obj loss 20.4904099 batch_class_loss 14.3158951 epoch total loss: 9012.19629\n",
            "Trained batch: 200 batch loss: 42.3396835 batch xy loss 6.45744562 batch wh loss 8.32127857 batch obj loss 16.9639282 batch_class_loss 10.5970325 epoch total loss: 9054.53613\n",
            "Trained batch: 201 batch loss: 43.0419197 batch xy loss 6.31216908 batch wh loss 9.13712788 batch obj loss 16.6396255 batch_class_loss 10.9529972 epoch total loss: 9097.57812\n",
            "Trained batch: 202 batch loss: 42.9646339 batch xy loss 6.90826368 batch wh loss 7.00165272 batch obj loss 18.2237206 batch_class_loss 10.8309946 epoch total loss: 9140.54297\n",
            "Trained batch: 203 batch loss: 53.6154442 batch xy loss 8.49545288 batch wh loss 12.0762234 batch obj loss 20.5147953 batch_class_loss 12.5289736 epoch total loss: 9194.1582\n",
            "Trained batch: 204 batch loss: 45.6923 batch xy loss 7.22607231 batch wh loss 8.44503784 batch obj loss 17.8420868 batch_class_loss 12.1791019 epoch total loss: 9239.85059\n",
            "Trained batch: 205 batch loss: 51.4501228 batch xy loss 7.49469662 batch wh loss 9.50237274 batch obj loss 19.9482689 batch_class_loss 14.5047865 epoch total loss: 9291.30078\n",
            "Trained batch: 206 batch loss: 44.7697639 batch xy loss 6.90258217 batch wh loss 9.3899765 batch obj loss 17.0198174 batch_class_loss 11.457386 epoch total loss: 9336.07\n",
            "Trained batch: 207 batch loss: 45.5302963 batch xy loss 6.46461868 batch wh loss 8.0355711 batch obj loss 18.4445 batch_class_loss 12.5856094 epoch total loss: 9381.60059\n",
            "Trained batch: 208 batch loss: 42.273262 batch xy loss 6.58542156 batch wh loss 7.73363447 batch obj loss 17.0525322 batch_class_loss 10.9016762 epoch total loss: 9423.87402\n",
            "Trained batch: 209 batch loss: 43.3929672 batch xy loss 6.04800034 batch wh loss 9.51055241 batch obj loss 17.0023842 batch_class_loss 10.8320322 epoch total loss: 9467.2666\n",
            "Trained batch: 210 batch loss: 38.6863251 batch xy loss 5.92352057 batch wh loss 6.74984 batch obj loss 15.8365593 batch_class_loss 10.1764011 epoch total loss: 9505.95312\n",
            "Trained batch: 211 batch loss: 44.3111115 batch xy loss 6.23384571 batch wh loss 8.15336704 batch obj loss 17.6334114 batch_class_loss 12.2904911 epoch total loss: 9550.26465\n",
            "Trained batch: 212 batch loss: 52.2507248 batch xy loss 7.81728125 batch wh loss 10.9232283 batch obj loss 19.5053253 batch_class_loss 14.0048904 epoch total loss: 9602.51562\n",
            "Trained batch: 213 batch loss: 49.8290977 batch xy loss 7.63787079 batch wh loss 9.44467354 batch obj loss 19.1427612 batch_class_loss 13.603797 epoch total loss: 9652.34473\n",
            "Trained batch: 214 batch loss: 34.4122429 batch xy loss 4.92017651 batch wh loss 5.04572868 batch obj loss 14.7383966 batch_class_loss 9.70794201 epoch total loss: 9686.75684\n",
            "Trained batch: 215 batch loss: 52.3290787 batch xy loss 8.29665852 batch wh loss 10.099184 batch obj loss 19.6460438 batch_class_loss 14.2871962 epoch total loss: 9739.08594\n",
            "Trained batch: 216 batch loss: 48.5094452 batch xy loss 6.93280363 batch wh loss 10.1435509 batch obj loss 18.5961266 batch_class_loss 12.8369627 epoch total loss: 9787.5957\n",
            "Trained batch: 217 batch loss: 40.8747406 batch xy loss 5.67487144 batch wh loss 8.57204151 batch obj loss 17.0621681 batch_class_loss 9.56566 epoch total loss: 9828.4707\n",
            "Trained batch: 218 batch loss: 42.6611748 batch xy loss 6.35541058 batch wh loss 8.70601273 batch obj loss 16.956192 batch_class_loss 10.6435633 epoch total loss: 9871.13184\n",
            "Trained batch: 219 batch loss: 48.3391418 batch xy loss 6.86839724 batch wh loss 9.35928631 batch obj loss 19.494894 batch_class_loss 12.6165686 epoch total loss: 9919.4707\n",
            "Trained batch: 220 batch loss: 39.0151482 batch xy loss 5.85672617 batch wh loss 6.55844641 batch obj loss 16.3470516 batch_class_loss 10.2529287 epoch total loss: 9958.48633\n",
            "Trained batch: 221 batch loss: 48.9112892 batch xy loss 7.34320116 batch wh loss 12.7090254 batch obj loss 17.1775589 batch_class_loss 11.6815014 epoch total loss: 10007.3975\n",
            "Trained batch: 222 batch loss: 56.971344 batch xy loss 7.50987864 batch wh loss 17.2149525 batch obj loss 19.5453281 batch_class_loss 12.7011824 epoch total loss: 10064.3691\n",
            "Trained batch: 223 batch loss: 42.739048 batch xy loss 5.95407152 batch wh loss 9.37338829 batch obj loss 16.733572 batch_class_loss 10.6780186 epoch total loss: 10107.1084\n",
            "Trained batch: 224 batch loss: 40.6070099 batch xy loss 5.7976594 batch wh loss 8.04894829 batch obj loss 16.0127716 batch_class_loss 10.7476301 epoch total loss: 10147.7158\n",
            "Trained batch: 225 batch loss: 44.959259 batch xy loss 6.94491959 batch wh loss 8.77549 batch obj loss 18.1201363 batch_class_loss 11.1187115 epoch total loss: 10192.6748\n",
            "Trained batch: 226 batch loss: 39.8221321 batch xy loss 5.90765715 batch wh loss 7.08019352 batch obj loss 16.0735493 batch_class_loss 10.7607269 epoch total loss: 10232.4971\n",
            "Trained batch: 227 batch loss: 42.3142281 batch xy loss 6.54576111 batch wh loss 7.95195436 batch obj loss 16.644022 batch_class_loss 11.1724882 epoch total loss: 10274.8115\n",
            "Trained batch: 228 batch loss: 47.8837433 batch xy loss 6.7062583 batch wh loss 9.82299232 batch obj loss 18.1566601 batch_class_loss 13.1978359 epoch total loss: 10322.6953\n",
            "Trained batch: 229 batch loss: 42.4087181 batch xy loss 5.88271856 batch wh loss 7.43555355 batch obj loss 17.102993 batch_class_loss 11.9874563 epoch total loss: 10365.1045\n",
            "Trained batch: 230 batch loss: 53.3935776 batch xy loss 7.05818653 batch wh loss 14.4575529 batch obj loss 19.2134457 batch_class_loss 12.6643906 epoch total loss: 10418.498\n",
            "Trained batch: 231 batch loss: 33.2504539 batch xy loss 5.52657747 batch wh loss 6.50399828 batch obj loss 13.1164598 batch_class_loss 8.10341835 epoch total loss: 10451.748\n",
            "Trained batch: 232 batch loss: 55.8809128 batch xy loss 8.12337 batch wh loss 12.4428673 batch obj loss 20.332859 batch_class_loss 14.9818153 epoch total loss: 10507.6289\n",
            "Trained batch: 233 batch loss: 51.2878456 batch xy loss 7.41483498 batch wh loss 10.2072926 batch obj loss 19.5873699 batch_class_loss 14.0783472 epoch total loss: 10558.917\n",
            "Trained batch: 234 batch loss: 44.5695419 batch xy loss 6.97555113 batch wh loss 9.82342 batch obj loss 17.1360435 batch_class_loss 10.6345291 epoch total loss: 10603.4863\n",
            "Trained batch: 235 batch loss: 51.9412 batch xy loss 7.87309408 batch wh loss 12.5205021 batch obj loss 19.1038609 batch_class_loss 12.4437447 epoch total loss: 10655.4277\n",
            "Trained batch: 236 batch loss: 45.4789658 batch xy loss 6.43941689 batch wh loss 10.1769104 batch obj loss 17.8193188 batch_class_loss 11.0433168 epoch total loss: 10700.9062\n",
            "Trained batch: 237 batch loss: 48.0158806 batch xy loss 6.99079514 batch wh loss 10.3006039 batch obj loss 18.5372 batch_class_loss 12.1872826 epoch total loss: 10748.9219\n",
            "Trained batch: 238 batch loss: 44.2084846 batch xy loss 6.57749462 batch wh loss 8.31005573 batch obj loss 17.6514 batch_class_loss 11.6695328 epoch total loss: 10793.1299\n",
            "Trained batch: 239 batch loss: 36.6346626 batch xy loss 5.31502151 batch wh loss 6.71383476 batch obj loss 15.8655443 batch_class_loss 8.74026108 epoch total loss: 10829.7646\n",
            "Trained batch: 240 batch loss: 38.9662361 batch xy loss 6.01393 batch wh loss 7.19863272 batch obj loss 15.9875221 batch_class_loss 9.76615143 epoch total loss: 10868.7305\n",
            "Trained batch: 241 batch loss: 47.8727684 batch xy loss 6.0678215 batch wh loss 10.7352524 batch obj loss 18.151865 batch_class_loss 12.9178305 epoch total loss: 10916.6035\n",
            "Trained batch: 242 batch loss: 46.2261887 batch xy loss 7.03986645 batch wh loss 10.0159216 batch obj loss 17.3665962 batch_class_loss 11.8038025 epoch total loss: 10962.8301\n",
            "Trained batch: 243 batch loss: 35.8650475 batch xy loss 5.0226 batch wh loss 7.37004232 batch obj loss 14.1718464 batch_class_loss 9.30055809 epoch total loss: 10998.6953\n",
            "Trained batch: 244 batch loss: 41.2777901 batch xy loss 6.24181652 batch wh loss 8.86710644 batch obj loss 15.889246 batch_class_loss 10.2796211 epoch total loss: 11039.9727\n",
            "Trained batch: 245 batch loss: 51.7605743 batch xy loss 7.0575428 batch wh loss 13.6176453 batch obj loss 18.5574131 batch_class_loss 12.5279722 epoch total loss: 11091.7334\n",
            "Trained batch: 246 batch loss: 51.3214798 batch xy loss 8.24851131 batch wh loss 9.62661266 batch obj loss 19.4620094 batch_class_loss 13.9843464 epoch total loss: 11143.0547\n",
            "Trained batch: 247 batch loss: 38.6450348 batch xy loss 5.94824123 batch wh loss 7.9937048 batch obj loss 15.2771225 batch_class_loss 9.42596531 epoch total loss: 11181.7\n",
            "Trained batch: 248 batch loss: 37.9187469 batch xy loss 5.36266947 batch wh loss 6.60294151 batch obj loss 15.7820301 batch_class_loss 10.1711054 epoch total loss: 11219.6191\n",
            "Trained batch: 249 batch loss: 48.777565 batch xy loss 6.84466267 batch wh loss 10.1331091 batch obj loss 19.0438824 batch_class_loss 12.7559109 epoch total loss: 11268.3965\n",
            "Trained batch: 250 batch loss: 43.2948265 batch xy loss 5.81481457 batch wh loss 9.15310574 batch obj loss 16.9928818 batch_class_loss 11.3340244 epoch total loss: 11311.6914\n",
            "Trained batch: 251 batch loss: 41.4217567 batch xy loss 6.53746033 batch wh loss 7.89214373 batch obj loss 16.3365688 batch_class_loss 10.6555824 epoch total loss: 11353.1133\n",
            "Trained batch: 252 batch loss: 47.5368767 batch xy loss 6.87606812 batch wh loss 10.5187912 batch obj loss 18.0369911 batch_class_loss 12.1050262 epoch total loss: 11400.6504\n",
            "Trained batch: 253 batch loss: 51.1921501 batch xy loss 8.22683811 batch wh loss 10.2255383 batch obj loss 19.7609901 batch_class_loss 12.9787827 epoch total loss: 11451.8428\n",
            "Trained batch: 254 batch loss: 48.2207146 batch xy loss 7.79449081 batch wh loss 8.8719511 batch obj loss 19.0643177 batch_class_loss 12.489954 epoch total loss: 11500.0635\n",
            "Trained batch: 255 batch loss: 44.2974319 batch xy loss 6.17776108 batch wh loss 9.49001122 batch obj loss 17.6863747 batch_class_loss 10.9432793 epoch total loss: 11544.3613\n",
            "Trained batch: 256 batch loss: 40.2670174 batch xy loss 5.88226938 batch wh loss 8.01967335 batch obj loss 16.2033844 batch_class_loss 10.1616926 epoch total loss: 11584.6279\n",
            "Trained batch: 257 batch loss: 55.2288475 batch xy loss 8.41260242 batch wh loss 13.4524469 batch obj loss 19.885519 batch_class_loss 13.4782772 epoch total loss: 11639.8564\n",
            "Trained batch: 258 batch loss: 45.3386 batch xy loss 6.91899776 batch wh loss 9.19862 batch obj loss 17.9319878 batch_class_loss 11.2889948 epoch total loss: 11685.1953\n",
            "Trained batch: 259 batch loss: 43.9158592 batch xy loss 6.68068027 batch wh loss 9.50639915 batch obj loss 17.0875053 batch_class_loss 10.6412745 epoch total loss: 11729.1113\n",
            "Trained batch: 260 batch loss: 48.7131271 batch xy loss 7.32080841 batch wh loss 8.27882099 batch obj loss 20.2126484 batch_class_loss 12.9008427 epoch total loss: 11777.8242\n",
            "Trained batch: 261 batch loss: 43.3775597 batch xy loss 6.11577177 batch wh loss 7.77993584 batch obj loss 17.7285824 batch_class_loss 11.7532654 epoch total loss: 11821.2021\n",
            "Trained batch: 262 batch loss: 42.1100426 batch xy loss 5.91844511 batch wh loss 8.08495617 batch obj loss 17.1098118 batch_class_loss 10.9968319 epoch total loss: 11863.3125\n",
            "Trained batch: 263 batch loss: 45.0278931 batch xy loss 6.47309875 batch wh loss 10.2041292 batch obj loss 17.5562344 batch_class_loss 10.7944317 epoch total loss: 11908.3408\n",
            "Trained batch: 264 batch loss: 34.885128 batch xy loss 5.54164124 batch wh loss 5.97523117 batch obj loss 14.7131052 batch_class_loss 8.65515137 epoch total loss: 11943.2256\n",
            "Trained batch: 265 batch loss: 43.3955231 batch xy loss 6.58355665 batch wh loss 7.86219501 batch obj loss 17.1079636 batch_class_loss 11.8418083 epoch total loss: 11986.6211\n",
            "Trained batch: 266 batch loss: 41.5001869 batch xy loss 5.8047 batch wh loss 9.5048151 batch obj loss 15.5813627 batch_class_loss 10.6093092 epoch total loss: 12028.1211\n",
            "Trained batch: 267 batch loss: 43.8955116 batch xy loss 6.53658485 batch wh loss 9.05914402 batch obj loss 16.5612221 batch_class_loss 11.7385645 epoch total loss: 12072.0166\n",
            "Trained batch: 268 batch loss: 37.1302185 batch xy loss 5.50027657 batch wh loss 6.26384926 batch obj loss 14.9080906 batch_class_loss 10.4580021 epoch total loss: 12109.1465\n",
            "Trained batch: 269 batch loss: 36.0143166 batch xy loss 5.22857237 batch wh loss 7.85109234 batch obj loss 13.8265257 batch_class_loss 9.10812664 epoch total loss: 12145.1611\n",
            "Trained batch: 270 batch loss: 40.5213852 batch xy loss 6.50098419 batch wh loss 7.84281206 batch obj loss 15.7022734 batch_class_loss 10.4753132 epoch total loss: 12185.6826\n",
            "Trained batch: 271 batch loss: 53.0571899 batch xy loss 7.73272419 batch wh loss 10.8254662 batch obj loss 19.9467621 batch_class_loss 14.5522385 epoch total loss: 12238.7402\n",
            "Trained batch: 272 batch loss: 39.8923607 batch xy loss 6.2101469 batch wh loss 7.99792767 batch obj loss 15.2061977 batch_class_loss 10.4780884 epoch total loss: 12278.6328\n",
            "Trained batch: 273 batch loss: 46.3244629 batch xy loss 6.67936325 batch wh loss 8.91472435 batch obj loss 18.5642815 batch_class_loss 12.1660919 epoch total loss: 12324.957\n",
            "Trained batch: 274 batch loss: 46.1302795 batch xy loss 6.75876617 batch wh loss 9.61286926 batch obj loss 17.7919369 batch_class_loss 11.9667063 epoch total loss: 12371.0869\n",
            "Trained batch: 275 batch loss: 33.7114296 batch xy loss 5.72964859 batch wh loss 6.11083126 batch obj loss 13.9878674 batch_class_loss 7.88308144 epoch total loss: 12404.7988\n",
            "Trained batch: 276 batch loss: 43.3984566 batch xy loss 6.50760078 batch wh loss 8.34228 batch obj loss 17.3286095 batch_class_loss 11.2199669 epoch total loss: 12448.1973\n",
            "Trained batch: 277 batch loss: 38.4469833 batch xy loss 5.29649782 batch wh loss 7.40773726 batch obj loss 15.8252735 batch_class_loss 9.91748 epoch total loss: 12486.6445\n",
            "Trained batch: 278 batch loss: 59.1438179 batch xy loss 8.10348701 batch wh loss 13.1626129 batch obj loss 22.6285686 batch_class_loss 15.2491522 epoch total loss: 12545.7881\n",
            "Trained batch: 279 batch loss: 39.1190071 batch xy loss 5.61469173 batch wh loss 8.63645363 batch obj loss 15.3652649 batch_class_loss 9.5026 epoch total loss: 12584.9072\n",
            "Trained batch: 280 batch loss: 44.5684929 batch xy loss 6.62592316 batch wh loss 9.45405197 batch obj loss 17.2057858 batch_class_loss 11.2827301 epoch total loss: 12629.4756\n",
            "Trained batch: 281 batch loss: 40.1358833 batch xy loss 5.59776545 batch wh loss 8.46748 batch obj loss 15.6855574 batch_class_loss 10.3850803 epoch total loss: 12669.6113\n",
            "Trained batch: 282 batch loss: 51.6235046 batch xy loss 7.04716873 batch wh loss 11.6749954 batch obj loss 19.6818 batch_class_loss 13.2195387 epoch total loss: 12721.2344\n",
            "Trained batch: 283 batch loss: 48.7949295 batch xy loss 7.44314194 batch wh loss 11.2275915 batch obj loss 17.3582058 batch_class_loss 12.7659931 epoch total loss: 12770.0293\n",
            "Trained batch: 284 batch loss: 42.5403976 batch xy loss 6.6063714 batch wh loss 7.31574678 batch obj loss 17.0721436 batch_class_loss 11.5461416 epoch total loss: 12812.5693\n",
            "Trained batch: 285 batch loss: 38.4108047 batch xy loss 5.08149529 batch wh loss 8.02685547 batch obj loss 15.0287771 batch_class_loss 10.273674 epoch total loss: 12850.9805\n",
            "Trained batch: 286 batch loss: 41.9156876 batch xy loss 6.05718231 batch wh loss 9.40520668 batch obj loss 16.2233982 batch_class_loss 10.2298994 epoch total loss: 12892.8965\n",
            "Trained batch: 287 batch loss: 46.1617165 batch xy loss 6.53069925 batch wh loss 8.44862938 batch obj loss 18.8028564 batch_class_loss 12.3795338 epoch total loss: 12939.0586\n",
            "Trained batch: 288 batch loss: 39.071022 batch xy loss 5.87931919 batch wh loss 7.6268034 batch obj loss 15.6555557 batch_class_loss 9.90934467 epoch total loss: 12978.1299\n",
            "Trained batch: 289 batch loss: 41.279808 batch xy loss 6.82461452 batch wh loss 7.91911125 batch obj loss 16.1294937 batch_class_loss 10.4065924 epoch total loss: 13019.4102\n",
            "Trained batch: 290 batch loss: 37.8227806 batch xy loss 5.36163568 batch wh loss 7.03525734 batch obj loss 15.6812706 batch_class_loss 9.74461842 epoch total loss: 13057.2334\n",
            "Trained batch: 291 batch loss: 41.3552322 batch xy loss 6.08597231 batch wh loss 7.46733093 batch obj loss 17.186388 batch_class_loss 10.6155424 epoch total loss: 13098.5889\n",
            "Trained batch: 292 batch loss: 43.4336548 batch xy loss 6.59011078 batch wh loss 8.68229866 batch obj loss 17.0797081 batch_class_loss 11.0815372 epoch total loss: 13142.0225\n",
            "Trained batch: 293 batch loss: 47.5992966 batch xy loss 6.1456027 batch wh loss 11.053443 batch obj loss 17.9623108 batch_class_loss 12.4379387 epoch total loss: 13189.6221\n",
            "Trained batch: 294 batch loss: 39.5836334 batch xy loss 6.51036692 batch wh loss 8.19168663 batch obj loss 15.2792149 batch_class_loss 9.60236263 epoch total loss: 13229.2061\n",
            "Trained batch: 295 batch loss: 55.043 batch xy loss 7.96652937 batch wh loss 11.6050205 batch obj loss 20.72262 batch_class_loss 14.7488327 epoch total loss: 13284.249\n",
            "Trained batch: 296 batch loss: 41.1120415 batch xy loss 5.48941422 batch wh loss 10.6557608 batch obj loss 15.071003 batch_class_loss 9.89586353 epoch total loss: 13325.3613\n",
            "Trained batch: 297 batch loss: 42.343956 batch xy loss 6.49582529 batch wh loss 8.02219772 batch obj loss 16.504467 batch_class_loss 11.3214664 epoch total loss: 13367.7051\n",
            "Trained batch: 298 batch loss: 36.9564209 batch xy loss 5.09723043 batch wh loss 6.81463528 batch obj loss 15.1763134 batch_class_loss 9.86824 epoch total loss: 13404.6611\n",
            "Trained batch: 299 batch loss: 41.4903793 batch xy loss 5.78712 batch wh loss 7.34396362 batch obj loss 17.0008087 batch_class_loss 11.3584833 epoch total loss: 13446.1514\n",
            "Trained batch: 300 batch loss: 54.8633881 batch xy loss 7.77748108 batch wh loss 14.6448231 batch obj loss 18.4028873 batch_class_loss 14.0382 epoch total loss: 13501.0146\n",
            "Trained batch: 301 batch loss: 47.228508 batch xy loss 7.81067276 batch wh loss 8.94473839 batch obj loss 18.0630989 batch_class_loss 12.409997 epoch total loss: 13548.2432\n",
            "Trained batch: 302 batch loss: 44.5305252 batch xy loss 6.14416504 batch wh loss 9.61409473 batch obj loss 17.379343 batch_class_loss 11.3929214 epoch total loss: 13592.7734\n",
            "Trained batch: 303 batch loss: 46.1265144 batch xy loss 7.15048027 batch wh loss 9.40096664 batch obj loss 18.0785122 batch_class_loss 11.4965553 epoch total loss: 13638.9\n",
            "Trained batch: 304 batch loss: 39.1007767 batch xy loss 5.5846014 batch wh loss 7.50950098 batch obj loss 16.1781693 batch_class_loss 9.82850742 epoch total loss: 13678.001\n",
            "Trained batch: 305 batch loss: 48.1952782 batch xy loss 7.36396694 batch wh loss 10.1598988 batch obj loss 19.2290592 batch_class_loss 11.4423523 epoch total loss: 13726.1963\n",
            "Trained batch: 306 batch loss: 47.0254707 batch xy loss 7.76137447 batch wh loss 9.45449543 batch obj loss 18.1778069 batch_class_loss 11.631794 epoch total loss: 13773.2217\n",
            "Trained batch: 307 batch loss: 43.6303902 batch xy loss 6.5049367 batch wh loss 7.85225201 batch obj loss 17.1908932 batch_class_loss 12.0823059 epoch total loss: 13816.8525\n",
            "Trained batch: 308 batch loss: 45.5537949 batch xy loss 7.18873835 batch wh loss 10.961586 batch obj loss 16.5294495 batch_class_loss 10.8740177 epoch total loss: 13862.4062\n",
            "Trained batch: 309 batch loss: 40.4913673 batch xy loss 5.85978174 batch wh loss 7.51209497 batch obj loss 15.9600525 batch_class_loss 11.1594372 epoch total loss: 13902.8975\n",
            "Trained batch: 310 batch loss: 45.7990417 batch xy loss 6.72720718 batch wh loss 9.35670948 batch obj loss 17.9101963 batch_class_loss 11.8049307 epoch total loss: 13948.6963\n",
            "Trained batch: 311 batch loss: 32.1493073 batch xy loss 5.16844559 batch wh loss 5.25725 batch obj loss 13.6077871 batch_class_loss 8.11582279 epoch total loss: 13980.8457\n",
            "Trained batch: 312 batch loss: 40.4933548 batch xy loss 6.18831873 batch wh loss 8.3517065 batch obj loss 15.3751249 batch_class_loss 10.5782032 epoch total loss: 14021.3389\n",
            "Trained batch: 313 batch loss: 41.9923248 batch xy loss 5.94002438 batch wh loss 8.83351 batch obj loss 16.6961288 batch_class_loss 10.522666 epoch total loss: 14063.3311\n",
            "Trained batch: 314 batch loss: 40.6227417 batch xy loss 5.57146454 batch wh loss 10.1792364 batch obj loss 15.3894501 batch_class_loss 9.48259 epoch total loss: 14103.9541\n",
            "Trained batch: 315 batch loss: 46.2214088 batch xy loss 6.83426189 batch wh loss 10.6964397 batch obj loss 16.4026852 batch_class_loss 12.288023 epoch total loss: 14150.1758\n",
            "Trained batch: 316 batch loss: 39.871994 batch xy loss 5.45763874 batch wh loss 7.28407288 batch obj loss 16.5717354 batch_class_loss 10.558547 epoch total loss: 14190.0479\n",
            "Trained batch: 317 batch loss: 38.3083076 batch xy loss 6.17299318 batch wh loss 7.92961121 batch obj loss 14.8295212 batch_class_loss 9.37618256 epoch total loss: 14228.3564\n",
            "Trained batch: 318 batch loss: 52.9550285 batch xy loss 7.31631899 batch wh loss 10.3706141 batch obj loss 20.5926647 batch_class_loss 14.6754332 epoch total loss: 14281.3115\n",
            "Trained batch: 319 batch loss: 39.8408699 batch xy loss 6.44129086 batch wh loss 8.4654 batch obj loss 15.7986765 batch_class_loss 9.1355 epoch total loss: 14321.1523\n",
            "Trained batch: 320 batch loss: 40.5845795 batch xy loss 6.90891647 batch wh loss 7.74872923 batch obj loss 15.5701408 batch_class_loss 10.3567905 epoch total loss: 14361.7373\n",
            "Trained batch: 321 batch loss: 46.0465202 batch xy loss 7.51598215 batch wh loss 9.17501354 batch obj loss 18.1108494 batch_class_loss 11.2446775 epoch total loss: 14407.7842\n",
            "Trained batch: 322 batch loss: 54.907486 batch xy loss 7.77933 batch wh loss 11.4747295 batch obj loss 21.2713776 batch_class_loss 14.3820467 epoch total loss: 14462.6914\n",
            "Trained batch: 323 batch loss: 39.2725639 batch xy loss 5.97364902 batch wh loss 6.25444412 batch obj loss 16.6316757 batch_class_loss 10.412797 epoch total loss: 14501.9639\n",
            "Trained batch: 324 batch loss: 29.1141319 batch xy loss 4.03502 batch wh loss 4.69402218 batch obj loss 12.9800978 batch_class_loss 7.40498924 epoch total loss: 14531.0781\n",
            "Trained batch: 325 batch loss: 49.9679489 batch xy loss 7.34278631 batch wh loss 10.1120758 batch obj loss 19.3848858 batch_class_loss 13.1282034 epoch total loss: 14581.0459\n",
            "Trained batch: 326 batch loss: 45.5602341 batch xy loss 6.70639038 batch wh loss 8.71786499 batch obj loss 17.3402863 batch_class_loss 12.7956944 epoch total loss: 14626.6064\n",
            "Trained batch: 327 batch loss: 49.7550278 batch xy loss 7.11935711 batch wh loss 11.2230415 batch obj loss 18.5878849 batch_class_loss 12.8247433 epoch total loss: 14676.3613\n",
            "Trained batch: 328 batch loss: 48.8826485 batch xy loss 6.8663311 batch wh loss 10.4261465 batch obj loss 18.1138954 batch_class_loss 13.4762774 epoch total loss: 14725.2441\n",
            "Trained batch: 329 batch loss: 51.9523315 batch xy loss 6.95043182 batch wh loss 11.7981405 batch obj loss 19.3649216 batch_class_loss 13.8388376 epoch total loss: 14777.1963\n",
            "Trained batch: 330 batch loss: 41.3817 batch xy loss 5.63760853 batch wh loss 9.29374123 batch obj loss 16.2846661 batch_class_loss 10.1656828 epoch total loss: 14818.5781\n",
            "Trained batch: 331 batch loss: 38.6213722 batch xy loss 4.73688793 batch wh loss 7.45132542 batch obj loss 16.3361 batch_class_loss 10.0970602 epoch total loss: 14857.1992\n",
            "Trained batch: 332 batch loss: 37.7327118 batch xy loss 5.57600212 batch wh loss 8.23782921 batch obj loss 14.8679924 batch_class_loss 9.05088902 epoch total loss: 14894.9316\n",
            "Trained batch: 333 batch loss: 42.778923 batch xy loss 6.74987125 batch wh loss 8.24286 batch obj loss 17.1632614 batch_class_loss 10.6229305 epoch total loss: 14937.7109\n",
            "Trained batch: 334 batch loss: 52.61092 batch xy loss 7.07986689 batch wh loss 12.7583113 batch obj loss 19.3450108 batch_class_loss 13.4277315 epoch total loss: 14990.3223\n",
            "Trained batch: 335 batch loss: 49.5900497 batch xy loss 6.494174 batch wh loss 13.1067114 batch obj loss 17.8538494 batch_class_loss 12.1353216 epoch total loss: 15039.9121\n",
            "Trained batch: 336 batch loss: 38.5940933 batch xy loss 5.56280136 batch wh loss 7.69858408 batch obj loss 15.6235828 batch_class_loss 9.7091217 epoch total loss: 15078.5059\n",
            "Trained batch: 337 batch loss: 39.5410233 batch xy loss 5.61866045 batch wh loss 8.21546841 batch obj loss 15.3886261 batch_class_loss 10.3182688 epoch total loss: 15118.0469\n",
            "Trained batch: 338 batch loss: 46.8537178 batch xy loss 6.23015 batch wh loss 10.8833961 batch obj loss 17.6526604 batch_class_loss 12.087513 epoch total loss: 15164.9\n",
            "Trained batch: 339 batch loss: 48.6003799 batch xy loss 7.88175964 batch wh loss 11.5548878 batch obj loss 17.2635212 batch_class_loss 11.9002094 epoch total loss: 15213.501\n",
            "Trained batch: 340 batch loss: 35.5627975 batch xy loss 4.94320154 batch wh loss 7.54340601 batch obj loss 14.0597954 batch_class_loss 9.01639366 epoch total loss: 15249.0635\n",
            "Trained batch: 341 batch loss: 54.4467316 batch xy loss 7.13489914 batch wh loss 15.8178663 batch obj loss 18.8034763 batch_class_loss 12.6904879 epoch total loss: 15303.5098\n",
            "Trained batch: 342 batch loss: 46.7699051 batch xy loss 7.44344711 batch wh loss 8.90948486 batch obj loss 18.1409569 batch_class_loss 12.276022 epoch total loss: 15350.2793\n",
            "Trained batch: 343 batch loss: 47.490078 batch xy loss 6.91368103 batch wh loss 10.1118259 batch obj loss 18.6799393 batch_class_loss 11.7846289 epoch total loss: 15397.7695\n",
            "Trained batch: 344 batch loss: 51.1936111 batch xy loss 7.69133282 batch wh loss 9.60535049 batch obj loss 20.3643494 batch_class_loss 13.5325756 epoch total loss: 15448.9629\n",
            "Trained batch: 345 batch loss: 46.2172356 batch xy loss 6.52727222 batch wh loss 10.0799122 batch obj loss 18.0460854 batch_class_loss 11.5639639 epoch total loss: 15495.1797\n",
            "Trained batch: 346 batch loss: 41.907505 batch xy loss 6.8064003 batch wh loss 8.91338062 batch obj loss 16.6109695 batch_class_loss 9.57675838 epoch total loss: 15537.0869\n",
            "Trained batch: 347 batch loss: 49.5692253 batch xy loss 7.10286045 batch wh loss 11.7218914 batch obj loss 18.6263847 batch_class_loss 12.1180859 epoch total loss: 15586.6562\n",
            "Trained batch: 348 batch loss: 58.0912056 batch xy loss 8.63598537 batch wh loss 11.8814468 batch obj loss 22.0546303 batch_class_loss 15.5191402 epoch total loss: 15644.7471\n",
            "Trained batch: 349 batch loss: 48.861515 batch xy loss 6.35540867 batch wh loss 12.5269928 batch obj loss 18.6808033 batch_class_loss 11.2983103 epoch total loss: 15693.6084\n",
            "Trained batch: 350 batch loss: 45.552639 batch xy loss 6.26223183 batch wh loss 10.9077396 batch obj loss 17.1592484 batch_class_loss 11.2234144 epoch total loss: 15739.1611\n",
            "Trained batch: 351 batch loss: 48.4878044 batch xy loss 7.11575747 batch wh loss 11.6020489 batch obj loss 17.8571 batch_class_loss 11.9129047 epoch total loss: 15787.6494\n",
            "Trained batch: 352 batch loss: 41.51828 batch xy loss 6.22311878 batch wh loss 8.38405418 batch obj loss 16.3860416 batch_class_loss 10.5250626 epoch total loss: 15829.168\n",
            "Trained batch: 353 batch loss: 50.1753349 batch xy loss 7.38650799 batch wh loss 11.7153788 batch obj loss 18.340704 batch_class_loss 12.7327423 epoch total loss: 15879.3438\n",
            "Trained batch: 354 batch loss: 40.7244644 batch xy loss 5.24495792 batch wh loss 9.33581161 batch obj loss 15.7470474 batch_class_loss 10.3966446 epoch total loss: 15920.0684\n",
            "Trained batch: 355 batch loss: 49.9065933 batch xy loss 6.80377197 batch wh loss 10.461256 batch obj loss 19.4315357 batch_class_loss 13.2100296 epoch total loss: 15969.9746\n",
            "Trained batch: 356 batch loss: 36.2400742 batch xy loss 5.27807713 batch wh loss 7.83449078 batch obj loss 14.2273064 batch_class_loss 8.90019798 epoch total loss: 16006.2148\n",
            "Trained batch: 357 batch loss: 40.7031898 batch xy loss 5.76916647 batch wh loss 7.93223858 batch obj loss 15.9997969 batch_class_loss 11.0019865 epoch total loss: 16046.918\n",
            "Trained batch: 358 batch loss: 46.6595459 batch xy loss 6.53250885 batch wh loss 10.7596111 batch obj loss 17.7762642 batch_class_loss 11.5911655 epoch total loss: 16093.5771\n",
            "Trained batch: 359 batch loss: 40.9561195 batch xy loss 6.4506917 batch wh loss 8.7073822 batch obj loss 15.4099607 batch_class_loss 10.3880825 epoch total loss: 16134.5332\n",
            "Trained batch: 360 batch loss: 45.6682968 batch xy loss 6.68940496 batch wh loss 10.4496756 batch obj loss 17.3450069 batch_class_loss 11.1842051 epoch total loss: 16180.2012\n",
            "Trained batch: 361 batch loss: 46.1017647 batch xy loss 6.07785 batch wh loss 10.2322302 batch obj loss 18.0086079 batch_class_loss 11.7830791 epoch total loss: 16226.3027\n",
            "Trained batch: 362 batch loss: 42.0205269 batch xy loss 5.99551249 batch wh loss 8.9135952 batch obj loss 16.1332817 batch_class_loss 10.978138 epoch total loss: 16268.3232\n",
            "Trained batch: 363 batch loss: 40.9009094 batch xy loss 5.95977211 batch wh loss 8.12408161 batch obj loss 16.2838173 batch_class_loss 10.5332403 epoch total loss: 16309.2246\n",
            "Trained batch: 364 batch loss: 38.8869629 batch xy loss 5.74021959 batch wh loss 7.99895191 batch obj loss 15.6613541 batch_class_loss 9.48643589 epoch total loss: 16348.1113\n",
            "Trained batch: 365 batch loss: 45.9481735 batch xy loss 6.45176792 batch wh loss 10.5286398 batch obj loss 17.4901047 batch_class_loss 11.4776621 epoch total loss: 16394.0586\n",
            "Trained batch: 366 batch loss: 44.1105 batch xy loss 6.46577168 batch wh loss 9.42451477 batch obj loss 16.8169479 batch_class_loss 11.403265 epoch total loss: 16438.1699\n",
            "Trained batch: 367 batch loss: 39.7772217 batch xy loss 5.93475437 batch wh loss 7.57998657 batch obj loss 15.7957726 batch_class_loss 10.4667082 epoch total loss: 16477.9473\n",
            "Trained batch: 368 batch loss: 46.555603 batch xy loss 7.02170372 batch wh loss 11.0936699 batch obj loss 17.0013 batch_class_loss 11.4389277 epoch total loss: 16524.502\n",
            "Trained batch: 369 batch loss: 48.8710632 batch xy loss 7.57351112 batch wh loss 9.90519524 batch obj loss 18.0955925 batch_class_loss 13.2967644 epoch total loss: 16573.373\n",
            "Trained batch: 370 batch loss: 52.8738708 batch xy loss 8.07474 batch wh loss 10.158577 batch obj loss 19.9140892 batch_class_loss 14.7264671 epoch total loss: 16626.2461\n",
            "Trained batch: 371 batch loss: 49.5829239 batch xy loss 7.43097782 batch wh loss 10.0048599 batch obj loss 18.843195 batch_class_loss 13.3038902 epoch total loss: 16675.8281\n",
            "Trained batch: 372 batch loss: 42.6664581 batch xy loss 6.2438817 batch wh loss 8.71176434 batch obj loss 17.37187 batch_class_loss 10.3389435 epoch total loss: 16718.4941\n",
            "Trained batch: 373 batch loss: 52.9832687 batch xy loss 7.25229931 batch wh loss 11.3600702 batch obj loss 20.8876324 batch_class_loss 13.4832649 epoch total loss: 16771.4766\n",
            "Trained batch: 374 batch loss: 40.709549 batch xy loss 5.30635548 batch wh loss 7.91341686 batch obj loss 17.1770401 batch_class_loss 10.3127346 epoch total loss: 16812.1855\n",
            "Trained batch: 375 batch loss: 39.8239555 batch xy loss 6.12085819 batch wh loss 6.93187428 batch obj loss 16.4533176 batch_class_loss 10.3179016 epoch total loss: 16852.0098\n",
            "Trained batch: 376 batch loss: 39.0330429 batch xy loss 4.97380543 batch wh loss 9.65352821 batch obj loss 14.9707584 batch_class_loss 9.43495083 epoch total loss: 16891.043\n",
            "Trained batch: 377 batch loss: 38.8547096 batch xy loss 5.80184078 batch wh loss 7.04877567 batch obj loss 15.692174 batch_class_loss 10.3119202 epoch total loss: 16929.8984\n",
            "Trained batch: 378 batch loss: 47.095665 batch xy loss 6.94112396 batch wh loss 9.79192734 batch obj loss 18.1385841 batch_class_loss 12.2240267 epoch total loss: 16976.9941\n",
            "Trained batch: 379 batch loss: 46.7637177 batch xy loss 6.24301863 batch wh loss 10.2695217 batch obj loss 17.7424164 batch_class_loss 12.5087605 epoch total loss: 17023.7578\n",
            "Trained batch: 380 batch loss: 47.2094688 batch xy loss 7.74272633 batch wh loss 9.05455875 batch obj loss 17.948782 batch_class_loss 12.4634066 epoch total loss: 17070.9668\n",
            "Trained batch: 381 batch loss: 42.4378624 batch xy loss 6.46174145 batch wh loss 8.09565353 batch obj loss 16.6029606 batch_class_loss 11.2775078 epoch total loss: 17113.4043\n",
            "Trained batch: 382 batch loss: 40.7013741 batch xy loss 5.7635088 batch wh loss 8.09771729 batch obj loss 16.6271381 batch_class_loss 10.2130098 epoch total loss: 17154.1055\n",
            "Trained batch: 383 batch loss: 40.6469688 batch xy loss 6.52850723 batch wh loss 7.52738905 batch obj loss 16.2154903 batch_class_loss 10.3755779 epoch total loss: 17194.752\n",
            "Trained batch: 384 batch loss: 37.4705 batch xy loss 6.06301594 batch wh loss 7.34650326 batch obj loss 15.2298183 batch_class_loss 8.83116245 epoch total loss: 17232.2227\n",
            "Trained batch: 385 batch loss: 50.2207489 batch xy loss 6.73063946 batch wh loss 9.62553787 batch obj loss 19.0203609 batch_class_loss 14.8442097 epoch total loss: 17282.4434\n",
            "Trained batch: 386 batch loss: 38.6651077 batch xy loss 6.41059637 batch wh loss 7.65867519 batch obj loss 15.5220318 batch_class_loss 9.07380199 epoch total loss: 17321.1094\n",
            "Trained batch: 387 batch loss: 41.9745712 batch xy loss 6.51031399 batch wh loss 7.80288601 batch obj loss 16.200716 batch_class_loss 11.4606533 epoch total loss: 17363.084\n",
            "Trained batch: 388 batch loss: 36.4810448 batch xy loss 5.48932648 batch wh loss 6.64704418 batch obj loss 14.6556683 batch_class_loss 9.68900299 epoch total loss: 17399.5645\n",
            "Trained batch: 389 batch loss: 43.8887711 batch xy loss 6.61493683 batch wh loss 8.83579731 batch obj loss 17.840023 batch_class_loss 10.5980186 epoch total loss: 17443.4531\n",
            "Trained batch: 390 batch loss: 36.9243088 batch xy loss 5.18102932 batch wh loss 8.19602394 batch obj loss 14.2592087 batch_class_loss 9.28804207 epoch total loss: 17480.377\n",
            "Trained batch: 391 batch loss: 39.7887154 batch xy loss 5.49338341 batch wh loss 8.4565773 batch obj loss 15.5591841 batch_class_loss 10.2795706 epoch total loss: 17520.166\n",
            "Trained batch: 392 batch loss: 48.5089874 batch xy loss 7.48751259 batch wh loss 9.33394718 batch obj loss 18.2222767 batch_class_loss 13.4652557 epoch total loss: 17568.6758\n",
            "Trained batch: 393 batch loss: 40.3432617 batch xy loss 5.93635702 batch wh loss 8.09828186 batch obj loss 15.994978 batch_class_loss 10.3136454 epoch total loss: 17609.0195\n",
            "Trained batch: 394 batch loss: 40.6239204 batch xy loss 6.64044333 batch wh loss 8.01691532 batch obj loss 16.0839138 batch_class_loss 9.88265 epoch total loss: 17649.6426\n",
            "Trained batch: 395 batch loss: 38.6658363 batch xy loss 5.57144117 batch wh loss 7.53267336 batch obj loss 15.6085844 batch_class_loss 9.95313835 epoch total loss: 17688.3086\n",
            "Trained batch: 396 batch loss: 38.7648697 batch xy loss 5.42876148 batch wh loss 7.65812 batch obj loss 15.8633823 batch_class_loss 9.81460571 epoch total loss: 17727.0742\n",
            "Trained batch: 397 batch loss: 34.1477242 batch xy loss 4.7413168 batch wh loss 6.00995874 batch obj loss 14.7937527 batch_class_loss 8.60269642 epoch total loss: 17761.2227\n",
            "Trained batch: 398 batch loss: 48.4629059 batch xy loss 6.914361 batch wh loss 9.44732475 batch obj loss 19.1745243 batch_class_loss 12.9266958 epoch total loss: 17809.6855\n",
            "Trained batch: 399 batch loss: 36.2654533 batch xy loss 5.78795767 batch wh loss 6.77140141 batch obj loss 14.940711 batch_class_loss 8.76538467 epoch total loss: 17845.9512\n",
            "Trained batch: 400 batch loss: 40.7506104 batch xy loss 6.09388065 batch wh loss 7.04495621 batch obj loss 16.5650673 batch_class_loss 11.0467129 epoch total loss: 17886.7012\n",
            "Trained batch: 401 batch loss: 46.3834381 batch xy loss 7.0118289 batch wh loss 8.73004341 batch obj loss 17.7354832 batch_class_loss 12.9060822 epoch total loss: 17933.084\n",
            "Trained batch: 402 batch loss: 51.6484337 batch xy loss 8.46967697 batch wh loss 10.9085665 batch obj loss 18.9081821 batch_class_loss 13.3620062 epoch total loss: 17984.7324\n",
            "Trained batch: 403 batch loss: 41.2228394 batch xy loss 6.02936268 batch wh loss 7.59429455 batch obj loss 16.4787903 batch_class_loss 11.1203938 epoch total loss: 18025.9551\n",
            "Trained batch: 404 batch loss: 36.7022057 batch xy loss 5.90893412 batch wh loss 7.7807579 batch obj loss 13.886199 batch_class_loss 9.12631321 epoch total loss: 18062.6582\n",
            "Trained batch: 405 batch loss: 43.7528687 batch xy loss 6.67024708 batch wh loss 9.68601322 batch obj loss 16.6641197 batch_class_loss 10.7324867 epoch total loss: 18106.4102\n",
            "Trained batch: 406 batch loss: 34.5182762 batch xy loss 4.91114664 batch wh loss 7.02521467 batch obj loss 14.240593 batch_class_loss 8.34132481 epoch total loss: 18140.9277\n",
            "Trained batch: 407 batch loss: 36.5597 batch xy loss 4.99478292 batch wh loss 6.51266241 batch obj loss 15.4172449 batch_class_loss 9.63501167 epoch total loss: 18177.4883\n",
            "Trained batch: 408 batch loss: 45.0745125 batch xy loss 5.98836 batch wh loss 9.70224571 batch obj loss 17.8774853 batch_class_loss 11.506422 epoch total loss: 18222.5625\n",
            "Trained batch: 409 batch loss: 43.1682281 batch xy loss 6.44351387 batch wh loss 8.51724434 batch obj loss 16.8437138 batch_class_loss 11.3637552 epoch total loss: 18265.7305\n",
            "Trained batch: 410 batch loss: 53.0583344 batch xy loss 7.16806889 batch wh loss 13.7704735 batch obj loss 18.6983528 batch_class_loss 13.421442 epoch total loss: 18318.7891\n",
            "Trained batch: 411 batch loss: 48.5347748 batch xy loss 7.21590137 batch wh loss 9.70043182 batch obj loss 18.7893562 batch_class_loss 12.8290863 epoch total loss: 18367.3242\n",
            "Trained batch: 412 batch loss: 40.2369385 batch xy loss 5.91387177 batch wh loss 8.34496403 batch obj loss 15.9788399 batch_class_loss 9.9992609 epoch total loss: 18407.5605\n",
            "Trained batch: 413 batch loss: 53.8312874 batch xy loss 8.23681259 batch wh loss 11.579668 batch obj loss 20.2237988 batch_class_loss 13.7910089 epoch total loss: 18461.3926\n",
            "Trained batch: 414 batch loss: 56.8000107 batch xy loss 8.79323483 batch wh loss 12.3833599 batch obj loss 21.3392124 batch_class_loss 14.2842016 epoch total loss: 18518.1934\n",
            "Trained batch: 415 batch loss: 45.7363548 batch xy loss 7.65943146 batch wh loss 8.09596825 batch obj loss 18.2755966 batch_class_loss 11.7053566 epoch total loss: 18563.9297\n",
            "Trained batch: 416 batch loss: 61.1615562 batch xy loss 9.01473427 batch wh loss 12.3213673 batch obj loss 23.1471043 batch_class_loss 16.6783543 epoch total loss: 18625.0918\n",
            "Trained batch: 417 batch loss: 33.9444962 batch xy loss 4.71053886 batch wh loss 5.32831669 batch obj loss 15.9403305 batch_class_loss 7.96531 epoch total loss: 18659.0371\n",
            "Trained batch: 418 batch loss: 56.9147835 batch xy loss 8.01484 batch wh loss 12.6590891 batch obj loss 21.9542828 batch_class_loss 14.2865696 epoch total loss: 18715.9512\n",
            "Trained batch: 419 batch loss: 36.2764778 batch xy loss 6.1589222 batch wh loss 6.33904123 batch obj loss 15.3934717 batch_class_loss 8.38504505 epoch total loss: 18752.2285\n",
            "Trained batch: 420 batch loss: 47.9330673 batch xy loss 6.37959433 batch wh loss 10.4733315 batch obj loss 18.6739883 batch_class_loss 12.4061518 epoch total loss: 18800.1621\n",
            "Trained batch: 421 batch loss: 36.0921364 batch xy loss 4.83436155 batch wh loss 6.39177608 batch obj loss 15.3369131 batch_class_loss 9.52908611 epoch total loss: 18836.2539\n",
            "Trained batch: 422 batch loss: 51.4516907 batch xy loss 7.02239847 batch wh loss 11.189002 batch obj loss 19.3107395 batch_class_loss 13.9295483 epoch total loss: 18887.7051\n",
            "Trained batch: 423 batch loss: 42.3926315 batch xy loss 6.4308629 batch wh loss 8.11753273 batch obj loss 16.7034264 batch_class_loss 11.14081 epoch total loss: 18930.0977\n",
            "Trained batch: 424 batch loss: 46.5631104 batch xy loss 6.56710482 batch wh loss 10.547327 batch obj loss 17.0683765 batch_class_loss 12.3803062 epoch total loss: 18976.6602\n",
            "Trained batch: 425 batch loss: 38.991684 batch xy loss 5.39169121 batch wh loss 6.72337055 batch obj loss 15.8449 batch_class_loss 11.031723 epoch total loss: 19015.6523\n",
            "Trained batch: 426 batch loss: 43.9927788 batch xy loss 6.35885334 batch wh loss 8.52021408 batch obj loss 17.4002228 batch_class_loss 11.7134886 epoch total loss: 19059.6445\n",
            "Trained batch: 427 batch loss: 47.4902649 batch xy loss 6.82913923 batch wh loss 9.0800066 batch obj loss 18.7624779 batch_class_loss 12.8186417 epoch total loss: 19107.1348\n",
            "Trained batch: 428 batch loss: 50.4618683 batch xy loss 7.90791368 batch wh loss 9.35440922 batch obj loss 19.4581833 batch_class_loss 13.7413616 epoch total loss: 19157.5957\n",
            "Trained batch: 429 batch loss: 41.8732 batch xy loss 5.76211071 batch wh loss 10.5842896 batch obj loss 15.7722139 batch_class_loss 9.75458908 epoch total loss: 19199.4688\n",
            "Trained batch: 430 batch loss: 53.1960945 batch xy loss 7.31760025 batch wh loss 11.7741795 batch obj loss 20.438591 batch_class_loss 13.66572 epoch total loss: 19252.6641\n",
            "Trained batch: 431 batch loss: 46.1164703 batch xy loss 6.47827387 batch wh loss 8.93401718 batch obj loss 18.9591389 batch_class_loss 11.7450409 epoch total loss: 19298.7812\n",
            "Trained batch: 432 batch loss: 48.7740707 batch xy loss 7.62619 batch wh loss 8.94647503 batch obj loss 19.3697262 batch_class_loss 12.8316832 epoch total loss: 19347.5547\n",
            "Trained batch: 433 batch loss: 13.9644213 batch xy loss 1.86039162 batch wh loss 3.07452 batch obj loss 5.91851807 batch_class_loss 3.11099148 epoch total loss: 19361.5195\n",
            "20201008-155127 Epoch 2 train loss 44.71482467651367, total train batches 433, 79.78092193603516 examples per second\n",
            "2 , 6.592508 , 9.211450 , 11.518733 , 17.392134 , 44.714825 , 19.144144\n",
            "\n",
            "20201008-155340 Epoch 2 val loss 19.14414405822754, total val batches 191, 92.11642456054688 examples per second\n",
            "Model /content/gdrive/My Drive/results/epoch-2-loss-19.144.h5 saved.\n",
            "20201008-155340 Started epoch 3 with learning rate 0.01. Current LR patience count is 1 epochs. Last lowest train loss is 44.71482467651367. Last lowest val loss is 19.14414405822754.\n",
            "Trained batch: 1 batch loss: 33.7659683 batch xy loss 5.4641366 batch wh loss 6.45671415 batch obj loss 13.7261057 batch_class_loss 8.11900806 epoch total loss: 33.7659683\n",
            "Trained batch: 2 batch loss: 35.6153603 batch xy loss 5.6091795 batch wh loss 7.53379726 batch obj loss 13.5871325 batch_class_loss 8.885252 epoch total loss: 69.3813324\n",
            "Trained batch: 3 batch loss: 39.4062347 batch xy loss 6.11428881 batch wh loss 7.46095467 batch obj loss 15.4178238 batch_class_loss 10.413167 epoch total loss: 108.787567\n",
            "Trained batch: 4 batch loss: 49.009037 batch xy loss 7.55491829 batch wh loss 8.85372734 batch obj loss 18.8954372 batch_class_loss 13.704958 epoch total loss: 157.7966\n",
            "Trained batch: 5 batch loss: 46.618206 batch xy loss 6.84108686 batch wh loss 8.71141338 batch obj loss 18.3854351 batch_class_loss 12.6802731 epoch total loss: 204.41481\n",
            "Trained batch: 6 batch loss: 36.7688484 batch xy loss 5.36507225 batch wh loss 7.28932571 batch obj loss 15.0063286 batch_class_loss 9.10812 epoch total loss: 241.183655\n",
            "Trained batch: 7 batch loss: 31.0275612 batch xy loss 4.82994604 batch wh loss 4.49359226 batch obj loss 13.3008 batch_class_loss 8.40322113 epoch total loss: 272.211212\n",
            "Trained batch: 8 batch loss: 42.8311386 batch xy loss 5.90678024 batch wh loss 8.57999897 batch obj loss 16.8065987 batch_class_loss 11.5377588 epoch total loss: 315.042358\n",
            "Trained batch: 9 batch loss: 44.5609169 batch xy loss 6.96564293 batch wh loss 8.59222412 batch obj loss 17.1258564 batch_class_loss 11.8771935 epoch total loss: 359.603271\n",
            "Trained batch: 10 batch loss: 36.8336563 batch xy loss 5.30576897 batch wh loss 7.1773 batch obj loss 15.2238693 batch_class_loss 9.12671852 epoch total loss: 396.43692\n",
            "Trained batch: 11 batch loss: 39.3643417 batch xy loss 5.97626114 batch wh loss 6.59024143 batch obj loss 15.9979115 batch_class_loss 10.7999258 epoch total loss: 435.80127\n",
            "Trained batch: 12 batch loss: 58.945034 batch xy loss 8.09071064 batch wh loss 13.9404936 batch obj loss 21.2901268 batch_class_loss 15.623702 epoch total loss: 494.746307\n",
            "Trained batch: 13 batch loss: 43.0591621 batch xy loss 7.36815166 batch wh loss 8.80610561 batch obj loss 16.2895603 batch_class_loss 10.5953455 epoch total loss: 537.805481\n",
            "Trained batch: 14 batch loss: 40.8601379 batch xy loss 5.95584583 batch wh loss 7.63134623 batch obj loss 16.8163567 batch_class_loss 10.4565868 epoch total loss: 578.665649\n",
            "Trained batch: 15 batch loss: 61.4371109 batch xy loss 8.866045 batch wh loss 15.7311201 batch obj loss 21.7346973 batch_class_loss 15.1052437 epoch total loss: 640.102783\n",
            "Trained batch: 16 batch loss: 59.1649704 batch xy loss 8.66656494 batch wh loss 12.5752401 batch obj loss 22.7934895 batch_class_loss 15.1296806 epoch total loss: 699.267761\n",
            "Trained batch: 17 batch loss: 37.4933395 batch xy loss 4.95011282 batch wh loss 6.57648468 batch obj loss 16.3901043 batch_class_loss 9.57663822 epoch total loss: 736.761108\n",
            "Trained batch: 18 batch loss: 33.7066689 batch xy loss 4.94349909 batch wh loss 6.52146053 batch obj loss 14.6527538 batch_class_loss 7.58895493 epoch total loss: 770.467773\n",
            "Trained batch: 19 batch loss: 46.6787262 batch xy loss 6.77293 batch wh loss 8.93135548 batch obj loss 18.9588985 batch_class_loss 12.0155354 epoch total loss: 817.146484\n",
            "Trained batch: 20 batch loss: 52.8184624 batch xy loss 7.26467419 batch wh loss 10.8184071 batch obj loss 20.6009178 batch_class_loss 14.1344585 epoch total loss: 869.964966\n",
            "Trained batch: 21 batch loss: 33.0537186 batch xy loss 4.86650372 batch wh loss 5.84995604 batch obj loss 13.5896015 batch_class_loss 8.74765778 epoch total loss: 903.018677\n",
            "Trained batch: 22 batch loss: 45.2914925 batch xy loss 7.13176346 batch wh loss 7.7083087 batch obj loss 17.6331825 batch_class_loss 12.8182373 epoch total loss: 948.310181\n",
            "Trained batch: 23 batch loss: 37.299675 batch xy loss 5.42073393 batch wh loss 7.26789284 batch obj loss 14.8814974 batch_class_loss 9.72955418 epoch total loss: 985.609863\n",
            "Trained batch: 24 batch loss: 33.5608482 batch xy loss 4.53886509 batch wh loss 8.14762878 batch obj loss 12.9424343 batch_class_loss 7.93191957 epoch total loss: 1019.17072\n",
            "Trained batch: 25 batch loss: 36.9341 batch xy loss 5.41862106 batch wh loss 7.08052158 batch obj loss 14.8242254 batch_class_loss 9.61073589 epoch total loss: 1056.10486\n",
            "Trained batch: 26 batch loss: 42.5306435 batch xy loss 6.31430292 batch wh loss 7.74185848 batch obj loss 16.920433 batch_class_loss 11.5540476 epoch total loss: 1098.6355\n",
            "Trained batch: 27 batch loss: 45.3722076 batch xy loss 6.87847137 batch wh loss 7.85288191 batch obj loss 17.6040478 batch_class_loss 13.0368071 epoch total loss: 1144.00769\n",
            "Trained batch: 28 batch loss: 37.1786957 batch xy loss 5.91739845 batch wh loss 8.07804 batch obj loss 13.904191 batch_class_loss 9.27906513 epoch total loss: 1181.1864\n",
            "Trained batch: 29 batch loss: 40.8333969 batch xy loss 6.47962379 batch wh loss 8.9228754 batch obj loss 15.5438032 batch_class_loss 9.88709354 epoch total loss: 1222.01978\n",
            "Trained batch: 30 batch loss: 42.4498634 batch xy loss 6.81361485 batch wh loss 7.34276581 batch obj loss 17.4616642 batch_class_loss 10.8318176 epoch total loss: 1264.4696\n",
            "Trained batch: 31 batch loss: 48.2482567 batch xy loss 7.81237268 batch wh loss 8.18116856 batch obj loss 18.7176514 batch_class_loss 13.5370598 epoch total loss: 1312.7179\n",
            "Trained batch: 32 batch loss: 45.1838303 batch xy loss 6.88879585 batch wh loss 9.05837536 batch obj loss 17.5228062 batch_class_loss 11.7138538 epoch total loss: 1357.90173\n",
            "Trained batch: 33 batch loss: 43.2722092 batch xy loss 6.44849443 batch wh loss 8.10654068 batch obj loss 18.3568649 batch_class_loss 10.3603153 epoch total loss: 1401.17395\n",
            "Trained batch: 34 batch loss: 40.9090195 batch xy loss 6.72425318 batch wh loss 7.65153551 batch obj loss 17.0667896 batch_class_loss 9.46644 epoch total loss: 1442.08301\n",
            "Trained batch: 35 batch loss: 45.4607 batch xy loss 6.46479607 batch wh loss 10.083601 batch obj loss 17.4996071 batch_class_loss 11.4126978 epoch total loss: 1487.5437\n",
            "Trained batch: 36 batch loss: 33.9923859 batch xy loss 4.81229925 batch wh loss 6.46743202 batch obj loss 14.3546972 batch_class_loss 8.35795784 epoch total loss: 1521.53613\n",
            "Trained batch: 37 batch loss: 32.1677284 batch xy loss 4.55454063 batch wh loss 5.82744551 batch obj loss 13.6172724 batch_class_loss 8.16846848 epoch total loss: 1553.70386\n",
            "Trained batch: 38 batch loss: 32.9006081 batch xy loss 4.90063906 batch wh loss 5.48449612 batch obj loss 13.9101114 batch_class_loss 8.60536289 epoch total loss: 1586.60449\n",
            "Trained batch: 39 batch loss: 44.8457565 batch xy loss 7.01285601 batch wh loss 9.60146332 batch obj loss 17.2557087 batch_class_loss 10.9757252 epoch total loss: 1631.4502\n",
            "Trained batch: 40 batch loss: 41.4161415 batch xy loss 5.71779537 batch wh loss 8.20574188 batch obj loss 16.394846 batch_class_loss 11.0977573 epoch total loss: 1672.86633\n",
            "Trained batch: 41 batch loss: 38.5644188 batch xy loss 5.09354162 batch wh loss 8.02784443 batch obj loss 14.6997709 batch_class_loss 10.7432632 epoch total loss: 1711.43079\n",
            "Trained batch: 42 batch loss: 41.2552185 batch xy loss 5.82583 batch wh loss 7.75022411 batch obj loss 15.814827 batch_class_loss 11.864337 epoch total loss: 1752.68604\n",
            "Trained batch: 43 batch loss: 37.8649216 batch xy loss 5.95801592 batch wh loss 6.99584198 batch obj loss 14.7292 batch_class_loss 10.1818638 epoch total loss: 1790.5509\n",
            "Trained batch: 44 batch loss: 35.8918 batch xy loss 5.55991459 batch wh loss 5.75883245 batch obj loss 14.2833376 batch_class_loss 10.2897167 epoch total loss: 1826.44275\n",
            "Trained batch: 45 batch loss: 36.6397438 batch xy loss 5.35130644 batch wh loss 7.22790384 batch obj loss 14.2841015 batch_class_loss 9.77643108 epoch total loss: 1863.08252\n",
            "Trained batch: 46 batch loss: 47.0393562 batch xy loss 6.94525862 batch wh loss 10.436965 batch obj loss 17.6032906 batch_class_loss 12.0538435 epoch total loss: 1910.12183\n",
            "Trained batch: 47 batch loss: 35.9670296 batch xy loss 5.54645634 batch wh loss 6.84620047 batch obj loss 14.3874054 batch_class_loss 9.1869688 epoch total loss: 1946.08887\n",
            "Trained batch: 48 batch loss: 46.3182564 batch xy loss 6.71856594 batch wh loss 9.69015121 batch obj loss 17.4087963 batch_class_loss 12.5007439 epoch total loss: 1992.4071\n",
            "Trained batch: 49 batch loss: 41.3312073 batch xy loss 6.34444666 batch wh loss 8.50011921 batch obj loss 16.0286751 batch_class_loss 10.4579649 epoch total loss: 2033.73828\n",
            "Trained batch: 50 batch loss: 50.7285271 batch xy loss 8.48146534 batch wh loss 9.60978413 batch obj loss 19.5721855 batch_class_loss 13.065094 epoch total loss: 2084.4668\n",
            "Trained batch: 51 batch loss: 39.5515747 batch xy loss 5.86651897 batch wh loss 6.18305302 batch obj loss 17.2202244 batch_class_loss 10.2817755 epoch total loss: 2124.01831\n",
            "Trained batch: 52 batch loss: 49.6370316 batch xy loss 7.36153507 batch wh loss 11.0189219 batch obj loss 18.316309 batch_class_loss 12.9402676 epoch total loss: 2173.65527\n",
            "Trained batch: 53 batch loss: 40.0984497 batch xy loss 6.26249027 batch wh loss 8.41891193 batch obj loss 15.4669161 batch_class_loss 9.95013 epoch total loss: 2213.75366\n",
            "Trained batch: 54 batch loss: 34.6119347 batch xy loss 5.14233828 batch wh loss 5.7476368 batch obj loss 15.412384 batch_class_loss 8.3095789 epoch total loss: 2248.36548\n",
            "Trained batch: 55 batch loss: 36.7077 batch xy loss 5.58311558 batch wh loss 5.34589958 batch obj loss 16.0783424 batch_class_loss 9.70033932 epoch total loss: 2285.07324\n",
            "Trained batch: 56 batch loss: 44.3215942 batch xy loss 6.64827394 batch wh loss 9.78958797 batch obj loss 16.375946 batch_class_loss 11.507782 epoch total loss: 2329.39478\n",
            "Trained batch: 57 batch loss: 42.8338776 batch xy loss 6.34355736 batch wh loss 6.62249947 batch obj loss 18.2471504 batch_class_loss 11.6206703 epoch total loss: 2372.22876\n",
            "Trained batch: 58 batch loss: 42.681118 batch xy loss 5.76776695 batch wh loss 9.90574455 batch obj loss 15.5693684 batch_class_loss 11.4382381 epoch total loss: 2414.91\n",
            "Trained batch: 59 batch loss: 52.3670654 batch xy loss 7.01888847 batch wh loss 14.7214375 batch obj loss 17.6882725 batch_class_loss 12.9384623 epoch total loss: 2467.27686\n",
            "Trained batch: 60 batch loss: 35.4147339 batch xy loss 5.45860577 batch wh loss 6.67400742 batch obj loss 14.0838051 batch_class_loss 9.19831562 epoch total loss: 2502.69165\n",
            "Trained batch: 61 batch loss: 30.4132957 batch xy loss 4.50054312 batch wh loss 5.43570232 batch obj loss 12.5052986 batch_class_loss 7.9717536 epoch total loss: 2533.10498\n",
            "Trained batch: 62 batch loss: 41.6066399 batch xy loss 6.03926754 batch wh loss 7.35094595 batch obj loss 16.4962044 batch_class_loss 11.7202196 epoch total loss: 2574.71167\n",
            "Trained batch: 63 batch loss: 38.0618172 batch xy loss 6.04924488 batch wh loss 5.94168758 batch obj loss 15.4481192 batch_class_loss 10.6227627 epoch total loss: 2612.77344\n",
            "Trained batch: 64 batch loss: 39.9848366 batch xy loss 5.49917316 batch wh loss 8.75204659 batch obj loss 15.8202753 batch_class_loss 9.91334343 epoch total loss: 2652.7583\n",
            "Trained batch: 65 batch loss: 38.907692 batch xy loss 5.70151138 batch wh loss 7.22217798 batch obj loss 15.8171921 batch_class_loss 10.166811 epoch total loss: 2691.66602\n",
            "Trained batch: 66 batch loss: 41.8912 batch xy loss 5.76489639 batch wh loss 9.12008476 batch obj loss 15.8974171 batch_class_loss 11.1088028 epoch total loss: 2733.55713\n",
            "Trained batch: 67 batch loss: 39.300663 batch xy loss 6.76168728 batch wh loss 7.66228962 batch obj loss 15.1914625 batch_class_loss 9.68522263 epoch total loss: 2772.85791\n",
            "Trained batch: 68 batch loss: 40.8867455 batch xy loss 6.31702518 batch wh loss 8.68571472 batch obj loss 15.3767767 batch_class_loss 10.5072308 epoch total loss: 2813.74463\n",
            "Trained batch: 69 batch loss: 45.4365616 batch xy loss 6.4817338 batch wh loss 10.6796989 batch obj loss 17.2264481 batch_class_loss 11.0486784 epoch total loss: 2859.18115\n",
            "Trained batch: 70 batch loss: 44.7903824 batch xy loss 7.49919224 batch wh loss 8.49915123 batch obj loss 16.9270287 batch_class_loss 11.8650074 epoch total loss: 2903.97144\n",
            "Trained batch: 71 batch loss: 38.1394043 batch xy loss 5.61387491 batch wh loss 6.57886124 batch obj loss 16.043951 batch_class_loss 9.90271568 epoch total loss: 2942.11084\n",
            "Trained batch: 72 batch loss: 35.6603355 batch xy loss 5.29758 batch wh loss 6.34366941 batch obj loss 14.4723577 batch_class_loss 9.54673 epoch total loss: 2977.77124\n",
            "Trained batch: 73 batch loss: 44.7047729 batch xy loss 6.59209681 batch wh loss 8.75845909 batch obj loss 17.6996231 batch_class_loss 11.6545944 epoch total loss: 3022.47607\n",
            "Trained batch: 74 batch loss: 47.1014061 batch xy loss 6.84119606 batch wh loss 10.0893269 batch obj loss 18.1750793 batch_class_loss 11.9958029 epoch total loss: 3069.57739\n",
            "Trained batch: 75 batch loss: 46.0522194 batch xy loss 7.41688585 batch wh loss 9.05933475 batch obj loss 17.7351685 batch_class_loss 11.840826 epoch total loss: 3115.62964\n",
            "Trained batch: 76 batch loss: 36.9762306 batch xy loss 5.19617367 batch wh loss 5.86286831 batch obj loss 16.3392677 batch_class_loss 9.57792187 epoch total loss: 3152.60596\n",
            "Trained batch: 77 batch loss: 45.4658508 batch xy loss 6.84721899 batch wh loss 8.37020302 batch obj loss 18.0050507 batch_class_loss 12.2433777 epoch total loss: 3198.07178\n",
            "Trained batch: 78 batch loss: 37.7408524 batch xy loss 5.15353823 batch wh loss 6.79518175 batch obj loss 16.0633698 batch_class_loss 9.72876167 epoch total loss: 3235.81274\n",
            "Trained batch: 79 batch loss: 44.1175728 batch xy loss 6.55759287 batch wh loss 8.10262203 batch obj loss 17.9153938 batch_class_loss 11.5419617 epoch total loss: 3279.93042\n",
            "Trained batch: 80 batch loss: 34.9614487 batch xy loss 5.3492732 batch wh loss 6.7913847 batch obj loss 14.0959692 batch_class_loss 8.72482109 epoch total loss: 3314.89185\n",
            "Trained batch: 81 batch loss: 39.4454575 batch xy loss 6.33448648 batch wh loss 7.69623852 batch obj loss 15.451087 batch_class_loss 9.96364689 epoch total loss: 3354.3374\n",
            "Trained batch: 82 batch loss: 47.4218979 batch xy loss 7.01674271 batch wh loss 8.98375797 batch obj loss 17.9695 batch_class_loss 13.4518957 epoch total loss: 3401.75928\n",
            "Trained batch: 83 batch loss: 40.8675919 batch xy loss 6.52492 batch wh loss 7.19507885 batch obj loss 16.4957733 batch_class_loss 10.651823 epoch total loss: 3442.62695\n",
            "Trained batch: 84 batch loss: 46.4845734 batch xy loss 6.74683619 batch wh loss 10.5678101 batch obj loss 17.2237682 batch_class_loss 11.9461555 epoch total loss: 3489.11157\n",
            "Trained batch: 85 batch loss: 49.3130951 batch xy loss 8.14832211 batch wh loss 10.9696436 batch obj loss 17.4326477 batch_class_loss 12.7624798 epoch total loss: 3538.42456\n",
            "Trained batch: 86 batch loss: 42.389122 batch xy loss 5.88058424 batch wh loss 9.50759506 batch obj loss 16.4662495 batch_class_loss 10.5346966 epoch total loss: 3580.81372\n",
            "Trained batch: 87 batch loss: 34.2054558 batch xy loss 5.46109104 batch wh loss 5.20448828 batch obj loss 14.3448429 batch_class_loss 9.19503403 epoch total loss: 3615.01929\n",
            "Trained batch: 88 batch loss: 55.5314674 batch xy loss 7.98915958 batch wh loss 11.3929892 batch obj loss 20.8051815 batch_class_loss 15.3441362 epoch total loss: 3670.55078\n",
            "Trained batch: 89 batch loss: 58.7431793 batch xy loss 9.03352547 batch wh loss 13.0352182 batch obj loss 21.2504349 batch_class_loss 15.4239988 epoch total loss: 3729.29395\n",
            "Trained batch: 90 batch loss: 43.5479164 batch xy loss 6.70777416 batch wh loss 7.44533825 batch obj loss 18.1755981 batch_class_loss 11.2192011 epoch total loss: 3772.8418\n",
            "Trained batch: 91 batch loss: 48.1472664 batch xy loss 8.00032139 batch wh loss 9.41262531 batch obj loss 18.8095531 batch_class_loss 11.9247656 epoch total loss: 3820.98901\n",
            "Trained batch: 92 batch loss: 49.4856606 batch xy loss 7.42217 batch wh loss 8.96493 batch obj loss 20.8268356 batch_class_loss 12.2717276 epoch total loss: 3870.47461\n",
            "Trained batch: 93 batch loss: 50.0822601 batch xy loss 7.14275646 batch wh loss 10.6871986 batch obj loss 20.9171581 batch_class_loss 11.3351488 epoch total loss: 3920.55688\n",
            "Trained batch: 94 batch loss: 42.5453148 batch xy loss 5.55916834 batch wh loss 8.36169434 batch obj loss 18.9565201 batch_class_loss 9.66793251 epoch total loss: 3963.10229\n",
            "Trained batch: 95 batch loss: 41.6015701 batch xy loss 6.11927 batch wh loss 7.99672031 batch obj loss 16.9484062 batch_class_loss 10.5371761 epoch total loss: 4004.70386\n",
            "Trained batch: 96 batch loss: 41.6771927 batch xy loss 6.04405165 batch wh loss 8.04663467 batch obj loss 16.764082 batch_class_loss 10.8224287 epoch total loss: 4046.3811\n",
            "Trained batch: 97 batch loss: 33.2993393 batch xy loss 5.32427883 batch wh loss 6.76190567 batch obj loss 13.4513979 batch_class_loss 7.7617569 epoch total loss: 4079.68042\n",
            "Trained batch: 98 batch loss: 48.3323288 batch xy loss 7.73061 batch wh loss 10.1877708 batch obj loss 17.8094845 batch_class_loss 12.6044664 epoch total loss: 4128.0127\n",
            "Trained batch: 99 batch loss: 41.3944435 batch xy loss 5.4158206 batch wh loss 7.45233393 batch obj loss 16.8955212 batch_class_loss 11.6307669 epoch total loss: 4169.40723\n",
            "Trained batch: 100 batch loss: 50.0872726 batch xy loss 7.40709496 batch wh loss 10.5919256 batch obj loss 18.3396912 batch_class_loss 13.7485561 epoch total loss: 4219.49463\n",
            "Trained batch: 101 batch loss: 37.4961624 batch xy loss 5.14265585 batch wh loss 6.75805521 batch obj loss 15.3721561 batch_class_loss 10.2232981 epoch total loss: 4256.99072\n",
            "Trained batch: 102 batch loss: 38.2234116 batch xy loss 5.79571819 batch wh loss 8.34314 batch obj loss 14.3963041 batch_class_loss 9.68825054 epoch total loss: 4295.21436\n",
            "Trained batch: 103 batch loss: 51.4445343 batch xy loss 7.30483294 batch wh loss 12.3915701 batch obj loss 18.3208618 batch_class_loss 13.427269 epoch total loss: 4346.65869\n",
            "Trained batch: 104 batch loss: 42.9562531 batch xy loss 6.25849533 batch wh loss 9.64238 batch obj loss 15.7522306 batch_class_loss 11.3031502 epoch total loss: 4389.61475\n",
            "Trained batch: 105 batch loss: 37.1310959 batch xy loss 5.67150402 batch wh loss 8.02471066 batch obj loss 14.6472492 batch_class_loss 8.78763294 epoch total loss: 4426.74561\n",
            "Trained batch: 106 batch loss: 51.937355 batch xy loss 8.09589481 batch wh loss 11.5745401 batch obj loss 19.425127 batch_class_loss 12.841795 epoch total loss: 4478.68311\n",
            "Trained batch: 107 batch loss: 49.8516655 batch xy loss 7.8378 batch wh loss 11.6645193 batch obj loss 18.3433266 batch_class_loss 12.0060186 epoch total loss: 4528.53467\n",
            "Trained batch: 108 batch loss: 48.4457893 batch xy loss 7.27930832 batch wh loss 11.0074282 batch obj loss 18.258976 batch_class_loss 11.9000769 epoch total loss: 4576.98047\n",
            "Trained batch: 109 batch loss: 36.8116531 batch xy loss 5.30940962 batch wh loss 7.06632185 batch obj loss 15.6671295 batch_class_loss 8.76879311 epoch total loss: 4613.79199\n",
            "Trained batch: 110 batch loss: 42.4757538 batch xy loss 6.15571642 batch wh loss 8.80213451 batch obj loss 16.768198 batch_class_loss 10.7497044 epoch total loss: 4656.26758\n",
            "Trained batch: 111 batch loss: 40.8559799 batch xy loss 5.84224415 batch wh loss 8.54597569 batch obj loss 16.1643486 batch_class_loss 10.3034115 epoch total loss: 4697.12354\n",
            "Trained batch: 112 batch loss: 45.1700478 batch xy loss 7.31744719 batch wh loss 8.52395153 batch obj loss 18.2662449 batch_class_loss 11.0624037 epoch total loss: 4742.29346\n",
            "Trained batch: 113 batch loss: 48.5780487 batch xy loss 6.62332869 batch wh loss 9.88462067 batch obj loss 19.5629139 batch_class_loss 12.5071898 epoch total loss: 4790.87158\n",
            "Trained batch: 114 batch loss: 49.5348434 batch xy loss 6.82887411 batch wh loss 9.88844395 batch obj loss 19.7343674 batch_class_loss 13.0831556 epoch total loss: 4840.40625\n",
            "Trained batch: 115 batch loss: 45.5836411 batch xy loss 6.9486227 batch wh loss 7.99923468 batch obj loss 18.1979046 batch_class_loss 12.4378796 epoch total loss: 4885.98975\n",
            "Trained batch: 116 batch loss: 44.553009 batch xy loss 6.43270159 batch wh loss 9.6219492 batch obj loss 17.4300117 batch_class_loss 11.068346 epoch total loss: 4930.54297\n",
            "Trained batch: 117 batch loss: 39.861 batch xy loss 5.60396 batch wh loss 9.31507111 batch obj loss 15.0761929 batch_class_loss 9.86577702 epoch total loss: 4970.40381\n",
            "Trained batch: 118 batch loss: 41.6594543 batch xy loss 5.56482124 batch wh loss 9.62250519 batch obj loss 15.7782383 batch_class_loss 10.6938887 epoch total loss: 5012.06348\n",
            "Trained batch: 119 batch loss: 42.0262604 batch xy loss 6.76999807 batch wh loss 8.56935787 batch obj loss 16.3678436 batch_class_loss 10.3190622 epoch total loss: 5054.09\n",
            "Trained batch: 120 batch loss: 49.4861 batch xy loss 7.31589794 batch wh loss 9.90507412 batch obj loss 18.9313049 batch_class_loss 13.3338261 epoch total loss: 5103.57617\n",
            "Trained batch: 121 batch loss: 34.5051155 batch xy loss 4.94196892 batch wh loss 5.63706589 batch obj loss 14.9592438 batch_class_loss 8.96683598 epoch total loss: 5138.08105\n",
            "Trained batch: 122 batch loss: 44.0401535 batch xy loss 6.79406548 batch wh loss 8.47930813 batch obj loss 17.3947144 batch_class_loss 11.3720665 epoch total loss: 5182.12109\n",
            "Trained batch: 123 batch loss: 47.2973289 batch xy loss 6.99269772 batch wh loss 8.7862587 batch obj loss 18.5629482 batch_class_loss 12.9554243 epoch total loss: 5229.41846\n",
            "Trained batch: 124 batch loss: 44.7295876 batch xy loss 7.59524584 batch wh loss 7.98143625 batch obj loss 17.715395 batch_class_loss 11.4375095 epoch total loss: 5274.14795\n",
            "Trained batch: 125 batch loss: 41.9121513 batch xy loss 6.94989443 batch wh loss 6.86125755 batch obj loss 16.8894615 batch_class_loss 11.2115393 epoch total loss: 5316.06\n",
            "Trained batch: 126 batch loss: 40.933609 batch xy loss 6.01555157 batch wh loss 7.4581461 batch obj loss 16.5475235 batch_class_loss 10.9123898 epoch total loss: 5356.99365\n",
            "Trained batch: 127 batch loss: 38.0114822 batch xy loss 5.60986328 batch wh loss 7.29426 batch obj loss 15.1514378 batch_class_loss 9.95592 epoch total loss: 5395.00537\n",
            "Trained batch: 128 batch loss: 35.1391678 batch xy loss 5.43422222 batch wh loss 6.75639105 batch obj loss 14.5210896 batch_class_loss 8.42746544 epoch total loss: 5430.14453\n",
            "Trained batch: 129 batch loss: 44.4501038 batch xy loss 6.18846893 batch wh loss 9.72738838 batch obj loss 17.2011032 batch_class_loss 11.3331413 epoch total loss: 5474.59473\n",
            "Trained batch: 130 batch loss: 41.3778305 batch xy loss 5.76329136 batch wh loss 8.62460518 batch obj loss 16.8788986 batch_class_loss 10.1110344 epoch total loss: 5515.97266\n",
            "Trained batch: 131 batch loss: 36.0872803 batch xy loss 5.36597347 batch wh loss 7.54072523 batch obj loss 14.397027 batch_class_loss 8.78355694 epoch total loss: 5552.06\n",
            "Trained batch: 132 batch loss: 47.6216431 batch xy loss 6.40512753 batch wh loss 10.9224548 batch obj loss 18.2796669 batch_class_loss 12.0143957 epoch total loss: 5599.68164\n",
            "Trained batch: 133 batch loss: 40.5883369 batch xy loss 5.87226486 batch wh loss 6.53118086 batch obj loss 16.5455704 batch_class_loss 11.6393232 epoch total loss: 5640.27\n",
            "Trained batch: 134 batch loss: 47.1216507 batch xy loss 6.82241058 batch wh loss 10.1024799 batch obj loss 17.4731865 batch_class_loss 12.7235718 epoch total loss: 5687.3916\n",
            "Trained batch: 135 batch loss: 31.9346733 batch xy loss 4.56122637 batch wh loss 5.48331308 batch obj loss 13.9740067 batch_class_loss 7.9161253 epoch total loss: 5719.32617\n",
            "Trained batch: 136 batch loss: 43.0799332 batch xy loss 7.1199913 batch wh loss 8.37651 batch obj loss 16.2026958 batch_class_loss 11.3807373 epoch total loss: 5762.40625\n",
            "Trained batch: 137 batch loss: 40.8896751 batch xy loss 6.61409378 batch wh loss 7.40153 batch obj loss 15.8708115 batch_class_loss 11.0032396 epoch total loss: 5803.2959\n",
            "Trained batch: 138 batch loss: 34.748951 batch xy loss 5.32878494 batch wh loss 6.62856293 batch obj loss 14.2658262 batch_class_loss 8.52577782 epoch total loss: 5838.04492\n",
            "Trained batch: 139 batch loss: 40.0317612 batch xy loss 5.84545231 batch wh loss 7.4708333 batch obj loss 16.2176533 batch_class_loss 10.4978247 epoch total loss: 5878.07666\n",
            "Trained batch: 140 batch loss: 44.6974068 batch xy loss 6.81135225 batch wh loss 8.17213917 batch obj loss 17.1890526 batch_class_loss 12.5248623 epoch total loss: 5922.77393\n",
            "Trained batch: 141 batch loss: 39.3231621 batch xy loss 5.96737862 batch wh loss 7.88842487 batch obj loss 15.419899 batch_class_loss 10.0474586 epoch total loss: 5962.09717\n",
            "Trained batch: 142 batch loss: 53.4841 batch xy loss 7.82636404 batch wh loss 10.3897591 batch obj loss 21.4158669 batch_class_loss 13.8521118 epoch total loss: 6015.58105\n",
            "Trained batch: 143 batch loss: 50.8728104 batch xy loss 6.78373241 batch wh loss 10.0727539 batch obj loss 20.5280228 batch_class_loss 13.4883022 epoch total loss: 6066.4541\n",
            "Trained batch: 144 batch loss: 35.2907715 batch xy loss 5.27893162 batch wh loss 6.72330379 batch obj loss 14.6586857 batch_class_loss 8.62985229 epoch total loss: 6101.74512\n",
            "Trained batch: 145 batch loss: 36.807 batch xy loss 5.33769703 batch wh loss 7.3541708 batch obj loss 15.1997719 batch_class_loss 8.9153595 epoch total loss: 6138.55225\n",
            "Trained batch: 146 batch loss: 46.8144341 batch xy loss 6.63568068 batch wh loss 9.62424755 batch obj loss 18.2103386 batch_class_loss 12.3441668 epoch total loss: 6185.3667\n",
            "Trained batch: 147 batch loss: 38.2333679 batch xy loss 5.60536528 batch wh loss 7.23895741 batch obj loss 15.3765793 batch_class_loss 10.0124655 epoch total loss: 6223.6\n",
            "Trained batch: 148 batch loss: 44.9599304 batch xy loss 6.67667484 batch wh loss 8.32262325 batch obj loss 17.6489849 batch_class_loss 12.3116455 epoch total loss: 6268.56\n",
            "Trained batch: 149 batch loss: 46.8477249 batch xy loss 7.64776564 batch wh loss 9.07355785 batch obj loss 18.0103703 batch_class_loss 12.1160336 epoch total loss: 6315.40771\n",
            "Trained batch: 150 batch loss: 39.8300095 batch xy loss 5.59205389 batch wh loss 8.18388939 batch obj loss 15.7380047 batch_class_loss 10.3160648 epoch total loss: 6355.23779\n",
            "Trained batch: 151 batch loss: 53.7083511 batch xy loss 8.13275051 batch wh loss 13.7391586 batch obj loss 18.5178337 batch_class_loss 13.3186045 epoch total loss: 6408.94629\n",
            "Trained batch: 152 batch loss: 34.8700027 batch xy loss 5.20582151 batch wh loss 7.67363119 batch obj loss 13.5848007 batch_class_loss 8.40574837 epoch total loss: 6443.81641\n",
            "Trained batch: 153 batch loss: 43.039238 batch xy loss 6.8064642 batch wh loss 8.01381874 batch obj loss 17.4923553 batch_class_loss 10.7265978 epoch total loss: 6486.85547\n",
            "Trained batch: 154 batch loss: 45.9156685 batch xy loss 7.03395844 batch wh loss 9.48457 batch obj loss 17.6383133 batch_class_loss 11.7588272 epoch total loss: 6532.771\n",
            "Trained batch: 155 batch loss: 43.6778336 batch xy loss 6.98822 batch wh loss 8.08298111 batch obj loss 17.2842636 batch_class_loss 11.3223686 epoch total loss: 6576.44873\n",
            "Trained batch: 156 batch loss: 42.2632561 batch xy loss 6.60698414 batch wh loss 8.17754 batch obj loss 16.8371658 batch_class_loss 10.6415644 epoch total loss: 6618.71191\n",
            "Trained batch: 157 batch loss: 39.8910179 batch xy loss 5.52936554 batch wh loss 9.17065144 batch obj loss 16.0858746 batch_class_loss 9.10512924 epoch total loss: 6658.60303\n",
            "Trained batch: 158 batch loss: 52.4655914 batch xy loss 7.44915724 batch wh loss 10.3025236 batch obj loss 20.6464386 batch_class_loss 14.0674706 epoch total loss: 6711.06885\n",
            "Trained batch: 159 batch loss: 39.7091141 batch xy loss 6.44938564 batch wh loss 7.26001787 batch obj loss 15.6421185 batch_class_loss 10.3575916 epoch total loss: 6750.77783\n",
            "Trained batch: 160 batch loss: 45.5409088 batch xy loss 6.74878 batch wh loss 10.4050283 batch obj loss 17.7211437 batch_class_loss 10.6659565 epoch total loss: 6796.31885\n",
            "Trained batch: 161 batch loss: 34.9658661 batch xy loss 5.07657671 batch wh loss 6.50019932 batch obj loss 14.3844872 batch_class_loss 9.00460434 epoch total loss: 6831.28467\n",
            "Trained batch: 162 batch loss: 33.6926956 batch xy loss 4.51841164 batch wh loss 5.76026583 batch obj loss 14.4690819 batch_class_loss 8.94493675 epoch total loss: 6864.97754\n",
            "Trained batch: 163 batch loss: 42.8124771 batch xy loss 6.27160692 batch wh loss 10.2227983 batch obj loss 16.0360069 batch_class_loss 10.2820644 epoch total loss: 6907.79\n",
            "Trained batch: 164 batch loss: 38.3084145 batch xy loss 5.7222352 batch wh loss 7.91783237 batch obj loss 14.816721 batch_class_loss 9.85162163 epoch total loss: 6946.09863\n",
            "Trained batch: 165 batch loss: 35.9523506 batch xy loss 5.11205721 batch wh loss 7.05737829 batch obj loss 14.6548014 batch_class_loss 9.12811279 epoch total loss: 6982.05078\n",
            "Trained batch: 166 batch loss: 45.1050797 batch xy loss 6.48165035 batch wh loss 8.76183128 batch obj loss 17.3206158 batch_class_loss 12.5409794 epoch total loss: 7027.15576\n",
            "Trained batch: 167 batch loss: 38.8624763 batch xy loss 6.07785034 batch wh loss 6.8653307 batch obj loss 15.6753874 batch_class_loss 10.2439051 epoch total loss: 7066.01807\n",
            "Trained batch: 168 batch loss: 45.2112045 batch xy loss 6.96889734 batch wh loss 10.8530159 batch obj loss 16.4580936 batch_class_loss 10.9311962 epoch total loss: 7111.22949\n",
            "Trained batch: 169 batch loss: 39.4520035 batch xy loss 5.29969788 batch wh loss 8.29466438 batch obj loss 15.2169695 batch_class_loss 10.6406746 epoch total loss: 7150.68164\n",
            "Trained batch: 170 batch loss: 47.5184784 batch xy loss 7.3203516 batch wh loss 8.61091805 batch obj loss 18.6144943 batch_class_loss 12.9727173 epoch total loss: 7198.2\n",
            "Trained batch: 171 batch loss: 47.1894188 batch xy loss 7.83912563 batch wh loss 10.3837481 batch obj loss 17.7608814 batch_class_loss 11.2056637 epoch total loss: 7245.38965\n",
            "Trained batch: 172 batch loss: 41.1905212 batch xy loss 5.87391186 batch wh loss 8.59106541 batch obj loss 16.6523037 batch_class_loss 10.0732384 epoch total loss: 7286.58\n",
            "Trained batch: 173 batch loss: 52.5744514 batch xy loss 8.06697178 batch wh loss 11.5842304 batch obj loss 19.9800663 batch_class_loss 12.9431782 epoch total loss: 7339.1543\n",
            "Trained batch: 174 batch loss: 40.2493706 batch xy loss 5.25228 batch wh loss 8.29886818 batch obj loss 16.5166759 batch_class_loss 10.1815434 epoch total loss: 7379.40381\n",
            "Trained batch: 175 batch loss: 47.0476608 batch xy loss 7.11465073 batch wh loss 8.87783718 batch obj loss 19.0479679 batch_class_loss 12.007205 epoch total loss: 7426.45166\n",
            "Trained batch: 176 batch loss: 33.7706 batch xy loss 5.00697708 batch wh loss 6.81486559 batch obj loss 13.7556906 batch_class_loss 8.19306374 epoch total loss: 7460.22217\n",
            "Trained batch: 177 batch loss: 45.3465 batch xy loss 6.70239592 batch wh loss 9.63173294 batch obj loss 17.8675365 batch_class_loss 11.1448345 epoch total loss: 7505.56885\n",
            "Trained batch: 178 batch loss: 42.2259254 batch xy loss 6.16598177 batch wh loss 10.4133987 batch obj loss 15.7248983 batch_class_loss 9.92164516 epoch total loss: 7547.79492\n",
            "Trained batch: 179 batch loss: 45.4903488 batch xy loss 6.48223925 batch wh loss 9.41630077 batch obj loss 17.5195541 batch_class_loss 12.0722532 epoch total loss: 7593.28516\n",
            "Trained batch: 180 batch loss: 49.1135445 batch xy loss 7.19859648 batch wh loss 12.166338 batch obj loss 18.023632 batch_class_loss 11.7249756 epoch total loss: 7642.39893\n",
            "Trained batch: 181 batch loss: 37.7141838 batch xy loss 5.49329615 batch wh loss 7.46841669 batch obj loss 15.0560369 batch_class_loss 9.69643307 epoch total loss: 7680.11328\n",
            "Trained batch: 182 batch loss: 48.4129715 batch xy loss 8.11048698 batch wh loss 8.03840065 batch obj loss 18.9433746 batch_class_loss 13.3207111 epoch total loss: 7728.52637\n",
            "Trained batch: 183 batch loss: 43.5926552 batch xy loss 6.20404148 batch wh loss 9.90793514 batch obj loss 16.9607544 batch_class_loss 10.5199251 epoch total loss: 7772.11914\n",
            "Trained batch: 184 batch loss: 47.5584755 batch xy loss 7.27791786 batch wh loss 8.11753941 batch obj loss 20.0819359 batch_class_loss 12.0810814 epoch total loss: 7819.67773\n",
            "Trained batch: 185 batch loss: 48.6560936 batch xy loss 7.62314224 batch wh loss 9.10921383 batch obj loss 18.9185524 batch_class_loss 13.0051842 epoch total loss: 7868.33398\n",
            "Trained batch: 186 batch loss: 46.4538345 batch xy loss 6.72412825 batch wh loss 8.24002838 batch obj loss 19.7535248 batch_class_loss 11.7361565 epoch total loss: 7914.7876\n",
            "Trained batch: 187 batch loss: 44.5665855 batch xy loss 6.61735916 batch wh loss 9.41130447 batch obj loss 17.7233219 batch_class_loss 10.8146 epoch total loss: 7959.354\n",
            "Trained batch: 188 batch loss: 39.6632614 batch xy loss 6.20536852 batch wh loss 8.10977936 batch obj loss 15.6968403 batch_class_loss 9.65127373 epoch total loss: 7999.01709\n",
            "Trained batch: 189 batch loss: 47.5300789 batch xy loss 6.54203463 batch wh loss 11.1220131 batch obj loss 17.8653126 batch_class_loss 12.0007172 epoch total loss: 8046.54736\n",
            "Trained batch: 190 batch loss: 53.1242409 batch xy loss 7.57711458 batch wh loss 9.85076523 batch obj loss 20.9131508 batch_class_loss 14.7832079 epoch total loss: 8099.67139\n",
            "Trained batch: 191 batch loss: 41.6214638 batch xy loss 6.10725307 batch wh loss 10.5969353 batch obj loss 15.0866451 batch_class_loss 9.83063507 epoch total loss: 8141.29297\n",
            "Trained batch: 192 batch loss: 37.3524284 batch xy loss 5.49218559 batch wh loss 7.04976845 batch obj loss 15.5100565 batch_class_loss 9.30041695 epoch total loss: 8178.64551\n",
            "Trained batch: 193 batch loss: 41.1335144 batch xy loss 5.5350666 batch wh loss 9.88150215 batch obj loss 15.9521551 batch_class_loss 9.76479149 epoch total loss: 8219.7793\n",
            "Trained batch: 194 batch loss: 45.1303253 batch xy loss 6.77821207 batch wh loss 10.2941732 batch obj loss 16.8507862 batch_class_loss 11.207159 epoch total loss: 8264.90918\n",
            "Trained batch: 195 batch loss: 49.5403023 batch xy loss 7.54291058 batch wh loss 10.2415905 batch obj loss 19.3861294 batch_class_loss 12.3696737 epoch total loss: 8314.44922\n",
            "Trained batch: 196 batch loss: 46.1226196 batch xy loss 6.79606 batch wh loss 10.2347031 batch obj loss 17.2278786 batch_class_loss 11.8639746 epoch total loss: 8360.57227\n",
            "Trained batch: 197 batch loss: 48.2032967 batch xy loss 7.02967 batch wh loss 10.4462929 batch obj loss 18.3624916 batch_class_loss 12.3648415 epoch total loss: 8408.77539\n",
            "Trained batch: 198 batch loss: 46.4512558 batch xy loss 6.9197526 batch wh loss 9.55338478 batch obj loss 17.2166595 batch_class_loss 12.7614584 epoch total loss: 8455.22656\n",
            "Trained batch: 199 batch loss: 47.6941528 batch xy loss 6.75470161 batch wh loss 10.9561157 batch obj loss 18.5033226 batch_class_loss 11.4800129 epoch total loss: 8502.9209\n",
            "Trained batch: 200 batch loss: 42.8140068 batch xy loss 6.0906024 batch wh loss 7.85409546 batch obj loss 17.8268776 batch_class_loss 11.0424328 epoch total loss: 8545.73535\n",
            "Trained batch: 201 batch loss: 53.7497406 batch xy loss 7.87793922 batch wh loss 10.9476023 batch obj loss 21.1437454 batch_class_loss 13.7804527 epoch total loss: 8599.48535\n",
            "Trained batch: 202 batch loss: 48.8183784 batch xy loss 7.03260088 batch wh loss 10.5259199 batch obj loss 19.1516285 batch_class_loss 12.1082306 epoch total loss: 8648.30371\n",
            "Trained batch: 203 batch loss: 39.6349106 batch xy loss 5.74849129 batch wh loss 7.78277779 batch obj loss 16.6234665 batch_class_loss 9.48017216 epoch total loss: 8687.93848\n",
            "Trained batch: 204 batch loss: 48.598732 batch xy loss 6.99407864 batch wh loss 9.17985916 batch obj loss 19.1649265 batch_class_loss 13.2598677 epoch total loss: 8736.53711\n",
            "Trained batch: 205 batch loss: 49.4973946 batch xy loss 7.78457737 batch wh loss 9.27689266 batch obj loss 20.0625401 batch_class_loss 12.3733883 epoch total loss: 8786.03418\n",
            "Trained batch: 206 batch loss: 56.2630501 batch xy loss 8.58133698 batch wh loss 11.2779264 batch obj loss 21.6598244 batch_class_loss 14.7439623 epoch total loss: 8842.29688\n",
            "Trained batch: 207 batch loss: 39.6758499 batch xy loss 6.20918274 batch wh loss 7.14970827 batch obj loss 16.2270699 batch_class_loss 10.0898933 epoch total loss: 8881.97266\n",
            "Trained batch: 208 batch loss: 48.936142 batch xy loss 6.27353954 batch wh loss 10.7964211 batch obj loss 19.3545513 batch_class_loss 12.5116291 epoch total loss: 8930.90918\n",
            "Trained batch: 209 batch loss: 43.0677719 batch xy loss 5.79291153 batch wh loss 10.5626183 batch obj loss 16.2934227 batch_class_loss 10.4188213 epoch total loss: 8973.97656\n",
            "Trained batch: 210 batch loss: 42.6049118 batch xy loss 6.15517235 batch wh loss 7.88116121 batch obj loss 17.4613094 batch_class_loss 11.1072674 epoch total loss: 9016.58105\n",
            "Trained batch: 211 batch loss: 44.5892868 batch xy loss 6.97877502 batch wh loss 8.11818314 batch obj loss 17.7563572 batch_class_loss 11.7359734 epoch total loss: 9061.17\n",
            "Trained batch: 212 batch loss: 40.3463 batch xy loss 5.7591 batch wh loss 7.93915176 batch obj loss 16.266613 batch_class_loss 10.3814325 epoch total loss: 9101.5166\n",
            "Trained batch: 213 batch loss: 42.9620247 batch xy loss 6.61282253 batch wh loss 8.02951431 batch obj loss 16.5349026 batch_class_loss 11.7847862 epoch total loss: 9144.47852\n",
            "Trained batch: 214 batch loss: 42.9258156 batch xy loss 6.75624323 batch wh loss 9.18485546 batch obj loss 16.2728806 batch_class_loss 10.7118368 epoch total loss: 9187.4043\n",
            "Trained batch: 215 batch loss: 39.9346924 batch xy loss 5.65741825 batch wh loss 8.18145752 batch obj loss 16.5166836 batch_class_loss 9.57913399 epoch total loss: 9227.33887\n",
            "Trained batch: 216 batch loss: 37.5818062 batch xy loss 5.7591877 batch wh loss 6.25313282 batch obj loss 15.6120119 batch_class_loss 9.95747185 epoch total loss: 9264.9209\n",
            "Trained batch: 217 batch loss: 42.8625183 batch xy loss 6.54880047 batch wh loss 8.60268402 batch obj loss 16.7668705 batch_class_loss 10.9441662 epoch total loss: 9307.7832\n",
            "Trained batch: 218 batch loss: 41.7761879 batch xy loss 6.21979618 batch wh loss 8.19495106 batch obj loss 17.3389568 batch_class_loss 10.0224819 epoch total loss: 9349.56\n",
            "Trained batch: 219 batch loss: 36.3410187 batch xy loss 5.58093166 batch wh loss 6.62360191 batch obj loss 15.1135426 batch_class_loss 9.02294 epoch total loss: 9385.9\n",
            "Trained batch: 220 batch loss: 39.8921623 batch xy loss 7.09516096 batch wh loss 8.14694786 batch obj loss 14.8392658 batch_class_loss 9.81078434 epoch total loss: 9425.79297\n",
            "Trained batch: 221 batch loss: 41.9811516 batch xy loss 6.00577545 batch wh loss 9.11066818 batch obj loss 16.2986526 batch_class_loss 10.5660524 epoch total loss: 9467.77441\n",
            "Trained batch: 222 batch loss: 35.985611 batch xy loss 5.27854776 batch wh loss 6.31514502 batch obj loss 15.0940809 batch_class_loss 9.2978363 epoch total loss: 9503.76\n",
            "Trained batch: 223 batch loss: 47.0365944 batch xy loss 6.52294636 batch wh loss 11.1191759 batch obj loss 17.192585 batch_class_loss 12.2018833 epoch total loss: 9550.7959\n",
            "Trained batch: 224 batch loss: 34.4811363 batch xy loss 4.82415199 batch wh loss 6.26915359 batch obj loss 14.5777349 batch_class_loss 8.81009579 epoch total loss: 9585.27734\n",
            "Trained batch: 225 batch loss: 42.4993744 batch xy loss 5.62935972 batch wh loss 8.9366169 batch obj loss 16.6636963 batch_class_loss 11.2697048 epoch total loss: 9627.77637\n",
            "Trained batch: 226 batch loss: 42.9960823 batch xy loss 6.34717035 batch wh loss 8.28510189 batch obj loss 16.7380981 batch_class_loss 11.6257143 epoch total loss: 9670.77246\n",
            "Trained batch: 227 batch loss: 44.4793549 batch xy loss 6.73778725 batch wh loss 8.68134117 batch obj loss 17.399601 batch_class_loss 11.6606236 epoch total loss: 9715.25195\n",
            "Trained batch: 228 batch loss: 44.3518829 batch xy loss 6.74807 batch wh loss 8.32205772 batch obj loss 18.1617813 batch_class_loss 11.1199722 epoch total loss: 9759.60352\n",
            "Trained batch: 229 batch loss: 42.5987701 batch xy loss 6.86305237 batch wh loss 7.66656494 batch obj loss 17.0774021 batch_class_loss 10.9917469 epoch total loss: 9802.20215\n",
            "Trained batch: 230 batch loss: 46.2745247 batch xy loss 6.87088299 batch wh loss 9.29519749 batch obj loss 18.7183952 batch_class_loss 11.3900528 epoch total loss: 9848.47656\n",
            "Trained batch: 231 batch loss: 42.7648468 batch xy loss 6.37725353 batch wh loss 9.05542183 batch obj loss 17.1933079 batch_class_loss 10.1388607 epoch total loss: 9891.24121\n",
            "Trained batch: 232 batch loss: 39.422451 batch xy loss 5.38597679 batch wh loss 9.85982513 batch obj loss 15.0081367 batch_class_loss 9.16851521 epoch total loss: 9930.66406\n",
            "Trained batch: 233 batch loss: 47.1293678 batch xy loss 7.42358828 batch wh loss 9.83863926 batch obj loss 17.7037754 batch_class_loss 12.1633682 epoch total loss: 9977.79297\n",
            "Trained batch: 234 batch loss: 42.98592 batch xy loss 5.96143532 batch wh loss 7.39021921 batch obj loss 17.542984 batch_class_loss 12.091279 epoch total loss: 10020.7793\n",
            "Trained batch: 235 batch loss: 41.2127113 batch xy loss 6.27909136 batch wh loss 7.36797142 batch obj loss 16.3653622 batch_class_loss 11.2002878 epoch total loss: 10061.9922\n",
            "Trained batch: 236 batch loss: 45.7791557 batch xy loss 7.16055441 batch wh loss 9.2047348 batch obj loss 17.5753574 batch_class_loss 11.8385115 epoch total loss: 10107.7715\n",
            "Trained batch: 237 batch loss: 48.3724213 batch xy loss 7.38914537 batch wh loss 10.9771681 batch obj loss 18.0585823 batch_class_loss 11.9475212 epoch total loss: 10156.1436\n",
            "Trained batch: 238 batch loss: 44.8859863 batch xy loss 6.30654573 batch wh loss 8.47957706 batch obj loss 18.0627117 batch_class_loss 12.0371485 epoch total loss: 10201.0293\n",
            "Trained batch: 239 batch loss: 38.1385956 batch xy loss 5.42066622 batch wh loss 6.82339907 batch obj loss 15.7041988 batch_class_loss 10.1903296 epoch total loss: 10239.168\n",
            "Trained batch: 240 batch loss: 37.8039627 batch xy loss 5.13177633 batch wh loss 7.93531466 batch obj loss 15.0456238 batch_class_loss 9.69124794 epoch total loss: 10276.9717\n",
            "Trained batch: 241 batch loss: 47.344677 batch xy loss 6.48192596 batch wh loss 10.3547831 batch obj loss 18.1923351 batch_class_loss 12.3156376 epoch total loss: 10324.3164\n",
            "Trained batch: 242 batch loss: 45.6477203 batch xy loss 7.28078461 batch wh loss 9.96094 batch obj loss 17.4404869 batch_class_loss 10.9655066 epoch total loss: 10369.9639\n",
            "Trained batch: 243 batch loss: 43.3916 batch xy loss 5.66610575 batch wh loss 9.1260767 batch obj loss 17.3770638 batch_class_loss 11.2223568 epoch total loss: 10413.3555\n",
            "Trained batch: 244 batch loss: 43.1974106 batch xy loss 6.36960077 batch wh loss 9.57977 batch obj loss 17.2819099 batch_class_loss 9.9661293 epoch total loss: 10456.5527\n",
            "Trained batch: 245 batch loss: 47.6420059 batch xy loss 7.7721734 batch wh loss 9.02947807 batch obj loss 18.2385979 batch_class_loss 12.6017647 epoch total loss: 10504.1943\n",
            "Trained batch: 246 batch loss: 42.5707397 batch xy loss 6.75921106 batch wh loss 8.03569221 batch obj loss 16.8652878 batch_class_loss 10.9105434 epoch total loss: 10546.7646\n",
            "Trained batch: 247 batch loss: 42.8689423 batch xy loss 6.65036631 batch wh loss 9.16957855 batch obj loss 16.6060238 batch_class_loss 10.4429693 epoch total loss: 10589.6338\n",
            "Trained batch: 248 batch loss: 39.2325363 batch xy loss 5.70134 batch wh loss 6.78479195 batch obj loss 16.0252457 batch_class_loss 10.721158 epoch total loss: 10628.8662\n",
            "Trained batch: 249 batch loss: 42.3450394 batch xy loss 6.48566532 batch wh loss 8.66307259 batch obj loss 16.0575962 batch_class_loss 11.1387091 epoch total loss: 10671.2109\n",
            "Trained batch: 250 batch loss: 45.231575 batch xy loss 7.26630449 batch wh loss 7.47868 batch obj loss 18.2031193 batch_class_loss 12.2834692 epoch total loss: 10716.4424\n",
            "Trained batch: 251 batch loss: 45.2337189 batch xy loss 7.38985634 batch wh loss 8.94045067 batch obj loss 17.4327221 batch_class_loss 11.4706917 epoch total loss: 10761.6758\n",
            "Trained batch: 252 batch loss: 42.9610367 batch xy loss 6.67462158 batch wh loss 9.2984581 batch obj loss 16.2827644 batch_class_loss 10.7051935 epoch total loss: 10804.6367\n",
            "Trained batch: 253 batch loss: 34.4374771 batch xy loss 4.7216754 batch wh loss 5.76013517 batch obj loss 15.074357 batch_class_loss 8.88131142 epoch total loss: 10839.0742\n",
            "Trained batch: 254 batch loss: 42.8934021 batch xy loss 6.11460781 batch wh loss 11.1885786 batch obj loss 16.1036377 batch_class_loss 9.4865818 epoch total loss: 10881.9678\n",
            "Trained batch: 255 batch loss: 38.5958633 batch xy loss 5.66147041 batch wh loss 7.54002047 batch obj loss 15.5502949 batch_class_loss 9.84407711 epoch total loss: 10920.5635\n",
            "Trained batch: 256 batch loss: 42.6269035 batch xy loss 6.1478672 batch wh loss 8.50877571 batch obj loss 17.1186867 batch_class_loss 10.8515739 epoch total loss: 10963.1904\n",
            "Trained batch: 257 batch loss: 48.7137489 batch xy loss 7.759902 batch wh loss 9.58874607 batch obj loss 18.89744 batch_class_loss 12.4676628 epoch total loss: 11011.9043\n",
            "Trained batch: 258 batch loss: 42.0202751 batch xy loss 6.1166873 batch wh loss 8.18779373 batch obj loss 17.1663017 batch_class_loss 10.5494909 epoch total loss: 11053.9248\n",
            "Trained batch: 259 batch loss: 37.4308357 batch xy loss 5.82497358 batch wh loss 5.83669233 batch obj loss 16.0026073 batch_class_loss 9.76656246 epoch total loss: 11091.3555\n",
            "Trained batch: 260 batch loss: 38.8336258 batch xy loss 6.00853395 batch wh loss 7.75453043 batch obj loss 15.6741199 batch_class_loss 9.39644 epoch total loss: 11130.1895\n",
            "Trained batch: 261 batch loss: 39.7797852 batch xy loss 5.48879576 batch wh loss 9.00536156 batch obj loss 15.968668 batch_class_loss 9.31696415 epoch total loss: 11169.9688\n",
            "Trained batch: 262 batch loss: 39.5513077 batch xy loss 5.94875526 batch wh loss 7.27840805 batch obj loss 15.5759029 batch_class_loss 10.7482386 epoch total loss: 11209.5205\n",
            "Trained batch: 263 batch loss: 47.6974716 batch xy loss 7.91406393 batch wh loss 8.70462227 batch obj loss 18.1070919 batch_class_loss 12.9716911 epoch total loss: 11257.2178\n",
            "Trained batch: 264 batch loss: 57.0987167 batch xy loss 8.8527956 batch wh loss 11.3088474 batch obj loss 21.8428707 batch_class_loss 15.0942039 epoch total loss: 11314.3164\n",
            "Trained batch: 265 batch loss: 38.0682831 batch xy loss 5.85490942 batch wh loss 6.53426838 batch obj loss 14.9974222 batch_class_loss 10.6816845 epoch total loss: 11352.3848\n",
            "Trained batch: 266 batch loss: 36.1471901 batch xy loss 5.50585938 batch wh loss 6.70490932 batch obj loss 14.9765072 batch_class_loss 8.95991325 epoch total loss: 11388.5322\n",
            "Trained batch: 267 batch loss: 45.5300255 batch xy loss 6.4122 batch wh loss 10.1577549 batch obj loss 17.8787193 batch_class_loss 11.0813503 epoch total loss: 11434.0625\n",
            "Trained batch: 268 batch loss: 39.416008 batch xy loss 5.86488628 batch wh loss 8.01047421 batch obj loss 15.552866 batch_class_loss 9.98778343 epoch total loss: 11473.4785\n",
            "Trained batch: 269 batch loss: 37.0030136 batch xy loss 5.30358362 batch wh loss 8.43876648 batch obj loss 14.5176907 batch_class_loss 8.74297 epoch total loss: 11510.4814\n",
            "Trained batch: 270 batch loss: 35.5690155 batch xy loss 5.48749113 batch wh loss 6.60876 batch obj loss 14.3490648 batch_class_loss 9.12369823 epoch total loss: 11546.0508\n",
            "Trained batch: 271 batch loss: 34.167511 batch xy loss 4.86540031 batch wh loss 6.62989187 batch obj loss 14.5299759 batch_class_loss 8.14224148 epoch total loss: 11580.2188\n",
            "Trained batch: 272 batch loss: 40.74123 batch xy loss 5.48266125 batch wh loss 7.8836112 batch obj loss 16.1270714 batch_class_loss 11.2478886 epoch total loss: 11620.96\n",
            "Trained batch: 273 batch loss: 44.1047974 batch xy loss 6.50584555 batch wh loss 7.9363966 batch obj loss 17.5420284 batch_class_loss 12.1205301 epoch total loss: 11665.0645\n",
            "Trained batch: 274 batch loss: 44.6572456 batch xy loss 6.82425547 batch wh loss 8.46647167 batch obj loss 17.1744747 batch_class_loss 12.1920424 epoch total loss: 11709.7217\n",
            "Trained batch: 275 batch loss: 44.3467484 batch xy loss 6.04606152 batch wh loss 9.57080364 batch obj loss 17.3413582 batch_class_loss 11.3885231 epoch total loss: 11754.0684\n",
            "Trained batch: 276 batch loss: 49.760746 batch xy loss 7.51925135 batch wh loss 11.6661282 batch obj loss 18.5342178 batch_class_loss 12.0411453 epoch total loss: 11803.8291\n",
            "Trained batch: 277 batch loss: 40.2493019 batch xy loss 6.37563086 batch wh loss 8.29647064 batch obj loss 15.7337666 batch_class_loss 9.84343433 epoch total loss: 11844.0781\n",
            "Trained batch: 278 batch loss: 41.4645538 batch xy loss 6.08037949 batch wh loss 9.17471313 batch obj loss 16.2733803 batch_class_loss 9.93608284 epoch total loss: 11885.543\n",
            "Trained batch: 279 batch loss: 37.2892 batch xy loss 5.43117428 batch wh loss 7.84316254 batch obj loss 14.6945181 batch_class_loss 9.32034683 epoch total loss: 11922.832\n",
            "Trained batch: 280 batch loss: 44.1952209 batch xy loss 6.75178909 batch wh loss 8.49083233 batch obj loss 17.1690311 batch_class_loss 11.7835655 epoch total loss: 11967.0273\n",
            "Trained batch: 281 batch loss: 34.9480591 batch xy loss 5.43780613 batch wh loss 7.06876326 batch obj loss 13.9643507 batch_class_loss 8.47713947 epoch total loss: 12001.9756\n",
            "Trained batch: 282 batch loss: 48.0917053 batch xy loss 7.30724335 batch wh loss 10.1285477 batch obj loss 18.2146149 batch_class_loss 12.4412985 epoch total loss: 12050.0674\n",
            "Trained batch: 283 batch loss: 36.9731865 batch xy loss 4.98327827 batch wh loss 6.34828043 batch obj loss 15.5745029 batch_class_loss 10.0671301 epoch total loss: 12087.041\n",
            "Trained batch: 284 batch loss: 40.9402428 batch xy loss 6.47727871 batch wh loss 7.6304574 batch obj loss 16.4292431 batch_class_loss 10.4032612 epoch total loss: 12127.9814\n",
            "Trained batch: 285 batch loss: 35.7699966 batch xy loss 5.75806522 batch wh loss 6.11833143 batch obj loss 14.1025505 batch_class_loss 9.79105091 epoch total loss: 12163.751\n",
            "Trained batch: 286 batch loss: 33.2689896 batch xy loss 4.80922365 batch wh loss 5.78516531 batch obj loss 14.0309258 batch_class_loss 8.6436739 epoch total loss: 12197.0195\n",
            "Trained batch: 287 batch loss: 51.4434204 batch xy loss 8.19746876 batch wh loss 10.6362276 batch obj loss 19.545845 batch_class_loss 13.0638809 epoch total loss: 12248.4629\n",
            "Trained batch: 288 batch loss: 38.4013252 batch xy loss 5.71777868 batch wh loss 7.74119568 batch obj loss 15.2788267 batch_class_loss 9.66352177 epoch total loss: 12286.8643\n",
            "Trained batch: 289 batch loss: 51.2921143 batch xy loss 8.33228493 batch wh loss 11.491272 batch obj loss 19.0986938 batch_class_loss 12.3698683 epoch total loss: 12338.1562\n",
            "Trained batch: 290 batch loss: 42.5945816 batch xy loss 6.26131153 batch wh loss 10.0469475 batch obj loss 16.041584 batch_class_loss 10.2447405 epoch total loss: 12380.751\n",
            "Trained batch: 291 batch loss: 44.4781075 batch xy loss 6.13431406 batch wh loss 9.68538 batch obj loss 17.1905041 batch_class_loss 11.4679089 epoch total loss: 12425.2295\n",
            "Trained batch: 292 batch loss: 45.5260811 batch xy loss 6.94539261 batch wh loss 9.5781641 batch obj loss 17.864151 batch_class_loss 11.1383762 epoch total loss: 12470.7559\n",
            "Trained batch: 293 batch loss: 36.3874207 batch xy loss 5.62603569 batch wh loss 6.7207942 batch obj loss 14.933836 batch_class_loss 9.10675526 epoch total loss: 12507.1436\n",
            "Trained batch: 294 batch loss: 38.041172 batch xy loss 5.45424175 batch wh loss 7.02782583 batch obj loss 15.765029 batch_class_loss 9.79407406 epoch total loss: 12545.1846\n",
            "Trained batch: 295 batch loss: 40.2931862 batch xy loss 5.85074854 batch wh loss 8.50439835 batch obj loss 15.9529152 batch_class_loss 9.98512554 epoch total loss: 12585.4775\n",
            "Trained batch: 296 batch loss: 40.216011 batch xy loss 5.72530079 batch wh loss 7.25838566 batch obj loss 16.4093781 batch_class_loss 10.8229475 epoch total loss: 12625.6934\n",
            "Trained batch: 297 batch loss: 36.3873672 batch xy loss 5.66300154 batch wh loss 6.65587425 batch obj loss 14.8023453 batch_class_loss 9.26614857 epoch total loss: 12662.0811\n",
            "Trained batch: 298 batch loss: 35.4930801 batch xy loss 5.37072372 batch wh loss 5.66507483 batch obj loss 14.8714867 batch_class_loss 9.5857935 epoch total loss: 12697.5742\n",
            "Trained batch: 299 batch loss: 42.189167 batch xy loss 6.32085943 batch wh loss 8.71451473 batch obj loss 16.0610714 batch_class_loss 11.0927229 epoch total loss: 12739.7637\n",
            "Trained batch: 300 batch loss: 49.6076431 batch xy loss 7.77754307 batch wh loss 8.17078304 batch obj loss 19.8040314 batch_class_loss 13.8552885 epoch total loss: 12789.3711\n",
            "Trained batch: 301 batch loss: 40.1527786 batch xy loss 6.0903883 batch wh loss 7.120121 batch obj loss 16.1439819 batch_class_loss 10.7982845 epoch total loss: 12829.5234\n",
            "Trained batch: 302 batch loss: 46.1943779 batch xy loss 6.9658947 batch wh loss 7.88448238 batch obj loss 18.3801537 batch_class_loss 12.9638453 epoch total loss: 12875.7178\n",
            "Trained batch: 303 batch loss: 45.8988228 batch xy loss 6.58516026 batch wh loss 10.9286079 batch obj loss 17.6927738 batch_class_loss 10.6922836 epoch total loss: 12921.6162\n",
            "Trained batch: 304 batch loss: 49.3490868 batch xy loss 7.63119411 batch wh loss 10.3663626 batch obj loss 18.8267422 batch_class_loss 12.5247869 epoch total loss: 12970.9648\n",
            "Trained batch: 305 batch loss: 42.2225494 batch xy loss 6.70211792 batch wh loss 7.79198742 batch obj loss 17.30159 batch_class_loss 10.4268532 epoch total loss: 13013.1875\n",
            "Trained batch: 306 batch loss: 33.2874527 batch xy loss 5.23998117 batch wh loss 5.72475243 batch obj loss 14.6823235 batch_class_loss 7.64039421 epoch total loss: 13046.4746\n",
            "Trained batch: 307 batch loss: 46.9323425 batch xy loss 6.70647955 batch wh loss 10.6159229 batch obj loss 18.5500031 batch_class_loss 11.0599365 epoch total loss: 13093.4072\n",
            "Trained batch: 308 batch loss: 40.4718 batch xy loss 5.72727394 batch wh loss 8.93000412 batch obj loss 15.6625385 batch_class_loss 10.1519833 epoch total loss: 13133.8789\n",
            "Trained batch: 309 batch loss: 40.1326904 batch xy loss 5.97196436 batch wh loss 6.36661959 batch obj loss 16.9612427 batch_class_loss 10.8328667 epoch total loss: 13174.0117\n",
            "Trained batch: 310 batch loss: 39.6429443 batch xy loss 5.8752327 batch wh loss 6.45895433 batch obj loss 16.4389954 batch_class_loss 10.8697605 epoch total loss: 13213.6543\n",
            "Trained batch: 311 batch loss: 48.2040672 batch xy loss 7.36983395 batch wh loss 9.76860714 batch obj loss 18.0919209 batch_class_loss 12.9737024 epoch total loss: 13261.8584\n",
            "Trained batch: 312 batch loss: 41.1906738 batch xy loss 6.2204833 batch wh loss 7.92782116 batch obj loss 16.0676365 batch_class_loss 10.9747314 epoch total loss: 13303.0488\n",
            "Trained batch: 313 batch loss: 36.8783951 batch xy loss 5.50723076 batch wh loss 6.19097328 batch obj loss 14.9032078 batch_class_loss 10.2769794 epoch total loss: 13339.9268\n",
            "Trained batch: 314 batch loss: 36.7192955 batch xy loss 6.25044823 batch wh loss 5.98921394 batch obj loss 14.9358206 batch_class_loss 9.54381466 epoch total loss: 13376.6465\n",
            "Trained batch: 315 batch loss: 51.0684204 batch xy loss 7.91794109 batch wh loss 11.2399731 batch obj loss 18.6514359 batch_class_loss 13.2590714 epoch total loss: 13427.7148\n",
            "Trained batch: 316 batch loss: 43.7644386 batch xy loss 6.04138136 batch wh loss 11.3361368 batch obj loss 16.2959976 batch_class_loss 10.0909214 epoch total loss: 13471.4795\n",
            "Trained batch: 317 batch loss: 43.0030289 batch xy loss 6.3796854 batch wh loss 8.01535 batch obj loss 17.1969357 batch_class_loss 11.4110565 epoch total loss: 13514.4824\n",
            "Trained batch: 318 batch loss: 35.9794083 batch xy loss 5.98596144 batch wh loss 6.48965836 batch obj loss 14.4753761 batch_class_loss 9.02841187 epoch total loss: 13550.4619\n",
            "Trained batch: 319 batch loss: 34.0342064 batch xy loss 4.6045 batch wh loss 6.37569618 batch obj loss 14.3058586 batch_class_loss 8.74815 epoch total loss: 13584.4961\n",
            "Trained batch: 320 batch loss: 48.4119339 batch xy loss 7.27018166 batch wh loss 8.94381237 batch obj loss 19.180624 batch_class_loss 13.017313 epoch total loss: 13632.9082\n",
            "Trained batch: 321 batch loss: 43.1059494 batch xy loss 5.95896292 batch wh loss 10.4665146 batch obj loss 16.4367886 batch_class_loss 10.243679 epoch total loss: 13676.0137\n",
            "Trained batch: 322 batch loss: 35.6456757 batch xy loss 4.8573184 batch wh loss 6.14090967 batch obj loss 15.2637978 batch_class_loss 9.38364887 epoch total loss: 13711.6592\n",
            "Trained batch: 323 batch loss: 45.5805817 batch xy loss 6.36903286 batch wh loss 10.6831961 batch obj loss 17.2019463 batch_class_loss 11.3264065 epoch total loss: 13757.2402\n",
            "Trained batch: 324 batch loss: 44.8294067 batch xy loss 6.54527 batch wh loss 8.9845 batch obj loss 17.2759953 batch_class_loss 12.0236406 epoch total loss: 13802.0693\n",
            "Trained batch: 325 batch loss: 44.3541527 batch xy loss 6.67527294 batch wh loss 6.96538544 batch obj loss 18.6892738 batch_class_loss 12.0242195 epoch total loss: 13846.4238\n",
            "Trained batch: 326 batch loss: 44.7776566 batch xy loss 7.01214838 batch wh loss 8.46217 batch obj loss 18.0771255 batch_class_loss 11.2262144 epoch total loss: 13891.2012\n",
            "Trained batch: 327 batch loss: 51.4776535 batch xy loss 7.75152 batch wh loss 10.628046 batch obj loss 19.2612896 batch_class_loss 13.8367977 epoch total loss: 13942.6787\n",
            "Trained batch: 328 batch loss: 55.2433777 batch xy loss 8.12175179 batch wh loss 13.4511604 batch obj loss 19.6089191 batch_class_loss 14.0615463 epoch total loss: 13997.9219\n",
            "Trained batch: 329 batch loss: 34.0698166 batch xy loss 5.08473682 batch wh loss 5.22132492 batch obj loss 14.467577 batch_class_loss 9.29617596 epoch total loss: 14031.9912\n",
            "Trained batch: 330 batch loss: 48.7162476 batch xy loss 7.68727 batch wh loss 11.8646679 batch obj loss 17.8843231 batch_class_loss 11.2799854 epoch total loss: 14080.707\n",
            "Trained batch: 331 batch loss: 50.2275505 batch xy loss 7.40424347 batch wh loss 9.16397476 batch obj loss 19.8122349 batch_class_loss 13.8470974 epoch total loss: 14130.9346\n",
            "Trained batch: 332 batch loss: 43.2012138 batch xy loss 6.69510269 batch wh loss 7.87276649 batch obj loss 17.3576279 batch_class_loss 11.2757158 epoch total loss: 14174.1357\n",
            "Trained batch: 333 batch loss: 44.1527214 batch xy loss 7.14297581 batch wh loss 7.44616508 batch obj loss 18.1930275 batch_class_loss 11.3705559 epoch total loss: 14218.2881\n",
            "Trained batch: 334 batch loss: 35.4918594 batch xy loss 5.62811613 batch wh loss 6.1054039 batch obj loss 14.7545385 batch_class_loss 9.00379944 epoch total loss: 14253.7803\n",
            "Trained batch: 335 batch loss: 41.5578766 batch xy loss 6.2057023 batch wh loss 9.14844704 batch obj loss 15.7559795 batch_class_loss 10.447751 epoch total loss: 14295.3379\n",
            "Trained batch: 336 batch loss: 37.9204636 batch xy loss 5.28856421 batch wh loss 7.58048868 batch obj loss 15.8595724 batch_class_loss 9.19183826 epoch total loss: 14333.2588\n",
            "Trained batch: 337 batch loss: 35.6144447 batch xy loss 4.84360552 batch wh loss 6.26995564 batch obj loss 15.2350121 batch_class_loss 9.265872 epoch total loss: 14368.873\n",
            "Trained batch: 338 batch loss: 49.3453369 batch xy loss 7.37725353 batch wh loss 9.15000057 batch obj loss 19.1811275 batch_class_loss 13.6369534 epoch total loss: 14418.2188\n",
            "Trained batch: 339 batch loss: 35.9917564 batch xy loss 5.68393373 batch wh loss 7.03984976 batch obj loss 14.5187511 batch_class_loss 8.7492218 epoch total loss: 14454.2109\n",
            "Trained batch: 340 batch loss: 56.1078758 batch xy loss 8.72873306 batch wh loss 12.6536188 batch obj loss 20.3840618 batch_class_loss 14.3414612 epoch total loss: 14510.3184\n",
            "Trained batch: 341 batch loss: 43.3317184 batch xy loss 6.55369759 batch wh loss 9.01126 batch obj loss 16.7000675 batch_class_loss 11.0666933 epoch total loss: 14553.6504\n",
            "Trained batch: 342 batch loss: 45.0416451 batch xy loss 6.55717278 batch wh loss 8.4250145 batch obj loss 17.9412251 batch_class_loss 12.1182327 epoch total loss: 14598.6924\n",
            "Trained batch: 343 batch loss: 41.2497864 batch xy loss 6.45737839 batch wh loss 7.89480495 batch obj loss 16.1956825 batch_class_loss 10.7019176 epoch total loss: 14639.9424\n",
            "Trained batch: 344 batch loss: 38.1064453 batch xy loss 5.34809875 batch wh loss 8.38656 batch obj loss 15.3093643 batch_class_loss 9.06242275 epoch total loss: 14678.0488\n",
            "Trained batch: 345 batch loss: 46.7279778 batch xy loss 6.35670519 batch wh loss 9.21299839 batch obj loss 18.7084675 batch_class_loss 12.4498081 epoch total loss: 14724.7764\n",
            "Trained batch: 346 batch loss: 45.6509285 batch xy loss 7.41125202 batch wh loss 9.12593079 batch obj loss 18.1901932 batch_class_loss 10.9235516 epoch total loss: 14770.4277\n",
            "Trained batch: 347 batch loss: 54.5298882 batch xy loss 8.02563477 batch wh loss 11.7918873 batch obj loss 20.5561543 batch_class_loss 14.1562176 epoch total loss: 14824.958\n",
            "Trained batch: 348 batch loss: 41.4541245 batch xy loss 5.79543591 batch wh loss 8.94938946 batch obj loss 16.4048519 batch_class_loss 10.3044472 epoch total loss: 14866.4121\n",
            "Trained batch: 349 batch loss: 39.1638527 batch xy loss 5.96116972 batch wh loss 7.46701956 batch obj loss 15.9152412 batch_class_loss 9.82042313 epoch total loss: 14905.5762\n",
            "Trained batch: 350 batch loss: 45.7384644 batch xy loss 6.27774429 batch wh loss 9.66455841 batch obj loss 17.5949879 batch_class_loss 12.2011738 epoch total loss: 14951.3145\n",
            "Trained batch: 351 batch loss: 35.3504486 batch xy loss 5.31803036 batch wh loss 6.89761877 batch obj loss 14.4832554 batch_class_loss 8.65154266 epoch total loss: 14986.665\n",
            "Trained batch: 352 batch loss: 34.2464447 batch xy loss 4.77440596 batch wh loss 5.23597097 batch obj loss 15.4774408 batch_class_loss 8.75862598 epoch total loss: 15020.9111\n",
            "Trained batch: 353 batch loss: 34.0856781 batch xy loss 5.13859272 batch wh loss 5.8619318 batch obj loss 14.3561 batch_class_loss 8.7290554 epoch total loss: 15054.9971\n",
            "Trained batch: 354 batch loss: 45.0590134 batch xy loss 6.48851 batch wh loss 9.11377 batch obj loss 17.4225235 batch_class_loss 12.0342073 epoch total loss: 15100.0557\n",
            "Trained batch: 355 batch loss: 41.5801086 batch xy loss 6.47663164 batch wh loss 9.3667469 batch obj loss 15.6259155 batch_class_loss 10.1108112 epoch total loss: 15141.6357\n",
            "Trained batch: 356 batch loss: 52.28228 batch xy loss 7.73569584 batch wh loss 9.74786663 batch obj loss 19.9051704 batch_class_loss 14.8935499 epoch total loss: 15193.918\n",
            "Trained batch: 357 batch loss: 40.4496956 batch xy loss 6.24443197 batch wh loss 7.72603941 batch obj loss 16.5693588 batch_class_loss 9.90986633 epoch total loss: 15234.3672\n",
            "Trained batch: 358 batch loss: 37.0561562 batch xy loss 5.63177395 batch wh loss 6.4578824 batch obj loss 14.939621 batch_class_loss 10.0268803 epoch total loss: 15271.4238\n",
            "Trained batch: 359 batch loss: 43.0352936 batch xy loss 6.26369429 batch wh loss 7.75804234 batch obj loss 17.9123631 batch_class_loss 11.1011934 epoch total loss: 15314.459\n",
            "Trained batch: 360 batch loss: 30.5563583 batch xy loss 5.02157593 batch wh loss 5.84328318 batch obj loss 13.1545687 batch_class_loss 6.53692913 epoch total loss: 15345.0156\n",
            "Trained batch: 361 batch loss: 41.554657 batch xy loss 5.99380493 batch wh loss 8.6726532 batch obj loss 16.6252499 batch_class_loss 10.2629519 epoch total loss: 15386.5703\n",
            "Trained batch: 362 batch loss: 39.2966537 batch xy loss 6.21723223 batch wh loss 8.18143749 batch obj loss 15.4849329 batch_class_loss 9.41304874 epoch total loss: 15425.8672\n",
            "Trained batch: 363 batch loss: 42.2555847 batch xy loss 6.49556828 batch wh loss 7.51179028 batch obj loss 17.0308895 batch_class_loss 11.2173405 epoch total loss: 15468.123\n",
            "Trained batch: 364 batch loss: 38.874176 batch xy loss 5.50041151 batch wh loss 9.40624237 batch obj loss 14.381669 batch_class_loss 9.58585167 epoch total loss: 15506.9971\n",
            "Trained batch: 365 batch loss: 39.4636803 batch xy loss 6.32799149 batch wh loss 6.61833334 batch obj loss 15.7340546 batch_class_loss 10.7832985 epoch total loss: 15546.4609\n",
            "Trained batch: 366 batch loss: 39.407692 batch xy loss 5.83467579 batch wh loss 7.11903811 batch obj loss 15.8910294 batch_class_loss 10.5629501 epoch total loss: 15585.8682\n",
            "Trained batch: 367 batch loss: 47.2818413 batch xy loss 7.6158638 batch wh loss 9.02817631 batch obj loss 17.425806 batch_class_loss 13.2119923 epoch total loss: 15633.1504\n",
            "Trained batch: 368 batch loss: 50.4988518 batch xy loss 7.76404428 batch wh loss 9.20690727 batch obj loss 19.1500969 batch_class_loss 14.3778057 epoch total loss: 15683.6494\n",
            "Trained batch: 369 batch loss: 37.7802086 batch xy loss 5.65700626 batch wh loss 6.87960911 batch obj loss 15.7903061 batch_class_loss 9.45328808 epoch total loss: 15721.4297\n",
            "Trained batch: 370 batch loss: 49.447403 batch xy loss 7.55464935 batch wh loss 9.62355804 batch obj loss 19.187706 batch_class_loss 13.0814934 epoch total loss: 15770.877\n",
            "Trained batch: 371 batch loss: 45.8596497 batch xy loss 6.5136652 batch wh loss 11.5573196 batch obj loss 17.4007339 batch_class_loss 10.387928 epoch total loss: 15816.7363\n",
            "Trained batch: 372 batch loss: 34.2913589 batch xy loss 5.22864199 batch wh loss 6.79966688 batch obj loss 14.8302689 batch_class_loss 7.43278265 epoch total loss: 15851.0273\n",
            "Trained batch: 373 batch loss: 33.3445206 batch xy loss 5.45556784 batch wh loss 5.79772806 batch obj loss 14.4325523 batch_class_loss 7.65867 epoch total loss: 15884.3721\n",
            "Trained batch: 374 batch loss: 47.3144302 batch xy loss 6.60299492 batch wh loss 11.1929188 batch obj loss 18.0957909 batch_class_loss 11.4227285 epoch total loss: 15931.6865\n",
            "Trained batch: 375 batch loss: 48.8171425 batch xy loss 7.40125561 batch wh loss 8.7027359 batch obj loss 19.4288292 batch_class_loss 13.2843227 epoch total loss: 15980.5039\n",
            "Trained batch: 376 batch loss: 44.9898911 batch xy loss 6.31034851 batch wh loss 9.43270874 batch obj loss 17.5826778 batch_class_loss 11.6641541 epoch total loss: 16025.4941\n",
            "Trained batch: 377 batch loss: 36.8389435 batch xy loss 5.68963099 batch wh loss 6.83985472 batch obj loss 14.8771067 batch_class_loss 9.43235 epoch total loss: 16062.333\n",
            "Trained batch: 378 batch loss: 45.3086891 batch xy loss 7.72239494 batch wh loss 8.23578262 batch obj loss 17.4664745 batch_class_loss 11.8840399 epoch total loss: 16107.6416\n",
            "Trained batch: 379 batch loss: 47.9917336 batch xy loss 6.62985229 batch wh loss 10.0654974 batch obj loss 18.5055351 batch_class_loss 12.7908478 epoch total loss: 16155.6338\n",
            "Trained batch: 380 batch loss: 40.4304581 batch xy loss 6.05711269 batch wh loss 7.16706467 batch obj loss 16.85 batch_class_loss 10.3562775 epoch total loss: 16196.0645\n",
            "Trained batch: 381 batch loss: 51.6667786 batch xy loss 7.40237904 batch wh loss 11.4915419 batch obj loss 19.2739487 batch_class_loss 13.498908 epoch total loss: 16247.7314\n",
            "Trained batch: 382 batch loss: 40.0970726 batch xy loss 5.62242889 batch wh loss 7.37751245 batch obj loss 16.5422459 batch_class_loss 10.554882 epoch total loss: 16287.8281\n",
            "Trained batch: 383 batch loss: 46.8212814 batch xy loss 6.70735312 batch wh loss 11.7184687 batch obj loss 17.3739204 batch_class_loss 11.0215349 epoch total loss: 16334.6494\n",
            "Trained batch: 384 batch loss: 49.2883911 batch xy loss 7.52570486 batch wh loss 10.3153963 batch obj loss 19.7090912 batch_class_loss 11.7382069 epoch total loss: 16383.9375\n",
            "Trained batch: 385 batch loss: 45.4989929 batch xy loss 6.37720728 batch wh loss 8.31846523 batch obj loss 18.9777946 batch_class_loss 11.8255253 epoch total loss: 16429.4355\n",
            "Trained batch: 386 batch loss: 39.9821701 batch xy loss 6.15626144 batch wh loss 6.62386465 batch obj loss 17.2851982 batch_class_loss 9.91684723 epoch total loss: 16469.418\n",
            "Trained batch: 387 batch loss: 33.7192574 batch xy loss 4.85405159 batch wh loss 6.08792448 batch obj loss 13.7913904 batch_class_loss 8.98589325 epoch total loss: 16503.1367\n",
            "Trained batch: 388 batch loss: 30.7684956 batch xy loss 4.74873257 batch wh loss 5.40381336 batch obj loss 12.9400511 batch_class_loss 7.6759 epoch total loss: 16533.9043\n",
            "Trained batch: 389 batch loss: 46.6391144 batch xy loss 6.828475 batch wh loss 8.62570477 batch obj loss 18.7885628 batch_class_loss 12.396368 epoch total loss: 16580.543\n",
            "Trained batch: 390 batch loss: 48.1945953 batch xy loss 6.74406815 batch wh loss 10.62889 batch obj loss 18.2253685 batch_class_loss 12.5962658 epoch total loss: 16628.7383\n",
            "Trained batch: 391 batch loss: 40.6598511 batch xy loss 6.08331347 batch wh loss 7.84096432 batch obj loss 15.8415985 batch_class_loss 10.8939743 epoch total loss: 16669.3984\n",
            "Trained batch: 392 batch loss: 44.9184875 batch xy loss 6.17917252 batch wh loss 10.3401022 batch obj loss 16.7100105 batch_class_loss 11.6892023 epoch total loss: 16714.3164\n",
            "Trained batch: 393 batch loss: 43.1516342 batch xy loss 6.2241292 batch wh loss 9.25362873 batch obj loss 16.21105 batch_class_loss 11.462822 epoch total loss: 16757.4688\n",
            "Trained batch: 394 batch loss: 40.6790123 batch xy loss 6.47328758 batch wh loss 8.40567112 batch obj loss 15.2326107 batch_class_loss 10.5674429 epoch total loss: 16798.1484\n",
            "Trained batch: 395 batch loss: 45.9215927 batch xy loss 5.9498415 batch wh loss 11.1129198 batch obj loss 17.5118885 batch_class_loss 11.346941 epoch total loss: 16844.0703\n",
            "Trained batch: 396 batch loss: 42.7366562 batch xy loss 6.71017075 batch wh loss 8.12894821 batch obj loss 17.4573956 batch_class_loss 10.4401426 epoch total loss: 16886.8066\n",
            "Trained batch: 397 batch loss: 42.7447052 batch xy loss 5.55004835 batch wh loss 10.7727413 batch obj loss 16.1410122 batch_class_loss 10.280899 epoch total loss: 16929.5508\n",
            "Trained batch: 398 batch loss: 35.7192497 batch xy loss 4.84302664 batch wh loss 6.53224659 batch obj loss 15.8518848 batch_class_loss 8.49209118 epoch total loss: 16965.2695\n",
            "Trained batch: 399 batch loss: 46.3430176 batch xy loss 5.89449692 batch wh loss 10.4743109 batch obj loss 18.2352867 batch_class_loss 11.738924 epoch total loss: 17011.6133\n",
            "Trained batch: 400 batch loss: 38.6412354 batch xy loss 5.73286533 batch wh loss 7.74269867 batch obj loss 15.8794975 batch_class_loss 9.28617096 epoch total loss: 17050.2539\n",
            "Trained batch: 401 batch loss: 44.3850708 batch xy loss 6.38305283 batch wh loss 8.25856495 batch obj loss 18.1527405 batch_class_loss 11.5907145 epoch total loss: 17094.6387\n",
            "Trained batch: 402 batch loss: 43.1359482 batch xy loss 6.74218369 batch wh loss 8.36579895 batch obj loss 17.4615612 batch_class_loss 10.5664024 epoch total loss: 17137.7754\n",
            "Trained batch: 403 batch loss: 48.2902298 batch xy loss 8.10868 batch wh loss 9.17825 batch obj loss 18.5897141 batch_class_loss 12.4135885 epoch total loss: 17186.0664\n",
            "Trained batch: 404 batch loss: 49.0188828 batch xy loss 7.50416613 batch wh loss 8.88207817 batch obj loss 19.2841949 batch_class_loss 13.348444 epoch total loss: 17235.0859\n",
            "Trained batch: 405 batch loss: 38.3811951 batch xy loss 5.51684666 batch wh loss 7.1735754 batch obj loss 15.791851 batch_class_loss 9.89892483 epoch total loss: 17273.4668\n",
            "Trained batch: 406 batch loss: 42.3068504 batch xy loss 5.86286545 batch wh loss 8.94603729 batch obj loss 17.0638 batch_class_loss 10.4341478 epoch total loss: 17315.7734\n",
            "Trained batch: 407 batch loss: 46.9480247 batch xy loss 6.65843105 batch wh loss 11.4826355 batch obj loss 17.6240196 batch_class_loss 11.1829386 epoch total loss: 17362.7207\n",
            "Trained batch: 408 batch loss: 43.8180466 batch xy loss 6.22717857 batch wh loss 9.99881649 batch obj loss 16.3168221 batch_class_loss 11.2752285 epoch total loss: 17406.5391\n",
            "Trained batch: 409 batch loss: 43.7079849 batch xy loss 6.20277262 batch wh loss 10.0843115 batch obj loss 16.2475185 batch_class_loss 11.1733828 epoch total loss: 17450.2461\n",
            "Trained batch: 410 batch loss: 48.942421 batch xy loss 7.0860672 batch wh loss 12.5225143 batch obj loss 17.3850098 batch_class_loss 11.9488249 epoch total loss: 17499.1895\n",
            "Trained batch: 411 batch loss: 45.2530022 batch xy loss 6.98535 batch wh loss 8.6132822 batch obj loss 17.4159527 batch_class_loss 12.2384148 epoch total loss: 17544.4434\n",
            "Trained batch: 412 batch loss: 43.4804192 batch xy loss 7.227139 batch wh loss 8.70872116 batch obj loss 16.6222343 batch_class_loss 10.9223232 epoch total loss: 17587.9238\n",
            "Trained batch: 413 batch loss: 53.4550781 batch xy loss 7.79244661 batch wh loss 12.557971 batch obj loss 20.0158482 batch_class_loss 13.0888138 epoch total loss: 17641.3789\n",
            "Trained batch: 414 batch loss: 44.1236267 batch xy loss 6.74396276 batch wh loss 8.64579868 batch obj loss 17.3189545 batch_class_loss 11.4149094 epoch total loss: 17685.502\n",
            "Trained batch: 415 batch loss: 37.9260101 batch xy loss 5.51125336 batch wh loss 8.19863605 batch obj loss 14.7561245 batch_class_loss 9.45999527 epoch total loss: 17723.4277\n",
            "Trained batch: 416 batch loss: 39.1104126 batch xy loss 5.90863943 batch wh loss 7.62918282 batch obj loss 16.4402428 batch_class_loss 9.13234901 epoch total loss: 17762.5391\n",
            "Trained batch: 417 batch loss: 37.3824387 batch xy loss 5.37653637 batch wh loss 6.10378 batch obj loss 15.7848949 batch_class_loss 10.1172285 epoch total loss: 17799.9219\n",
            "Trained batch: 418 batch loss: 40.001503 batch xy loss 6.17692757 batch wh loss 7.87312698 batch obj loss 15.6184235 batch_class_loss 10.3330212 epoch total loss: 17839.9238\n",
            "Trained batch: 419 batch loss: 40.8314514 batch xy loss 5.90995693 batch wh loss 8.5381279 batch obj loss 15.6737719 batch_class_loss 10.7095976 epoch total loss: 17880.7559\n",
            "Trained batch: 420 batch loss: 47.1066437 batch xy loss 6.76258802 batch wh loss 10.281086 batch obj loss 18.0271587 batch_class_loss 12.0358095 epoch total loss: 17927.8633\n",
            "Trained batch: 421 batch loss: 38.1411476 batch xy loss 6.18280506 batch wh loss 7.26853 batch obj loss 15.1799021 batch_class_loss 9.50991058 epoch total loss: 17966.0039\n",
            "Trained batch: 422 batch loss: 47.8467216 batch xy loss 7.71734953 batch wh loss 11.8896523 batch obj loss 17.4452114 batch_class_loss 10.7945099 epoch total loss: 18013.8516\n",
            "Trained batch: 423 batch loss: 42.3551064 batch xy loss 5.97251654 batch wh loss 9.61750603 batch obj loss 16.1528263 batch_class_loss 10.6122541 epoch total loss: 18056.207\n",
            "Trained batch: 424 batch loss: 44.4553146 batch xy loss 5.83234 batch wh loss 9.87406158 batch obj loss 17.4624462 batch_class_loss 11.2864723 epoch total loss: 18100.6621\n",
            "Trained batch: 425 batch loss: 43.4021606 batch xy loss 6.77448654 batch wh loss 9.02583122 batch obj loss 16.8388691 batch_class_loss 10.7629738 epoch total loss: 18144.0645\n",
            "Trained batch: 426 batch loss: 31.6578503 batch xy loss 4.65636921 batch wh loss 5.88102818 batch obj loss 13.3858185 batch_class_loss 7.73463392 epoch total loss: 18175.7227\n",
            "Trained batch: 427 batch loss: 32.4310493 batch xy loss 4.05345154 batch wh loss 5.76067972 batch obj loss 14.6664705 batch_class_loss 7.95044422 epoch total loss: 18208.1543\n",
            "Trained batch: 428 batch loss: 42.6003723 batch xy loss 6.02732468 batch wh loss 10.1241913 batch obj loss 15.9632435 batch_class_loss 10.4856119 epoch total loss: 18250.7539\n",
            "Trained batch: 429 batch loss: 42.0901794 batch xy loss 6.15674686 batch wh loss 8.82520294 batch obj loss 16.0278721 batch_class_loss 11.0803566 epoch total loss: 18292.8438\n",
            "Trained batch: 430 batch loss: 40.7553329 batch xy loss 6.30465126 batch wh loss 7.05690479 batch obj loss 15.885932 batch_class_loss 11.5078449 epoch total loss: 18333.6\n",
            "Trained batch: 431 batch loss: 38.3476944 batch xy loss 5.68194914 batch wh loss 6.57789135 batch obj loss 15.5972071 batch_class_loss 10.4906454 epoch total loss: 18371.9473\n",
            "Trained batch: 432 batch loss: 44.2749557 batch xy loss 6.63676786 batch wh loss 8.92611885 batch obj loss 17.0628242 batch_class_loss 11.6492443 epoch total loss: 18416.2227\n",
            "Trained batch: 433 batch loss: 16.4852486 batch xy loss 2.26800561 batch wh loss 1.92875361 batch obj loss 7.37076235 batch_class_loss 4.91772699 epoch total loss: 18432.707\n",
            "20201008-155927 Epoch 3 train loss 42.56976318359375, total train batches 433, 79.997802734375 examples per second\n",
            "3 , 6.336670 , 8.530343 , 10.916584 , 16.786160 , 42.569763 , 17.399727\n",
            "\n",
            "20201008-160139 Epoch 3 val loss 17.39972686767578, total val batches 191, 92.03758239746094 examples per second\n",
            "Model /content/gdrive/My Drive/results/epoch-3-loss-17.400.h5 saved.\n",
            "20201008-160140 Started epoch 4 with learning rate 0.01. Current LR patience count is 1 epochs. Last lowest train loss is 42.56976318359375. Last lowest val loss is 17.39972686767578.\n",
            "Trained batch: 1 batch loss: 32.5677299 batch xy loss 4.80395031 batch wh loss 6.28763962 batch obj loss 12.8908424 batch_class_loss 8.58529472 epoch total loss: 32.5677299\n",
            "Trained batch: 2 batch loss: 40.6445 batch xy loss 6.03177834 batch wh loss 8.13850117 batch obj loss 15.1221199 batch_class_loss 11.3521051 epoch total loss: 73.2122345\n",
            "Trained batch: 3 batch loss: 38.7605515 batch xy loss 5.67548752 batch wh loss 7.27146435 batch obj loss 15.7890844 batch_class_loss 10.0245218 epoch total loss: 111.972786\n",
            "Trained batch: 4 batch loss: 44.276741 batch xy loss 6.1451478 batch wh loss 8.25279 batch obj loss 17.4756 batch_class_loss 12.4031982 epoch total loss: 156.249527\n",
            "Trained batch: 5 batch loss: 34.4242897 batch xy loss 5.29704762 batch wh loss 7.7369895 batch obj loss 13.7864037 batch_class_loss 7.60384703 epoch total loss: 190.673813\n",
            "Trained batch: 6 batch loss: 33.6122742 batch xy loss 4.94626331 batch wh loss 6.84610939 batch obj loss 14.0421667 batch_class_loss 7.77773428 epoch total loss: 224.286087\n",
            "Trained batch: 7 batch loss: 40.1336632 batch xy loss 6.1895566 batch wh loss 8.14171219 batch obj loss 15.6578674 batch_class_loss 10.1445284 epoch total loss: 264.419739\n",
            "Trained batch: 8 batch loss: 42.2965393 batch xy loss 6.57333946 batch wh loss 8.54932117 batch obj loss 16.0222683 batch_class_loss 11.1516075 epoch total loss: 306.716278\n",
            "Trained batch: 9 batch loss: 47.9761887 batch xy loss 7.22533798 batch wh loss 8.56756783 batch obj loss 19.4801865 batch_class_loss 12.7030983 epoch total loss: 354.692474\n",
            "Trained batch: 10 batch loss: 29.4953346 batch xy loss 4.4515028 batch wh loss 4.99599695 batch obj loss 12.5418606 batch_class_loss 7.50597143 epoch total loss: 384.187805\n",
            "Trained batch: 11 batch loss: 44.3817444 batch xy loss 6.89599085 batch wh loss 8.50272655 batch obj loss 17.535 batch_class_loss 11.4480267 epoch total loss: 428.56955\n",
            "Trained batch: 12 batch loss: 43.7645912 batch xy loss 7.22249699 batch wh loss 8.90253353 batch obj loss 16.3695488 batch_class_loss 11.2700129 epoch total loss: 472.334137\n",
            "Trained batch: 13 batch loss: 52.4364128 batch xy loss 7.79927826 batch wh loss 10.292016 batch obj loss 20.5662766 batch_class_loss 13.7788391 epoch total loss: 524.770569\n",
            "Trained batch: 14 batch loss: 38.7413101 batch xy loss 5.88143444 batch wh loss 9.04975796 batch obj loss 14.9696274 batch_class_loss 8.84048653 epoch total loss: 563.511902\n",
            "Trained batch: 15 batch loss: 46.592804 batch xy loss 6.79019165 batch wh loss 8.8963728 batch obj loss 18.5563717 batch_class_loss 12.3498688 epoch total loss: 610.104736\n",
            "Trained batch: 16 batch loss: 32.6023674 batch xy loss 4.64768696 batch wh loss 5.79265213 batch obj loss 13.8750715 batch_class_loss 8.28695583 epoch total loss: 642.707092\n",
            "Trained batch: 17 batch loss: 42.3286972 batch xy loss 6.04458761 batch wh loss 7.32281065 batch obj loss 17.3767052 batch_class_loss 11.5845947 epoch total loss: 685.035767\n",
            "Trained batch: 18 batch loss: 47.5749359 batch xy loss 6.87376785 batch wh loss 10.084794 batch obj loss 18.4609871 batch_class_loss 12.1553812 epoch total loss: 732.610718\n",
            "Trained batch: 19 batch loss: 38.3856888 batch xy loss 6.05855799 batch wh loss 6.88817596 batch obj loss 16.0155735 batch_class_loss 9.42337894 epoch total loss: 770.996399\n",
            "Trained batch: 20 batch loss: 48.5618172 batch xy loss 7.17849588 batch wh loss 9.69311237 batch obj loss 18.5343132 batch_class_loss 13.1558943 epoch total loss: 819.558228\n",
            "Trained batch: 21 batch loss: 47.5451279 batch xy loss 7.85078239 batch wh loss 9.17333221 batch obj loss 17.8898869 batch_class_loss 12.6311302 epoch total loss: 867.103333\n",
            "Trained batch: 22 batch loss: 36.5108948 batch xy loss 5.1315856 batch wh loss 8.29629421 batch obj loss 13.9415407 batch_class_loss 9.14147186 epoch total loss: 903.614258\n",
            "Trained batch: 23 batch loss: 42.5628738 batch xy loss 5.07657576 batch wh loss 12.2582474 batch obj loss 15.2331133 batch_class_loss 9.99494171 epoch total loss: 946.177124\n",
            "Trained batch: 24 batch loss: 33.0574 batch xy loss 4.96367598 batch wh loss 5.62578773 batch obj loss 13.8098335 batch_class_loss 8.65810108 epoch total loss: 979.234497\n",
            "Trained batch: 25 batch loss: 39.8450165 batch xy loss 6.18647671 batch wh loss 8.45003128 batch obj loss 15.3294811 batch_class_loss 9.87902546 epoch total loss: 1019.07953\n",
            "Trained batch: 26 batch loss: 31.4874821 batch xy loss 4.7996 batch wh loss 6.67489815 batch obj loss 12.6774712 batch_class_loss 7.33551264 epoch total loss: 1050.56702\n",
            "Trained batch: 27 batch loss: 39.7747574 batch xy loss 5.07342815 batch wh loss 8.51907539 batch obj loss 15.3112297 batch_class_loss 10.8710251 epoch total loss: 1090.3418\n",
            "Trained batch: 28 batch loss: 39.049633 batch xy loss 5.70334291 batch wh loss 7.3443284 batch obj loss 15.7303162 batch_class_loss 10.2716484 epoch total loss: 1129.39148\n",
            "Trained batch: 29 batch loss: 27.4906063 batch xy loss 3.98380423 batch wh loss 3.82304716 batch obj loss 12.0437069 batch_class_loss 7.64004803 epoch total loss: 1156.88208\n",
            "Trained batch: 30 batch loss: 44.5268364 batch xy loss 6.80947399 batch wh loss 8.36878204 batch obj loss 17.1966152 batch_class_loss 12.1519642 epoch total loss: 1201.40894\n",
            "Trained batch: 31 batch loss: 37.9618454 batch xy loss 5.89625454 batch wh loss 5.8882041 batch obj loss 16.1648941 batch_class_loss 10.0124922 epoch total loss: 1239.37073\n",
            "Trained batch: 32 batch loss: 41.2901344 batch xy loss 5.907547 batch wh loss 7.60705757 batch obj loss 16.7056122 batch_class_loss 11.0699196 epoch total loss: 1280.66089\n",
            "Trained batch: 33 batch loss: 40.4044342 batch xy loss 5.45673752 batch wh loss 7.50838614 batch obj loss 16.5671558 batch_class_loss 10.8721523 epoch total loss: 1321.06531\n",
            "Trained batch: 34 batch loss: 41.1791878 batch xy loss 6.48333263 batch wh loss 8.44342518 batch obj loss 15.71278 batch_class_loss 10.5396471 epoch total loss: 1362.24451\n",
            "Trained batch: 35 batch loss: 37.4055328 batch xy loss 5.09705448 batch wh loss 7.15207863 batch obj loss 15.549264 batch_class_loss 9.60714 epoch total loss: 1399.65\n",
            "Trained batch: 36 batch loss: 33.1716194 batch xy loss 5.26565409 batch wh loss 5.65870953 batch obj loss 13.8773479 batch_class_loss 8.36990833 epoch total loss: 1432.82166\n",
            "Trained batch: 37 batch loss: 43.1361046 batch xy loss 6.28119373 batch wh loss 8.89508343 batch obj loss 16.7252846 batch_class_loss 11.2345428 epoch total loss: 1475.95776\n",
            "Trained batch: 38 batch loss: 44.6556396 batch xy loss 6.55025291 batch wh loss 9.79219818 batch obj loss 17.1651554 batch_class_loss 11.1480331 epoch total loss: 1520.6134\n",
            "Trained batch: 39 batch loss: 52.9284554 batch xy loss 7.55017757 batch wh loss 10.5591087 batch obj loss 19.9297848 batch_class_loss 14.8893852 epoch total loss: 1573.54187\n",
            "Trained batch: 40 batch loss: 44.47855 batch xy loss 6.08552599 batch wh loss 10.3077116 batch obj loss 17.3135948 batch_class_loss 10.7717152 epoch total loss: 1618.02039\n",
            "Trained batch: 41 batch loss: 44.4572792 batch xy loss 6.4862 batch wh loss 11.0207424 batch obj loss 16.3767338 batch_class_loss 10.5736065 epoch total loss: 1662.47766\n",
            "Trained batch: 42 batch loss: 34.7355881 batch xy loss 5.87462759 batch wh loss 6.11794853 batch obj loss 14.4892206 batch_class_loss 8.25379 epoch total loss: 1697.21326\n",
            "Trained batch: 43 batch loss: 35.4006882 batch xy loss 5.74749088 batch wh loss 6.43616819 batch obj loss 14.8700972 batch_class_loss 8.34693241 epoch total loss: 1732.61389\n",
            "Trained batch: 44 batch loss: 49.5024605 batch xy loss 7.24384594 batch wh loss 9.07473373 batch obj loss 19.6972675 batch_class_loss 13.4866171 epoch total loss: 1782.11633\n",
            "Trained batch: 45 batch loss: 40.4178581 batch xy loss 5.50780487 batch wh loss 7.00269604 batch obj loss 16.8201904 batch_class_loss 11.0871677 epoch total loss: 1822.53418\n",
            "Trained batch: 46 batch loss: 40.989357 batch xy loss 5.33624792 batch wh loss 10.4812012 batch obj loss 15.2075548 batch_class_loss 9.96435 epoch total loss: 1863.52356\n",
            "Trained batch: 47 batch loss: 33.5374 batch xy loss 5.15594053 batch wh loss 5.27976179 batch obj loss 14.3546486 batch_class_loss 8.74705 epoch total loss: 1897.06091\n",
            "Trained batch: 48 batch loss: 39.2995796 batch xy loss 6.36911631 batch wh loss 6.61188459 batch obj loss 15.394392 batch_class_loss 10.9241924 epoch total loss: 1936.36047\n",
            "Trained batch: 49 batch loss: 42.4738541 batch xy loss 5.57957172 batch wh loss 8.32044888 batch obj loss 17.0487499 batch_class_loss 11.5250864 epoch total loss: 1978.83435\n",
            "Trained batch: 50 batch loss: 47.1478271 batch xy loss 6.44780731 batch wh loss 10.6674709 batch obj loss 17.8100777 batch_class_loss 12.2224751 epoch total loss: 2025.98218\n",
            "Trained batch: 51 batch loss: 46.1928215 batch xy loss 6.9346714 batch wh loss 9.1394186 batch obj loss 18.2114716 batch_class_loss 11.9072599 epoch total loss: 2072.17505\n",
            "Trained batch: 52 batch loss: 40.7305183 batch xy loss 5.7428236 batch wh loss 7.76387215 batch obj loss 16.4562759 batch_class_loss 10.7675467 epoch total loss: 2112.90552\n",
            "Trained batch: 53 batch loss: 43.1082764 batch xy loss 6.20046329 batch wh loss 9.66589546 batch obj loss 16.9972057 batch_class_loss 10.2447138 epoch total loss: 2156.01367\n",
            "Trained batch: 54 batch loss: 45.2128639 batch xy loss 6.57141972 batch wh loss 8.15714073 batch obj loss 19.0101261 batch_class_loss 11.4741745 epoch total loss: 2201.22656\n",
            "Trained batch: 55 batch loss: 32.4753418 batch xy loss 4.73662853 batch wh loss 5.5100255 batch obj loss 14.8450451 batch_class_loss 7.38364697 epoch total loss: 2233.7019\n",
            "Trained batch: 56 batch loss: 49.4243164 batch xy loss 7.47160721 batch wh loss 10.2897358 batch obj loss 18.8211784 batch_class_loss 12.8417883 epoch total loss: 2283.12622\n",
            "Trained batch: 57 batch loss: 34.9415779 batch xy loss 5.16834307 batch wh loss 7.01877117 batch obj loss 14.1907406 batch_class_loss 8.56372166 epoch total loss: 2318.06787\n",
            "Trained batch: 58 batch loss: 39.5664673 batch xy loss 5.99881458 batch wh loss 6.40656853 batch obj loss 16.7162094 batch_class_loss 10.4448757 epoch total loss: 2357.63428\n",
            "Trained batch: 59 batch loss: 37.0194168 batch xy loss 5.32536125 batch wh loss 6.02678394 batch obj loss 15.5117168 batch_class_loss 10.15555 epoch total loss: 2394.65381\n",
            "Trained batch: 60 batch loss: 38.1236076 batch xy loss 5.40990496 batch wh loss 7.2832737 batch obj loss 15.2959499 batch_class_loss 10.1344767 epoch total loss: 2432.77734\n",
            "Trained batch: 61 batch loss: 38.7542686 batch xy loss 5.50197601 batch wh loss 7.09253502 batch obj loss 15.6452837 batch_class_loss 10.514472 epoch total loss: 2471.53149\n",
            "Trained batch: 62 batch loss: 48.7976913 batch xy loss 6.69802284 batch wh loss 9.717103 batch obj loss 18.570734 batch_class_loss 13.8118372 epoch total loss: 2520.3291\n",
            "Trained batch: 63 batch loss: 38.9810944 batch xy loss 5.8951683 batch wh loss 7.42160511 batch obj loss 15.0538549 batch_class_loss 10.6104622 epoch total loss: 2559.3103\n",
            "Trained batch: 64 batch loss: 43.4933243 batch xy loss 7.12663651 batch wh loss 8.8592062 batch obj loss 16.4889488 batch_class_loss 11.0185299 epoch total loss: 2602.80371\n",
            "Trained batch: 65 batch loss: 41.6927834 batch xy loss 6.3280139 batch wh loss 7.95761442 batch obj loss 16.798048 batch_class_loss 10.609107 epoch total loss: 2644.49658\n",
            "Trained batch: 66 batch loss: 45.0233574 batch xy loss 6.62704659 batch wh loss 9.76935 batch obj loss 17.5628967 batch_class_loss 11.0640593 epoch total loss: 2689.52\n",
            "Trained batch: 67 batch loss: 36.4399796 batch xy loss 5.00843334 batch wh loss 8.52443886 batch obj loss 14.3676901 batch_class_loss 8.53941727 epoch total loss: 2725.96\n",
            "Trained batch: 68 batch loss: 37.4348717 batch xy loss 6.08169365 batch wh loss 7.46002483 batch obj loss 15.4010324 batch_class_loss 8.49212265 epoch total loss: 2763.39478\n",
            "Trained batch: 69 batch loss: 41.4915543 batch xy loss 6.66897869 batch wh loss 9.17255497 batch obj loss 15.9588814 batch_class_loss 9.69114 epoch total loss: 2804.88623\n",
            "Trained batch: 70 batch loss: 50.3573799 batch xy loss 6.98759794 batch wh loss 10.6190081 batch obj loss 19.3943291 batch_class_loss 13.3564434 epoch total loss: 2855.24365\n",
            "Trained batch: 71 batch loss: 44.8064308 batch xy loss 6.33021736 batch wh loss 7.45947886 batch obj loss 18.7342148 batch_class_loss 12.2825203 epoch total loss: 2900.05\n",
            "Trained batch: 72 batch loss: 39.9321671 batch xy loss 5.43347502 batch wh loss 8.66371441 batch obj loss 16.035099 batch_class_loss 9.79987907 epoch total loss: 2939.98218\n",
            "Trained batch: 73 batch loss: 45.2126541 batch xy loss 6.25800037 batch wh loss 10.2702494 batch obj loss 17.268631 batch_class_loss 11.4157696 epoch total loss: 2985.19482\n",
            "Trained batch: 74 batch loss: 37.0941887 batch xy loss 5.37609863 batch wh loss 7.13521051 batch obj loss 14.9244385 batch_class_loss 9.65844059 epoch total loss: 3022.28906\n",
            "Trained batch: 75 batch loss: 40.1961555 batch xy loss 5.89983559 batch wh loss 7.75152254 batch obj loss 16.2425213 batch_class_loss 10.3022738 epoch total loss: 3062.48511\n",
            "Trained batch: 76 batch loss: 43.2755966 batch xy loss 6.73427296 batch wh loss 9.14060593 batch obj loss 17.1705551 batch_class_loss 10.2301626 epoch total loss: 3105.76074\n",
            "Trained batch: 77 batch loss: 40.7952309 batch xy loss 6.21168375 batch wh loss 7.40563345 batch obj loss 16.1453094 batch_class_loss 11.0326033 epoch total loss: 3146.55591\n",
            "Trained batch: 78 batch loss: 39.845253 batch xy loss 5.59872961 batch wh loss 8.49907684 batch obj loss 15.6802788 batch_class_loss 10.0671673 epoch total loss: 3186.40112\n",
            "Trained batch: 79 batch loss: 40.7293243 batch xy loss 6.33337688 batch wh loss 8.53325176 batch obj loss 15.901309 batch_class_loss 9.96138763 epoch total loss: 3227.13037\n",
            "Trained batch: 80 batch loss: 45.3227921 batch xy loss 6.49777794 batch wh loss 9.05036545 batch obj loss 17.1698246 batch_class_loss 12.6048241 epoch total loss: 3272.45312\n",
            "Trained batch: 81 batch loss: 41.880455 batch xy loss 6.25186729 batch wh loss 7.4233532 batch obj loss 17.4928093 batch_class_loss 10.712429 epoch total loss: 3314.3335\n",
            "Trained batch: 82 batch loss: 40.0407791 batch xy loss 5.51992226 batch wh loss 9.15522385 batch obj loss 15.8133459 batch_class_loss 9.55229 epoch total loss: 3354.37427\n",
            "Trained batch: 83 batch loss: 45.1266708 batch xy loss 7.03179932 batch wh loss 8.38967896 batch obj loss 17.8155746 batch_class_loss 11.8896208 epoch total loss: 3399.50098\n",
            "Trained batch: 84 batch loss: 39.1005554 batch xy loss 5.66168213 batch wh loss 8.88909912 batch obj loss 15.0744991 batch_class_loss 9.47527218 epoch total loss: 3438.60156\n",
            "Trained batch: 85 batch loss: 40.6746292 batch xy loss 5.82052135 batch wh loss 9.52118111 batch obj loss 16.0920048 batch_class_loss 9.24092484 epoch total loss: 3479.27612\n",
            "Trained batch: 86 batch loss: 47.6464691 batch xy loss 6.84854794 batch wh loss 9.47643566 batch obj loss 18.8424015 batch_class_loss 12.4790783 epoch total loss: 3526.92261\n",
            "Trained batch: 87 batch loss: 44.0662346 batch xy loss 6.58554459 batch wh loss 8.55476761 batch obj loss 17.7635136 batch_class_loss 11.1624079 epoch total loss: 3570.98877\n",
            "Trained batch: 88 batch loss: 38.6157875 batch xy loss 5.29874659 batch wh loss 8.09466457 batch obj loss 15.7145081 batch_class_loss 9.50786686 epoch total loss: 3609.60449\n",
            "Trained batch: 89 batch loss: 37.5824089 batch xy loss 5.60924578 batch wh loss 6.94051075 batch obj loss 15.1751509 batch_class_loss 9.8575 epoch total loss: 3647.18701\n",
            "Trained batch: 90 batch loss: 45.5654411 batch xy loss 6.60210133 batch wh loss 8.4764967 batch obj loss 18.1789722 batch_class_loss 12.3078699 epoch total loss: 3692.75244\n",
            "Trained batch: 91 batch loss: 38.5592117 batch xy loss 6.52911 batch wh loss 6.98960686 batch obj loss 15.3571949 batch_class_loss 9.6833 epoch total loss: 3731.31177\n",
            "Trained batch: 92 batch loss: 43.792 batch xy loss 7.03500891 batch wh loss 8.2686615 batch obj loss 17.2859535 batch_class_loss 11.2023783 epoch total loss: 3775.10376\n",
            "Trained batch: 93 batch loss: 43.1739349 batch xy loss 6.68795252 batch wh loss 8.85522079 batch obj loss 16.8841248 batch_class_loss 10.7466364 epoch total loss: 3818.27759\n",
            "Trained batch: 94 batch loss: 48.0602264 batch xy loss 6.60171032 batch wh loss 9.70086 batch obj loss 19.0032501 batch_class_loss 12.7544136 epoch total loss: 3866.33789\n",
            "Trained batch: 95 batch loss: 36.6032104 batch xy loss 5.51079845 batch wh loss 6.309165 batch obj loss 15.4560919 batch_class_loss 9.32715225 epoch total loss: 3902.94116\n",
            "Trained batch: 96 batch loss: 53.166378 batch xy loss 7.60822153 batch wh loss 10.7283583 batch obj loss 20.8675079 batch_class_loss 13.9622917 epoch total loss: 3956.10742\n",
            "Trained batch: 97 batch loss: 40.4363174 batch xy loss 6.2871623 batch wh loss 8.41657829 batch obj loss 15.808733 batch_class_loss 9.92384434 epoch total loss: 3996.5437\n",
            "Trained batch: 98 batch loss: 49.7327271 batch xy loss 7.437253 batch wh loss 9.45392895 batch obj loss 19.6657486 batch_class_loss 13.1757956 epoch total loss: 4046.27637\n",
            "Trained batch: 99 batch loss: 47.3182 batch xy loss 7.46775627 batch wh loss 10.0949564 batch obj loss 17.8483162 batch_class_loss 11.9071732 epoch total loss: 4093.59448\n",
            "Trained batch: 100 batch loss: 37.0184059 batch xy loss 5.49120569 batch wh loss 6.47749186 batch obj loss 15.5707102 batch_class_loss 9.47899628 epoch total loss: 4130.61279\n",
            "Trained batch: 101 batch loss: 40.9703789 batch xy loss 5.82783461 batch wh loss 8.57498455 batch obj loss 16.1292877 batch_class_loss 10.4382744 epoch total loss: 4171.58301\n",
            "Trained batch: 102 batch loss: 32.6332054 batch xy loss 4.7104454 batch wh loss 6.6353426 batch obj loss 13.9657898 batch_class_loss 7.32162523 epoch total loss: 4204.21631\n",
            "Trained batch: 103 batch loss: 40.8486671 batch xy loss 6.15058899 batch wh loss 7.37918663 batch obj loss 16.5096664 batch_class_loss 10.8092279 epoch total loss: 4245.06494\n",
            "Trained batch: 104 batch loss: 42.1573334 batch xy loss 5.85912895 batch wh loss 7.72526693 batch obj loss 16.9570847 batch_class_loss 11.6158485 epoch total loss: 4287.22217\n",
            "Trained batch: 105 batch loss: 44.0131454 batch xy loss 6.44339848 batch wh loss 9.26064873 batch obj loss 16.7093945 batch_class_loss 11.5997019 epoch total loss: 4331.23535\n",
            "Trained batch: 106 batch loss: 34.4352798 batch xy loss 4.92658663 batch wh loss 6.35497665 batch obj loss 14.1075993 batch_class_loss 9.04611874 epoch total loss: 4365.67041\n",
            "Trained batch: 107 batch loss: 39.9116135 batch xy loss 6.20979595 batch wh loss 6.45159674 batch obj loss 15.8378239 batch_class_loss 11.4123955 epoch total loss: 4405.58203\n",
            "Trained batch: 108 batch loss: 39.4840546 batch xy loss 6.29035378 batch wh loss 8.04959679 batch obj loss 15.1115646 batch_class_loss 10.0325403 epoch total loss: 4445.06592\n",
            "Trained batch: 109 batch loss: 39.0395432 batch xy loss 6.0947504 batch wh loss 6.38929224 batch obj loss 15.5880089 batch_class_loss 10.9674902 epoch total loss: 4484.10547\n",
            "Trained batch: 110 batch loss: 37.1089516 batch xy loss 4.76516724 batch wh loss 6.69402885 batch obj loss 15.2740688 batch_class_loss 10.3756857 epoch total loss: 4521.21436\n",
            "Trained batch: 111 batch loss: 42.1871719 batch xy loss 6.56137514 batch wh loss 6.9013238 batch obj loss 17.3392582 batch_class_loss 11.3852167 epoch total loss: 4563.40137\n",
            "Trained batch: 112 batch loss: 30.0542183 batch xy loss 4.87133741 batch wh loss 4.69294119 batch obj loss 12.6456671 batch_class_loss 7.84427452 epoch total loss: 4593.45557\n",
            "Trained batch: 113 batch loss: 46.3575859 batch xy loss 6.31602764 batch wh loss 10.8104362 batch obj loss 18.0228767 batch_class_loss 11.2082443 epoch total loss: 4639.81299\n",
            "Trained batch: 114 batch loss: 35.686512 batch xy loss 5.83073807 batch wh loss 7.13695335 batch obj loss 14.3683252 batch_class_loss 8.3504982 epoch total loss: 4675.49951\n",
            "Trained batch: 115 batch loss: 51.720974 batch xy loss 7.9865756 batch wh loss 11.6218147 batch obj loss 18.6015472 batch_class_loss 13.5110397 epoch total loss: 4727.2207\n",
            "Trained batch: 116 batch loss: 39.459938 batch xy loss 6.01261091 batch wh loss 8.51287 batch obj loss 15.0509644 batch_class_loss 9.88349438 epoch total loss: 4766.68066\n",
            "Trained batch: 117 batch loss: 33.282856 batch xy loss 4.85791206 batch wh loss 6.79767656 batch obj loss 13.559371 batch_class_loss 8.06789875 epoch total loss: 4799.96338\n",
            "Trained batch: 118 batch loss: 45.1568527 batch xy loss 6.94512558 batch wh loss 9.74607754 batch obj loss 17.349472 batch_class_loss 11.1161804 epoch total loss: 4845.12\n",
            "Trained batch: 119 batch loss: 42.1899071 batch xy loss 6.02038383 batch wh loss 8.31471348 batch obj loss 17.3945694 batch_class_loss 10.4602356 epoch total loss: 4887.31\n",
            "Trained batch: 120 batch loss: 38.0370483 batch xy loss 5.33388615 batch wh loss 6.42886257 batch obj loss 16.2026196 batch_class_loss 10.071682 epoch total loss: 4925.34717\n",
            "Trained batch: 121 batch loss: 43.9780273 batch xy loss 6.63217545 batch wh loss 10.0269566 batch obj loss 16.7043343 batch_class_loss 10.614562 epoch total loss: 4969.3252\n",
            "Trained batch: 122 batch loss: 35.5400887 batch xy loss 5.30553627 batch wh loss 6.66172695 batch obj loss 14.46488 batch_class_loss 9.1079464 epoch total loss: 5004.86523\n",
            "Trained batch: 123 batch loss: 43.3467407 batch xy loss 6.28158283 batch wh loss 11.4461031 batch obj loss 15.775979 batch_class_loss 9.8430748 epoch total loss: 5048.21191\n",
            "Trained batch: 124 batch loss: 37.1399078 batch xy loss 5.24385643 batch wh loss 7.42723274 batch obj loss 15.5024567 batch_class_loss 8.96636295 epoch total loss: 5085.35205\n",
            "Trained batch: 125 batch loss: 42.2158127 batch xy loss 5.06978607 batch wh loss 10.156702 batch obj loss 16.4581947 batch_class_loss 10.531126 epoch total loss: 5127.56787\n",
            "Trained batch: 126 batch loss: 48.8238831 batch xy loss 7.64723682 batch wh loss 10.0634747 batch obj loss 18.2280903 batch_class_loss 12.8850822 epoch total loss: 5176.3916\n",
            "Trained batch: 127 batch loss: 36.5451088 batch xy loss 5.65575457 batch wh loss 7.72927952 batch obj loss 14.1654415 batch_class_loss 8.99463367 epoch total loss: 5212.93652\n",
            "Trained batch: 128 batch loss: 40.9236794 batch xy loss 6.25534058 batch wh loss 8.38068771 batch obj loss 15.7526817 batch_class_loss 10.5349665 epoch total loss: 5253.86035\n",
            "Trained batch: 129 batch loss: 36.154686 batch xy loss 5.71328211 batch wh loss 7.1312232 batch obj loss 14.0722389 batch_class_loss 9.23794 epoch total loss: 5290.01514\n",
            "Trained batch: 130 batch loss: 38.384 batch xy loss 5.38840437 batch wh loss 7.10200882 batch obj loss 16.3476028 batch_class_loss 9.54598236 epoch total loss: 5328.39893\n",
            "Trained batch: 131 batch loss: 40.1514473 batch xy loss 6.51675892 batch wh loss 8.13987637 batch obj loss 16.4463768 batch_class_loss 9.04844 epoch total loss: 5368.55029\n",
            "Trained batch: 132 batch loss: 40.9984474 batch xy loss 6.06970835 batch wh loss 7.08275223 batch obj loss 16.5899887 batch_class_loss 11.2559967 epoch total loss: 5409.54883\n",
            "Trained batch: 133 batch loss: 46.3904 batch xy loss 7.53067303 batch wh loss 9.08405876 batch obj loss 18.0232658 batch_class_loss 11.7524052 epoch total loss: 5455.93945\n",
            "Trained batch: 134 batch loss: 42.3544388 batch xy loss 6.15846682 batch wh loss 8.38491821 batch obj loss 16.2030582 batch_class_loss 11.6079979 epoch total loss: 5498.29395\n",
            "Trained batch: 135 batch loss: 42.5486412 batch xy loss 6.06578255 batch wh loss 8.3140049 batch obj loss 16.5239563 batch_class_loss 11.6449013 epoch total loss: 5540.84277\n",
            "Trained batch: 136 batch loss: 42.7855492 batch xy loss 6.21140242 batch wh loss 8.28120613 batch obj loss 16.8404846 batch_class_loss 11.4524565 epoch total loss: 5583.62842\n",
            "Trained batch: 137 batch loss: 59.2826576 batch xy loss 8.1862278 batch wh loss 16.0584393 batch obj loss 20.2330933 batch_class_loss 14.8048964 epoch total loss: 5642.91113\n",
            "Trained batch: 138 batch loss: 42.3234253 batch xy loss 6.57883263 batch wh loss 7.76720285 batch obj loss 17.1096916 batch_class_loss 10.8676949 epoch total loss: 5685.23438\n",
            "Trained batch: 139 batch loss: 46.9034538 batch xy loss 7.34474897 batch wh loss 10.051712 batch obj loss 17.2650394 batch_class_loss 12.2419538 epoch total loss: 5732.1377\n",
            "Trained batch: 140 batch loss: 36.8026733 batch xy loss 5.76999903 batch wh loss 6.90911436 batch obj loss 15.1369047 batch_class_loss 8.98665619 epoch total loss: 5768.94043\n",
            "Trained batch: 141 batch loss: 42.0565643 batch xy loss 6.03232479 batch wh loss 8.25104904 batch obj loss 17.0116501 batch_class_loss 10.7615395 epoch total loss: 5810.99707\n",
            "Trained batch: 142 batch loss: 35.8626289 batch xy loss 5.32229519 batch wh loss 6.0640049 batch obj loss 15.7281733 batch_class_loss 8.74815559 epoch total loss: 5846.86\n",
            "Trained batch: 143 batch loss: 35.2968788 batch xy loss 5.73631048 batch wh loss 5.46720791 batch obj loss 15.1802759 batch_class_loss 8.91308308 epoch total loss: 5882.15674\n",
            "Trained batch: 144 batch loss: 43.4167595 batch xy loss 6.1982193 batch wh loss 8.33766365 batch obj loss 18.1658745 batch_class_loss 10.7150021 epoch total loss: 5925.57373\n",
            "Trained batch: 145 batch loss: 48.3828735 batch xy loss 7.89454699 batch wh loss 9.16104412 batch obj loss 18.9207649 batch_class_loss 12.4065142 epoch total loss: 5973.95654\n",
            "Trained batch: 146 batch loss: 43.8395805 batch xy loss 7.28773069 batch wh loss 9.20427418 batch obj loss 16.9654369 batch_class_loss 10.3821363 epoch total loss: 6017.7959\n",
            "Trained batch: 147 batch loss: 34.3051262 batch xy loss 5.65899754 batch wh loss 4.80935 batch obj loss 15.0121126 batch_class_loss 8.82466793 epoch total loss: 6052.10107\n",
            "Trained batch: 148 batch loss: 38.6663437 batch xy loss 5.97825527 batch wh loss 6.19368887 batch obj loss 15.7722979 batch_class_loss 10.7220993 epoch total loss: 6090.76758\n",
            "Trained batch: 149 batch loss: 44.7387772 batch xy loss 5.71406698 batch wh loss 10.5243464 batch obj loss 16.8696766 batch_class_loss 11.6306868 epoch total loss: 6135.50635\n",
            "Trained batch: 150 batch loss: 51.3717 batch xy loss 7.54958725 batch wh loss 11.4175806 batch obj loss 18.7993259 batch_class_loss 13.6052065 epoch total loss: 6186.87793\n",
            "Trained batch: 151 batch loss: 39.2251434 batch xy loss 6.26270485 batch wh loss 6.41893053 batch obj loss 16.4862671 batch_class_loss 10.0572386 epoch total loss: 6226.10303\n",
            "Trained batch: 152 batch loss: 41.4362869 batch xy loss 6.27218771 batch wh loss 8.54630184 batch obj loss 15.9333687 batch_class_loss 10.6844282 epoch total loss: 6267.53955\n",
            "Trained batch: 153 batch loss: 46.6688805 batch xy loss 6.93983412 batch wh loss 8.03972721 batch obj loss 18.6029682 batch_class_loss 13.0863514 epoch total loss: 6314.2085\n",
            "Trained batch: 154 batch loss: 43.1656303 batch xy loss 6.28036404 batch wh loss 7.64103031 batch obj loss 17.8585758 batch_class_loss 11.3856592 epoch total loss: 6357.37402\n",
            "Trained batch: 155 batch loss: 38.4685364 batch xy loss 5.25690603 batch wh loss 7.03914976 batch obj loss 16.080658 batch_class_loss 10.0918236 epoch total loss: 6395.84277\n",
            "Trained batch: 156 batch loss: 41.276989 batch xy loss 6.04012775 batch wh loss 7.9854517 batch obj loss 16.4701042 batch_class_loss 10.7813044 epoch total loss: 6437.11963\n",
            "Trained batch: 157 batch loss: 39.17836 batch xy loss 6.28061056 batch wh loss 7.09873533 batch obj loss 15.6009216 batch_class_loss 10.1980925 epoch total loss: 6476.29785\n",
            "Trained batch: 158 batch loss: 45.5788689 batch xy loss 6.93174696 batch wh loss 9.60357666 batch obj loss 17.2640228 batch_class_loss 11.7795248 epoch total loss: 6521.87695\n",
            "Trained batch: 159 batch loss: 48.6869659 batch xy loss 6.90635824 batch wh loss 10.3427086 batch obj loss 18.965704 batch_class_loss 12.4721928 epoch total loss: 6570.56396\n",
            "Trained batch: 160 batch loss: 35.4231529 batch xy loss 5.05053234 batch wh loss 6.99611664 batch obj loss 14.7775288 batch_class_loss 8.59897709 epoch total loss: 6605.9873\n",
            "Trained batch: 161 batch loss: 42.0154877 batch xy loss 5.70745754 batch wh loss 7.34860706 batch obj loss 17.3388748 batch_class_loss 11.6205511 epoch total loss: 6648.00293\n",
            "Trained batch: 162 batch loss: 33.2155495 batch xy loss 4.95906639 batch wh loss 6.3017869 batch obj loss 14.0457745 batch_class_loss 7.90892124 epoch total loss: 6681.21826\n",
            "Trained batch: 163 batch loss: 39.0073814 batch xy loss 6.24111652 batch wh loss 8.0411 batch obj loss 15.3557596 batch_class_loss 9.3694067 epoch total loss: 6720.22559\n",
            "Trained batch: 164 batch loss: 39.939 batch xy loss 5.53614855 batch wh loss 7.5118928 batch obj loss 15.9464722 batch_class_loss 10.9444847 epoch total loss: 6760.16455\n",
            "Trained batch: 165 batch loss: 54.3364906 batch xy loss 7.58204508 batch wh loss 12.431776 batch obj loss 20.1300545 batch_class_loss 14.1926117 epoch total loss: 6814.50098\n",
            "Trained batch: 166 batch loss: 41.7593 batch xy loss 6.08654594 batch wh loss 8.13867283 batch obj loss 16.9723148 batch_class_loss 10.5617638 epoch total loss: 6856.26025\n",
            "Trained batch: 167 batch loss: 35.3611031 batch xy loss 6.00090313 batch wh loss 5.6523118 batch obj loss 14.7600355 batch_class_loss 8.94785309 epoch total loss: 6891.62158\n",
            "Trained batch: 168 batch loss: 41.7838364 batch xy loss 5.89148235 batch wh loss 9.07372475 batch obj loss 16.0441017 batch_class_loss 10.7745275 epoch total loss: 6933.40527\n",
            "Trained batch: 169 batch loss: 38.6727638 batch xy loss 5.17477322 batch wh loss 7.95023251 batch obj loss 15.7657375 batch_class_loss 9.78202057 epoch total loss: 6972.07812\n",
            "Trained batch: 170 batch loss: 52.523468 batch xy loss 8.09765625 batch wh loss 10.554882 batch obj loss 19.917469 batch_class_loss 13.9534636 epoch total loss: 7024.60156\n",
            "Trained batch: 171 batch loss: 40.8054619 batch xy loss 5.92803383 batch wh loss 7.91724634 batch obj loss 16.8365631 batch_class_loss 10.1236181 epoch total loss: 7065.40723\n",
            "Trained batch: 172 batch loss: 46.5077477 batch xy loss 7.28000259 batch wh loss 8.86574745 batch obj loss 18.5043888 batch_class_loss 11.8576069 epoch total loss: 7111.91504\n",
            "Trained batch: 173 batch loss: 42.7204399 batch xy loss 6.54548168 batch wh loss 9.48004341 batch obj loss 16.255743 batch_class_loss 10.4391699 epoch total loss: 7154.63525\n",
            "Trained batch: 174 batch loss: 40.9434738 batch xy loss 6.03446674 batch wh loss 8.02778625 batch obj loss 16.4739532 batch_class_loss 10.4072666 epoch total loss: 7195.57861\n",
            "Trained batch: 175 batch loss: 43.6825523 batch xy loss 6.03087664 batch wh loss 8.29857445 batch obj loss 17.7115421 batch_class_loss 11.6415586 epoch total loss: 7239.26123\n",
            "Trained batch: 176 batch loss: 54.0343 batch xy loss 7.81775475 batch wh loss 10.8345976 batch obj loss 20.2737961 batch_class_loss 15.1081505 epoch total loss: 7293.29541\n",
            "Trained batch: 177 batch loss: 41.3853951 batch xy loss 5.7888 batch wh loss 8.16101456 batch obj loss 16.6358032 batch_class_loss 10.7997789 epoch total loss: 7334.68066\n",
            "Trained batch: 178 batch loss: 38.9990158 batch xy loss 5.8735919 batch wh loss 8.00375175 batch obj loss 15.9896078 batch_class_loss 9.13206291 epoch total loss: 7373.67969\n",
            "Trained batch: 179 batch loss: 38.5447 batch xy loss 5.89042044 batch wh loss 7.52661705 batch obj loss 15.7584524 batch_class_loss 9.36920738 epoch total loss: 7412.22461\n",
            "Trained batch: 180 batch loss: 33.5183334 batch xy loss 4.95830631 batch wh loss 7.45835 batch obj loss 13.5129347 batch_class_loss 7.58873796 epoch total loss: 7445.74316\n",
            "Trained batch: 181 batch loss: 41.8862381 batch xy loss 6.00021648 batch wh loss 9.27587795 batch obj loss 17.1456871 batch_class_loss 9.46445751 epoch total loss: 7487.62939\n",
            "Trained batch: 182 batch loss: 42.2574539 batch xy loss 6.73201227 batch wh loss 7.5718 batch obj loss 17.040432 batch_class_loss 10.913209 epoch total loss: 7529.88672\n",
            "Trained batch: 183 batch loss: 36.517704 batch xy loss 5.52949619 batch wh loss 6.21561384 batch obj loss 15.75284 batch_class_loss 9.01975441 epoch total loss: 7566.4043\n",
            "Trained batch: 184 batch loss: 42.5427704 batch xy loss 6.14200211 batch wh loss 9.46545792 batch obj loss 15.8489799 batch_class_loss 11.0863304 epoch total loss: 7608.94727\n",
            "Trained batch: 185 batch loss: 44.279213 batch xy loss 6.81058741 batch wh loss 8.39977264 batch obj loss 16.7451267 batch_class_loss 12.3237247 epoch total loss: 7653.22656\n",
            "Trained batch: 186 batch loss: 39.8467255 batch xy loss 6.44802809 batch wh loss 8.01842594 batch obj loss 15.5086727 batch_class_loss 9.8716 epoch total loss: 7693.07324\n",
            "Trained batch: 187 batch loss: 47.3144417 batch xy loss 7.09685469 batch wh loss 7.81534147 batch obj loss 19.054678 batch_class_loss 13.3475676 epoch total loss: 7740.3877\n",
            "Trained batch: 188 batch loss: 41.2947464 batch xy loss 6.26630831 batch wh loss 7.82633591 batch obj loss 15.6661015 batch_class_loss 11.5360022 epoch total loss: 7781.68262\n",
            "Trained batch: 189 batch loss: 47.8952293 batch xy loss 6.41576147 batch wh loss 11.2851839 batch obj loss 18.303997 batch_class_loss 11.8902826 epoch total loss: 7829.57764\n",
            "Trained batch: 190 batch loss: 35.9674377 batch xy loss 5.72507668 batch wh loss 6.55892181 batch obj loss 14.7460594 batch_class_loss 8.93738174 epoch total loss: 7865.54492\n",
            "Trained batch: 191 batch loss: 44.174839 batch xy loss 6.51285934 batch wh loss 8.23383331 batch obj loss 18.2678394 batch_class_loss 11.160305 epoch total loss: 7909.71973\n",
            "Trained batch: 192 batch loss: 36.085762 batch xy loss 5.42174101 batch wh loss 5.9669795 batch obj loss 15.2184582 batch_class_loss 9.47858429 epoch total loss: 7945.80566\n",
            "Trained batch: 193 batch loss: 48.0041962 batch xy loss 7.28103161 batch wh loss 11.0438585 batch obj loss 17.9070606 batch_class_loss 11.7722464 epoch total loss: 7993.81\n",
            "Trained batch: 194 batch loss: 35.4575882 batch xy loss 5.29921818 batch wh loss 7.84301329 batch obj loss 13.8238335 batch_class_loss 8.49152374 epoch total loss: 8029.26758\n",
            "Trained batch: 195 batch loss: 42.5483704 batch xy loss 7.17288971 batch wh loss 8.48191261 batch obj loss 16.8480568 batch_class_loss 10.0455122 epoch total loss: 8071.81592\n",
            "Trained batch: 196 batch loss: 33.7436104 batch xy loss 5.09178734 batch wh loss 5.28456879 batch obj loss 13.974123 batch_class_loss 9.39313221 epoch total loss: 8105.55957\n",
            "Trained batch: 197 batch loss: 38.2178 batch xy loss 6.22431946 batch wh loss 8.20698929 batch obj loss 14.8244734 batch_class_loss 8.96201515 epoch total loss: 8143.77734\n",
            "Trained batch: 198 batch loss: 31.1018257 batch xy loss 5.20900059 batch wh loss 5.21438551 batch obj loss 13.1272821 batch_class_loss 7.55115604 epoch total loss: 8174.87939\n",
            "Trained batch: 199 batch loss: 41.4093742 batch xy loss 5.47114754 batch wh loss 8.78194237 batch obj loss 16.1470547 batch_class_loss 11.0092287 epoch total loss: 8216.28906\n",
            "Trained batch: 200 batch loss: 41.1670074 batch xy loss 5.78079319 batch wh loss 8.67774105 batch obj loss 15.7871437 batch_class_loss 10.9213285 epoch total loss: 8257.45605\n",
            "Trained batch: 201 batch loss: 47.5882111 batch xy loss 7.55856419 batch wh loss 9.65909481 batch obj loss 18.4272537 batch_class_loss 11.9432936 epoch total loss: 8305.04395\n",
            "Trained batch: 202 batch loss: 46.8007889 batch xy loss 7.1824007 batch wh loss 8.88358 batch obj loss 18.3912811 batch_class_loss 12.3435278 epoch total loss: 8351.84473\n",
            "Trained batch: 203 batch loss: 36.8565331 batch xy loss 5.57807255 batch wh loss 7.23972607 batch obj loss 15.0324459 batch_class_loss 9.00628853 epoch total loss: 8388.70117\n",
            "Trained batch: 204 batch loss: 39.39114 batch xy loss 5.84909487 batch wh loss 7.78031778 batch obj loss 15.9394131 batch_class_loss 9.82231331 epoch total loss: 8428.09277\n",
            "Trained batch: 205 batch loss: 38.7700043 batch xy loss 5.58638477 batch wh loss 8.31808376 batch obj loss 15.3776674 batch_class_loss 9.4878664 epoch total loss: 8466.8623\n",
            "Trained batch: 206 batch loss: 47.825695 batch xy loss 7.36586285 batch wh loss 9.76939 batch obj loss 18.9212875 batch_class_loss 11.7691536 epoch total loss: 8514.68848\n",
            "Trained batch: 207 batch loss: 42.5879517 batch xy loss 6.24412537 batch wh loss 7.7117033 batch obj loss 17.2493782 batch_class_loss 11.3827486 epoch total loss: 8557.27637\n",
            "Trained batch: 208 batch loss: 52.3140793 batch xy loss 7.90447 batch wh loss 10.2714148 batch obj loss 19.9550514 batch_class_loss 14.1831408 epoch total loss: 8609.59082\n",
            "Trained batch: 209 batch loss: 37.2203331 batch xy loss 5.78356266 batch wh loss 7.04375696 batch obj loss 14.7839737 batch_class_loss 9.60903931 epoch total loss: 8646.81152\n",
            "Trained batch: 210 batch loss: 33.7834244 batch xy loss 5.5729866 batch wh loss 5.12374592 batch obj loss 14.5329084 batch_class_loss 8.55378532 epoch total loss: 8680.59473\n",
            "Trained batch: 211 batch loss: 38.1716957 batch xy loss 5.55690956 batch wh loss 7.69176102 batch obj loss 15.4620447 batch_class_loss 9.46098137 epoch total loss: 8718.7666\n",
            "Trained batch: 212 batch loss: 41.0666122 batch xy loss 6.28127527 batch wh loss 9.58219337 batch obj loss 14.83568 batch_class_loss 10.367466 epoch total loss: 8759.83301\n",
            "Trained batch: 213 batch loss: 41.5985298 batch xy loss 6.32874107 batch wh loss 7.3092761 batch obj loss 16.7783432 batch_class_loss 11.1821728 epoch total loss: 8801.43164\n",
            "Trained batch: 214 batch loss: 37.9946823 batch xy loss 5.90372038 batch wh loss 6.79715443 batch obj loss 16.0368443 batch_class_loss 9.25696278 epoch total loss: 8839.42676\n",
            "Trained batch: 215 batch loss: 46.1880417 batch xy loss 6.91725254 batch wh loss 9.06758308 batch obj loss 18.0604534 batch_class_loss 12.1427536 epoch total loss: 8885.61523\n",
            "Trained batch: 216 batch loss: 51.0177193 batch xy loss 7.64184 batch wh loss 9.44626617 batch obj loss 20.4784622 batch_class_loss 13.4511528 epoch total loss: 8936.63281\n",
            "Trained batch: 217 batch loss: 51.8748055 batch xy loss 7.2788229 batch wh loss 11.1765289 batch obj loss 19.9490471 batch_class_loss 13.4704075 epoch total loss: 8988.50781\n",
            "Trained batch: 218 batch loss: 40.93787 batch xy loss 6.15143394 batch wh loss 9.02446175 batch obj loss 15.5871983 batch_class_loss 10.1747789 epoch total loss: 9029.44531\n",
            "Trained batch: 219 batch loss: 45.8301544 batch xy loss 6.72772408 batch wh loss 8.83740425 batch obj loss 18.027 batch_class_loss 12.2380295 epoch total loss: 9075.27539\n",
            "Trained batch: 220 batch loss: 38.4273491 batch xy loss 5.3255949 batch wh loss 9.17221737 batch obj loss 15.1774139 batch_class_loss 8.75212097 epoch total loss: 9113.70312\n",
            "Trained batch: 221 batch loss: 38.2016 batch xy loss 5.78334141 batch wh loss 7.41507912 batch obj loss 15.8178816 batch_class_loss 9.18529 epoch total loss: 9151.9043\n",
            "Trained batch: 222 batch loss: 49.2826462 batch xy loss 7.4656353 batch wh loss 9.36247253 batch obj loss 19.5296707 batch_class_loss 12.9248686 epoch total loss: 9201.18652\n",
            "Trained batch: 223 batch loss: 41.5771675 batch xy loss 5.97792721 batch wh loss 8.00853539 batch obj loss 16.9947376 batch_class_loss 10.5959692 epoch total loss: 9242.76367\n",
            "Trained batch: 224 batch loss: 33.8449936 batch xy loss 5.20292 batch wh loss 6.02482176 batch obj loss 13.9854374 batch_class_loss 8.63181782 epoch total loss: 9276.6084\n",
            "Trained batch: 225 batch loss: 43.9327316 batch xy loss 7.31798124 batch wh loss 8.95423794 batch obj loss 16.6459541 batch_class_loss 11.0145597 epoch total loss: 9320.54102\n",
            "Trained batch: 226 batch loss: 53.2650719 batch xy loss 8.33910942 batch wh loss 12.5561619 batch obj loss 19.0199986 batch_class_loss 13.3497982 epoch total loss: 9373.80566\n",
            "Trained batch: 227 batch loss: 40.5346756 batch xy loss 6.12672567 batch wh loss 7.66376 batch obj loss 16.7875061 batch_class_loss 9.95668 epoch total loss: 9414.34082\n",
            "Trained batch: 228 batch loss: 42.2063065 batch xy loss 5.81625 batch wh loss 8.53578281 batch obj loss 17.3783436 batch_class_loss 10.4759312 epoch total loss: 9456.54688\n",
            "Trained batch: 229 batch loss: 46.2982635 batch xy loss 7.0769763 batch wh loss 8.29078674 batch obj loss 18.1932583 batch_class_loss 12.7372437 epoch total loss: 9502.84473\n",
            "Trained batch: 230 batch loss: 47.3011627 batch xy loss 7.48408937 batch wh loss 8.52493095 batch obj loss 18.4869556 batch_class_loss 12.8051872 epoch total loss: 9550.14551\n",
            "Trained batch: 231 batch loss: 38.2982521 batch xy loss 5.85752201 batch wh loss 7.800035 batch obj loss 15.5369272 batch_class_loss 9.10377121 epoch total loss: 9588.44336\n",
            "Trained batch: 232 batch loss: 34.6519318 batch xy loss 5.58128262 batch wh loss 6.03881025 batch obj loss 14.6374121 batch_class_loss 8.3944273 epoch total loss: 9623.0957\n",
            "Trained batch: 233 batch loss: 47.1986084 batch xy loss 7.24701691 batch wh loss 7.77275229 batch obj loss 19.3165722 batch_class_loss 12.8622684 epoch total loss: 9670.29395\n",
            "Trained batch: 234 batch loss: 35.0416565 batch xy loss 5.98611593 batch wh loss 6.19588137 batch obj loss 14.2632141 batch_class_loss 8.59644318 epoch total loss: 9705.33594\n",
            "Trained batch: 235 batch loss: 45.3552094 batch xy loss 6.83199596 batch wh loss 8.68359756 batch obj loss 17.8020248 batch_class_loss 12.037591 epoch total loss: 9750.69141\n",
            "Trained batch: 236 batch loss: 50.010334 batch xy loss 7.47712708 batch wh loss 10.0286903 batch obj loss 19.1534958 batch_class_loss 13.3510227 epoch total loss: 9800.70215\n",
            "Trained batch: 237 batch loss: 35.8569717 batch xy loss 5.35093403 batch wh loss 6.92577887 batch obj loss 14.6519985 batch_class_loss 8.92826176 epoch total loss: 9836.56\n",
            "Trained batch: 238 batch loss: 42.704422 batch xy loss 6.27438211 batch wh loss 8.49938202 batch obj loss 16.7948608 batch_class_loss 11.1357956 epoch total loss: 9879.26367\n",
            "Trained batch: 239 batch loss: 41.5920639 batch xy loss 6.011168 batch wh loss 8.30069828 batch obj loss 16.0917969 batch_class_loss 11.1884012 epoch total loss: 9920.85547\n",
            "Trained batch: 240 batch loss: 34.1174583 batch xy loss 5.32332563 batch wh loss 5.97831202 batch obj loss 14.6485243 batch_class_loss 8.16729546 epoch total loss: 9954.97266\n",
            "Trained batch: 241 batch loss: 38.5107574 batch xy loss 6.12354612 batch wh loss 7.01044512 batch obj loss 15.9998722 batch_class_loss 9.376894 epoch total loss: 9993.4834\n",
            "Trained batch: 242 batch loss: 40.8419151 batch xy loss 5.99142075 batch wh loss 6.62732315 batch obj loss 16.7922897 batch_class_loss 11.4308825 epoch total loss: 10034.3252\n",
            "Trained batch: 243 batch loss: 39.7779045 batch xy loss 6.1176734 batch wh loss 6.95916462 batch obj loss 16.1608944 batch_class_loss 10.5401726 epoch total loss: 10074.1035\n",
            "Trained batch: 244 batch loss: 48.8684769 batch xy loss 7.43575096 batch wh loss 9.54544926 batch obj loss 19.0518589 batch_class_loss 12.8354187 epoch total loss: 10122.9717\n",
            "Trained batch: 245 batch loss: 48.1327667 batch xy loss 7.56462574 batch wh loss 9.70581245 batch obj loss 18.2466469 batch_class_loss 12.6156797 epoch total loss: 10171.1045\n",
            "Trained batch: 246 batch loss: 49.8700333 batch xy loss 7.54535484 batch wh loss 9.41642284 batch obj loss 19.8103733 batch_class_loss 13.0978813 epoch total loss: 10220.9746\n",
            "Trained batch: 247 batch loss: 40.3947678 batch xy loss 5.5748558 batch wh loss 9.03944302 batch obj loss 16.2656059 batch_class_loss 9.51486588 epoch total loss: 10261.3691\n",
            "Trained batch: 248 batch loss: 45.458252 batch xy loss 6.97275972 batch wh loss 8.28992748 batch obj loss 18.2791805 batch_class_loss 11.9163837 epoch total loss: 10306.8271\n",
            "Trained batch: 249 batch loss: 38.3862801 batch xy loss 5.75210142 batch wh loss 8.76423454 batch obj loss 15.1114559 batch_class_loss 8.75849152 epoch total loss: 10345.2139\n",
            "Trained batch: 250 batch loss: 51.3747826 batch xy loss 7.56181526 batch wh loss 10.5959644 batch obj loss 19.7332802 batch_class_loss 13.4837227 epoch total loss: 10396.5889\n",
            "Trained batch: 251 batch loss: 37.4696388 batch xy loss 6.00295544 batch wh loss 7.58195972 batch obj loss 14.8683796 batch_class_loss 9.01634407 epoch total loss: 10434.0586\n",
            "Trained batch: 252 batch loss: 43.4132347 batch xy loss 5.88715458 batch wh loss 10.6687927 batch obj loss 16.1990719 batch_class_loss 10.6582127 epoch total loss: 10477.4717\n",
            "Trained batch: 253 batch loss: 40.1140747 batch xy loss 6.38055658 batch wh loss 7.75967789 batch obj loss 16.253809 batch_class_loss 9.72003174 epoch total loss: 10517.5859\n",
            "Trained batch: 254 batch loss: 35.7646828 batch xy loss 5.01008368 batch wh loss 6.31821489 batch obj loss 15.168561 batch_class_loss 9.26782322 epoch total loss: 10553.3506\n",
            "Trained batch: 255 batch loss: 37.4117546 batch xy loss 5.75990772 batch wh loss 6.96587086 batch obj loss 14.9235535 batch_class_loss 9.76242447 epoch total loss: 10590.7627\n",
            "Trained batch: 256 batch loss: 41.5461464 batch xy loss 6.32689619 batch wh loss 8.8881321 batch obj loss 15.9331064 batch_class_loss 10.3980093 epoch total loss: 10632.3086\n",
            "Trained batch: 257 batch loss: 32.2388268 batch xy loss 4.61120129 batch wh loss 5.33309698 batch obj loss 13.9681597 batch_class_loss 8.32636642 epoch total loss: 10664.5479\n",
            "Trained batch: 258 batch loss: 42.9207 batch xy loss 6.42786264 batch wh loss 8.70054531 batch obj loss 16.835062 batch_class_loss 10.9572268 epoch total loss: 10707.4688\n",
            "Trained batch: 259 batch loss: 43.1666565 batch xy loss 5.53011131 batch wh loss 9.22587585 batch obj loss 17.5031567 batch_class_loss 10.9075108 epoch total loss: 10750.6357\n",
            "Trained batch: 260 batch loss: 44.7082901 batch xy loss 6.94848347 batch wh loss 8.36545944 batch obj loss 17.5551414 batch_class_loss 11.839201 epoch total loss: 10795.3438\n",
            "Trained batch: 261 batch loss: 40.5876 batch xy loss 5.77365732 batch wh loss 9.89108658 batch obj loss 15.3373117 batch_class_loss 9.58554745 epoch total loss: 10835.9316\n",
            "Trained batch: 262 batch loss: 49.1033325 batch xy loss 6.88719606 batch wh loss 10.9492331 batch obj loss 18.2875156 batch_class_loss 12.9793892 epoch total loss: 10885.0352\n",
            "Trained batch: 263 batch loss: 34.9904137 batch xy loss 5.06455135 batch wh loss 5.85221481 batch obj loss 14.7488337 batch_class_loss 9.32481384 epoch total loss: 10920.0254\n",
            "Trained batch: 264 batch loss: 42.3350067 batch xy loss 6.03285265 batch wh loss 10.0536833 batch obj loss 16.8203106 batch_class_loss 9.42816067 epoch total loss: 10962.3604\n",
            "Trained batch: 265 batch loss: 35.5844803 batch xy loss 5.54572105 batch wh loss 5.44798374 batch obj loss 15.2928619 batch_class_loss 9.29791451 epoch total loss: 10997.9453\n",
            "Trained batch: 266 batch loss: 40.7668724 batch xy loss 6.01877308 batch wh loss 8.42763 batch obj loss 15.9180336 batch_class_loss 10.4024315 epoch total loss: 11038.7119\n",
            "Trained batch: 267 batch loss: 40.0802498 batch xy loss 5.89655972 batch wh loss 7.92805099 batch obj loss 16.3478928 batch_class_loss 9.90774727 epoch total loss: 11078.792\n",
            "Trained batch: 268 batch loss: 31.7209511 batch xy loss 4.49725771 batch wh loss 4.99362 batch obj loss 13.7132654 batch_class_loss 8.51680946 epoch total loss: 11110.5127\n",
            "Trained batch: 269 batch loss: 36.9366417 batch xy loss 5.724823 batch wh loss 6.39993238 batch obj loss 14.8131008 batch_class_loss 9.99878502 epoch total loss: 11147.4492\n",
            "Trained batch: 270 batch loss: 37.225647 batch xy loss 5.14138317 batch wh loss 6.13489246 batch obj loss 15.3610983 batch_class_loss 10.588273 epoch total loss: 11184.6748\n",
            "Trained batch: 271 batch loss: 41.4350052 batch xy loss 6.45739269 batch wh loss 6.71878147 batch obj loss 16.5789871 batch_class_loss 11.679842 epoch total loss: 11226.1094\n",
            "Trained batch: 272 batch loss: 50.3040543 batch xy loss 6.84883404 batch wh loss 10.2549486 batch obj loss 19.3154354 batch_class_loss 13.8848333 epoch total loss: 11276.4131\n",
            "Trained batch: 273 batch loss: 34.4672661 batch xy loss 4.91776752 batch wh loss 6.26845932 batch obj loss 14.1624527 batch_class_loss 9.11858749 epoch total loss: 11310.8799\n",
            "Trained batch: 274 batch loss: 38.1832199 batch xy loss 5.75215387 batch wh loss 7.24917221 batch obj loss 14.9350662 batch_class_loss 10.2468262 epoch total loss: 11349.0635\n",
            "Trained batch: 275 batch loss: 42.1010818 batch xy loss 6.29291677 batch wh loss 8.98944378 batch obj loss 16.268074 batch_class_loss 10.5506535 epoch total loss: 11391.165\n",
            "Trained batch: 276 batch loss: 42.5185661 batch xy loss 6.7262 batch wh loss 8.30856895 batch obj loss 16.8439827 batch_class_loss 10.6398144 epoch total loss: 11433.6836\n",
            "Trained batch: 277 batch loss: 46.2804832 batch xy loss 7.61606216 batch wh loss 9.71748161 batch obj loss 17.6160851 batch_class_loss 11.3308563 epoch total loss: 11479.9639\n",
            "Trained batch: 278 batch loss: 44.3075562 batch xy loss 6.32736 batch wh loss 8.90955 batch obj loss 18.2856464 batch_class_loss 10.7850018 epoch total loss: 11524.2715\n",
            "Trained batch: 279 batch loss: 45.1466217 batch xy loss 7.21667719 batch wh loss 8.26991272 batch obj loss 18.0668583 batch_class_loss 11.5931778 epoch total loss: 11569.418\n",
            "Trained batch: 280 batch loss: 44.9172935 batch xy loss 7.14259195 batch wh loss 8.82004642 batch obj loss 17.6866302 batch_class_loss 11.2680178 epoch total loss: 11614.335\n",
            "Trained batch: 281 batch loss: 34.0384941 batch xy loss 4.85518122 batch wh loss 7.72409058 batch obj loss 14.033741 batch_class_loss 7.42548084 epoch total loss: 11648.373\n",
            "Trained batch: 282 batch loss: 42.7674561 batch xy loss 6.14366102 batch wh loss 8.50880337 batch obj loss 17.7688847 batch_class_loss 10.3461027 epoch total loss: 11691.1406\n",
            "Trained batch: 283 batch loss: 33.6308022 batch xy loss 5.00053215 batch wh loss 6.12562656 batch obj loss 14.182435 batch_class_loss 8.3222084 epoch total loss: 11724.7715\n",
            "Trained batch: 284 batch loss: 44.8054886 batch xy loss 6.60560465 batch wh loss 7.67098665 batch obj loss 18.2768822 batch_class_loss 12.2520113 epoch total loss: 11769.5771\n",
            "Trained batch: 285 batch loss: 40.116 batch xy loss 5.1496067 batch wh loss 7.74623 batch obj loss 16.1676807 batch_class_loss 11.0524826 epoch total loss: 11809.6934\n",
            "Trained batch: 286 batch loss: 50.4457703 batch xy loss 7.50943899 batch wh loss 9.503088 batch obj loss 19.1814461 batch_class_loss 14.2517986 epoch total loss: 11860.1387\n",
            "Trained batch: 287 batch loss: 42.1533127 batch xy loss 6.14933729 batch wh loss 7.5264554 batch obj loss 17.640379 batch_class_loss 10.8371401 epoch total loss: 11902.292\n",
            "Trained batch: 288 batch loss: 40.54039 batch xy loss 6.1953373 batch wh loss 6.72281837 batch obj loss 16.8729267 batch_class_loss 10.7493067 epoch total loss: 11942.832\n",
            "Trained batch: 289 batch loss: 42.9785614 batch xy loss 6.30962276 batch wh loss 6.88853073 batch obj loss 18.0179596 batch_class_loss 11.7624474 epoch total loss: 11985.8105\n",
            "Trained batch: 290 batch loss: 45.0497131 batch xy loss 7.29363251 batch wh loss 8.83956146 batch obj loss 17.2148323 batch_class_loss 11.7016897 epoch total loss: 12030.8604\n",
            "Trained batch: 291 batch loss: 44.1886368 batch xy loss 6.60152912 batch wh loss 9.27163792 batch obj loss 16.9983387 batch_class_loss 11.3171282 epoch total loss: 12075.0488\n",
            "Trained batch: 292 batch loss: 45.8717308 batch xy loss 6.29839134 batch wh loss 9.88842773 batch obj loss 17.8531799 batch_class_loss 11.8317299 epoch total loss: 12120.9209\n",
            "Trained batch: 293 batch loss: 44.2530251 batch xy loss 6.45891237 batch wh loss 7.96940613 batch obj loss 18.0433 batch_class_loss 11.7814074 epoch total loss: 12165.1738\n",
            "Trained batch: 294 batch loss: 43.030407 batch xy loss 5.64032269 batch wh loss 10.2220335 batch obj loss 16.3237724 batch_class_loss 10.8442783 epoch total loss: 12208.2041\n",
            "Trained batch: 295 batch loss: 46.003643 batch xy loss 7.09479 batch wh loss 9.42512798 batch obj loss 17.7993011 batch_class_loss 11.6844254 epoch total loss: 12254.208\n",
            "Trained batch: 296 batch loss: 43.039 batch xy loss 6.9522891 batch wh loss 8.57409286 batch obj loss 16.8380089 batch_class_loss 10.6746082 epoch total loss: 12297.2471\n",
            "Trained batch: 297 batch loss: 45.3414421 batch xy loss 6.46875 batch wh loss 10.2522202 batch obj loss 17.5397339 batch_class_loss 11.0807371 epoch total loss: 12342.5889\n",
            "Trained batch: 298 batch loss: 48.8347473 batch xy loss 7.74447966 batch wh loss 9.06891632 batch obj loss 19.2678757 batch_class_loss 12.7534752 epoch total loss: 12391.4238\n",
            "Trained batch: 299 batch loss: 36.3228531 batch xy loss 5.05017 batch wh loss 7.40686941 batch obj loss 14.8341131 batch_class_loss 9.03170109 epoch total loss: 12427.7471\n",
            "Trained batch: 300 batch loss: 43.5658379 batch xy loss 7.07118607 batch wh loss 7.79644346 batch obj loss 17.5481949 batch_class_loss 11.1500139 epoch total loss: 12471.3125\n",
            "Trained batch: 301 batch loss: 38.6691742 batch xy loss 4.91277027 batch wh loss 7.6803937 batch obj loss 16.643898 batch_class_loss 9.43210888 epoch total loss: 12509.9814\n",
            "Trained batch: 302 batch loss: 44.1326981 batch xy loss 6.71144676 batch wh loss 10.3905382 batch obj loss 16.5704365 batch_class_loss 10.4602785 epoch total loss: 12554.1143\n",
            "Trained batch: 303 batch loss: 43.8623199 batch xy loss 6.28241158 batch wh loss 8.58442783 batch obj loss 17.4731236 batch_class_loss 11.522356 epoch total loss: 12597.9766\n",
            "Trained batch: 304 batch loss: 36.1714096 batch xy loss 5.15447903 batch wh loss 7.16575098 batch obj loss 14.6929121 batch_class_loss 9.15826607 epoch total loss: 12634.1484\n",
            "Trained batch: 305 batch loss: 34.1779404 batch xy loss 5.20020866 batch wh loss 5.89780903 batch obj loss 14.3359146 batch_class_loss 8.7440052 epoch total loss: 12668.3262\n",
            "Trained batch: 306 batch loss: 35.3456345 batch xy loss 4.8654418 batch wh loss 6.60902 batch obj loss 14.7777901 batch_class_loss 9.09338093 epoch total loss: 12703.6719\n",
            "Trained batch: 307 batch loss: 46.1923561 batch xy loss 7.54105234 batch wh loss 7.89261436 batch obj loss 17.5968208 batch_class_loss 13.1618671 epoch total loss: 12749.8643\n",
            "Trained batch: 308 batch loss: 42.6504593 batch xy loss 6.82513523 batch wh loss 8.37280273 batch obj loss 16.0982342 batch_class_loss 11.3542891 epoch total loss: 12792.5146\n",
            "Trained batch: 309 batch loss: 52.7739258 batch xy loss 7.2304163 batch wh loss 11.4522648 batch obj loss 20.2112942 batch_class_loss 13.8799467 epoch total loss: 12845.2891\n",
            "Trained batch: 310 batch loss: 36.8053513 batch xy loss 5.12042 batch wh loss 7.17395115 batch obj loss 15.0634031 batch_class_loss 9.44758 epoch total loss: 12882.0947\n",
            "Trained batch: 311 batch loss: 38.4162292 batch xy loss 5.82453918 batch wh loss 6.74603176 batch obj loss 16.7498474 batch_class_loss 9.09581184 epoch total loss: 12920.5107\n",
            "Trained batch: 312 batch loss: 48.6651611 batch xy loss 7.39215899 batch wh loss 9.7822 batch obj loss 19.1482086 batch_class_loss 12.3425913 epoch total loss: 12969.1758\n",
            "Trained batch: 313 batch loss: 39.0258713 batch xy loss 6.04841137 batch wh loss 7.40111828 batch obj loss 16.4020786 batch_class_loss 9.17426205 epoch total loss: 13008.2012\n",
            "Trained batch: 314 batch loss: 35.3512039 batch xy loss 5.16160059 batch wh loss 7.11641312 batch obj loss 14.3440456 batch_class_loss 8.72914124 epoch total loss: 13043.5527\n",
            "Trained batch: 315 batch loss: 41.2114868 batch xy loss 6.28450394 batch wh loss 8.48049164 batch obj loss 15.7765818 batch_class_loss 10.6699076 epoch total loss: 13084.7646\n",
            "Trained batch: 316 batch loss: 50.4026222 batch xy loss 7.00346088 batch wh loss 11.1492262 batch obj loss 18.8494759 batch_class_loss 13.400465 epoch total loss: 13135.167\n",
            "Trained batch: 317 batch loss: 46.2694817 batch xy loss 7.23488331 batch wh loss 10.2900906 batch obj loss 17.313776 batch_class_loss 11.4307356 epoch total loss: 13181.4365\n",
            "Trained batch: 318 batch loss: 44.5293884 batch xy loss 6.88703728 batch wh loss 10.0712786 batch obj loss 16.9170265 batch_class_loss 10.6540413 epoch total loss: 13225.9658\n",
            "Trained batch: 319 batch loss: 30.0715446 batch xy loss 5.11787796 batch wh loss 5.7327714 batch obj loss 12.2238178 batch_class_loss 6.99707603 epoch total loss: 13256.0371\n",
            "Trained batch: 320 batch loss: 41.8640137 batch xy loss 6.27574873 batch wh loss 8.30370903 batch obj loss 16.6281319 batch_class_loss 10.6564274 epoch total loss: 13297.9014\n",
            "Trained batch: 321 batch loss: 32.5131073 batch xy loss 5.34564066 batch wh loss 5.40875959 batch obj loss 13.2836676 batch_class_loss 8.47503948 epoch total loss: 13330.4141\n",
            "Trained batch: 322 batch loss: 39.8682213 batch xy loss 6.07349968 batch wh loss 8.61384392 batch obj loss 15.1118317 batch_class_loss 10.0690441 epoch total loss: 13370.2822\n",
            "Trained batch: 323 batch loss: 42.4733353 batch xy loss 6.65294838 batch wh loss 7.90789223 batch obj loss 16.7904282 batch_class_loss 11.1220646 epoch total loss: 13412.7559\n",
            "Trained batch: 324 batch loss: 44.6691475 batch xy loss 7.24766779 batch wh loss 8.55710125 batch obj loss 17.1136227 batch_class_loss 11.7507544 epoch total loss: 13457.4248\n",
            "Trained batch: 325 batch loss: 41.5675735 batch xy loss 6.12405586 batch wh loss 7.84437084 batch obj loss 16.9880142 batch_class_loss 10.6111298 epoch total loss: 13498.9922\n",
            "Trained batch: 326 batch loss: 46.4918556 batch xy loss 6.39697552 batch wh loss 9.11946583 batch obj loss 19.0008965 batch_class_loss 11.9745178 epoch total loss: 13545.4844\n",
            "Trained batch: 327 batch loss: 34.5934525 batch xy loss 4.70362568 batch wh loss 7.12485886 batch obj loss 14.4879017 batch_class_loss 8.27706432 epoch total loss: 13580.0781\n",
            "Trained batch: 328 batch loss: 34.8980293 batch xy loss 5.0780592 batch wh loss 6.3211565 batch obj loss 14.3703766 batch_class_loss 9.12843513 epoch total loss: 13614.9766\n",
            "Trained batch: 329 batch loss: 40.6517563 batch xy loss 6.1336565 batch wh loss 8.5270071 batch obj loss 16.028717 batch_class_loss 9.96237946 epoch total loss: 13655.6279\n",
            "Trained batch: 330 batch loss: 28.6799107 batch xy loss 4.38566208 batch wh loss 4.1239357 batch obj loss 12.6640024 batch_class_loss 7.50630903 epoch total loss: 13684.3076\n",
            "Trained batch: 331 batch loss: 42.9082527 batch xy loss 6.31157875 batch wh loss 8.40195942 batch obj loss 16.4060516 batch_class_loss 11.788662 epoch total loss: 13727.2158\n",
            "Trained batch: 332 batch loss: 42.2686615 batch xy loss 5.69190264 batch wh loss 10.0718174 batch obj loss 15.8968296 batch_class_loss 10.6081152 epoch total loss: 13769.4844\n",
            "Trained batch: 333 batch loss: 33.8398552 batch xy loss 4.65439415 batch wh loss 6.15136766 batch obj loss 13.8264952 batch_class_loss 9.20759773 epoch total loss: 13803.3242\n",
            "Trained batch: 334 batch loss: 37.5970383 batch xy loss 5.5787406 batch wh loss 7.42766571 batch obj loss 14.7771988 batch_class_loss 9.81343365 epoch total loss: 13840.9209\n",
            "Trained batch: 335 batch loss: 40.8357315 batch xy loss 5.67341375 batch wh loss 9.79431629 batch obj loss 15.1831007 batch_class_loss 10.1849051 epoch total loss: 13881.7568\n",
            "Trained batch: 336 batch loss: 37.5354805 batch xy loss 5.16627312 batch wh loss 7.03757429 batch obj loss 14.8281631 batch_class_loss 10.5034704 epoch total loss: 13919.292\n",
            "Trained batch: 337 batch loss: 37.4881 batch xy loss 5.47831535 batch wh loss 7.83905268 batch obj loss 14.5333672 batch_class_loss 9.63736248 epoch total loss: 13956.7803\n",
            "Trained batch: 338 batch loss: 54.1735191 batch xy loss 7.96689749 batch wh loss 10.4817295 batch obj loss 20.4155216 batch_class_loss 15.3093719 epoch total loss: 14010.9541\n",
            "Trained batch: 339 batch loss: 38.3694496 batch xy loss 5.65383482 batch wh loss 8.0080986 batch obj loss 15.3637638 batch_class_loss 9.34374714 epoch total loss: 14049.3232\n",
            "Trained batch: 340 batch loss: 39.777298 batch xy loss 6.62451124 batch wh loss 7.5503664 batch obj loss 15.7223263 batch_class_loss 9.88009453 epoch total loss: 14089.1006\n",
            "Trained batch: 341 batch loss: 40.0441 batch xy loss 6.39182901 batch wh loss 7.29180908 batch obj loss 15.989727 batch_class_loss 10.3707371 epoch total loss: 14129.1445\n",
            "Trained batch: 342 batch loss: 46.8402939 batch xy loss 6.97276 batch wh loss 8.5392065 batch obj loss 19.3119087 batch_class_loss 12.0164185 epoch total loss: 14175.9844\n",
            "Trained batch: 343 batch loss: 37.333931 batch xy loss 6.15118694 batch wh loss 6.84902287 batch obj loss 15.5754852 batch_class_loss 8.75823402 epoch total loss: 14213.3184\n",
            "Trained batch: 344 batch loss: 46.6644135 batch xy loss 7.04582071 batch wh loss 9.17604065 batch obj loss 19.1254673 batch_class_loss 11.3170834 epoch total loss: 14259.9824\n",
            "Trained batch: 345 batch loss: 42.679245 batch xy loss 6.37804174 batch wh loss 8.89775467 batch obj loss 16.789547 batch_class_loss 10.613904 epoch total loss: 14302.6621\n",
            "Trained batch: 346 batch loss: 38.2360039 batch xy loss 5.76283121 batch wh loss 7.20075893 batch obj loss 15.1738253 batch_class_loss 10.0985899 epoch total loss: 14340.8984\n",
            "Trained batch: 347 batch loss: 36.417202 batch xy loss 5.33640623 batch wh loss 5.7845335 batch obj loss 15.748003 batch_class_loss 9.54826 epoch total loss: 14377.3154\n",
            "Trained batch: 348 batch loss: 42.2449913 batch xy loss 5.65794373 batch wh loss 10.3565435 batch obj loss 15.6319523 batch_class_loss 10.5985565 epoch total loss: 14419.5605\n",
            "Trained batch: 349 batch loss: 47.1750221 batch xy loss 7.86887026 batch wh loss 8.68922806 batch obj loss 18.3883114 batch_class_loss 12.2286148 epoch total loss: 14466.7354\n",
            "Trained batch: 350 batch loss: 36.8071442 batch xy loss 4.6215086 batch wh loss 7.07458735 batch obj loss 15.2556219 batch_class_loss 9.85542774 epoch total loss: 14503.543\n",
            "Trained batch: 351 batch loss: 37.0350227 batch xy loss 5.25139046 batch wh loss 6.99551964 batch obj loss 14.7778664 batch_class_loss 10.0102444 epoch total loss: 14540.5781\n",
            "Trained batch: 352 batch loss: 35.7061806 batch xy loss 5.58529377 batch wh loss 6.77579498 batch obj loss 13.9206905 batch_class_loss 9.42440224 epoch total loss: 14576.2842\n",
            "Trained batch: 353 batch loss: 38.3901215 batch xy loss 4.49022675 batch wh loss 9.43538094 batch obj loss 15.1191597 batch_class_loss 9.34535408 epoch total loss: 14614.6738\n",
            "Trained batch: 354 batch loss: 50.2443695 batch xy loss 6.63381529 batch wh loss 11.5681105 batch obj loss 18.5119762 batch_class_loss 13.530467 epoch total loss: 14664.918\n",
            "Trained batch: 355 batch loss: 37.8821335 batch xy loss 4.99805975 batch wh loss 7.64217138 batch obj loss 15.5113716 batch_class_loss 9.73053265 epoch total loss: 14702.8\n",
            "Trained batch: 356 batch loss: 42.5463791 batch xy loss 6.12166309 batch wh loss 8.35401535 batch obj loss 16.5889721 batch_class_loss 11.4817266 epoch total loss: 14745.3457\n",
            "Trained batch: 357 batch loss: 36.4907837 batch xy loss 6.06154966 batch wh loss 8.26290321 batch obj loss 14.2097588 batch_class_loss 7.95657158 epoch total loss: 14781.8369\n",
            "Trained batch: 358 batch loss: 37.3935699 batch xy loss 5.2380228 batch wh loss 7.07536173 batch obj loss 15.28162 batch_class_loss 9.79856491 epoch total loss: 14819.2305\n",
            "Trained batch: 359 batch loss: 43.7523117 batch xy loss 5.75646973 batch wh loss 8.92188263 batch obj loss 17.5547104 batch_class_loss 11.5192566 epoch total loss: 14862.9824\n",
            "Trained batch: 360 batch loss: 44.7693329 batch xy loss 6.09978104 batch wh loss 9.53002167 batch obj loss 17.4962387 batch_class_loss 11.6432886 epoch total loss: 14907.752\n",
            "Trained batch: 361 batch loss: 39.6369972 batch xy loss 5.71578 batch wh loss 8.98630238 batch obj loss 15.5194035 batch_class_loss 9.41551113 epoch total loss: 14947.3887\n",
            "Trained batch: 362 batch loss: 44.5045395 batch xy loss 6.26533699 batch wh loss 8.59551 batch obj loss 17.5154305 batch_class_loss 12.1282587 epoch total loss: 14991.8936\n",
            "Trained batch: 363 batch loss: 42.8076782 batch xy loss 6.3878088 batch wh loss 7.50724602 batch obj loss 17.1360607 batch_class_loss 11.7765589 epoch total loss: 15034.7012\n",
            "Trained batch: 364 batch loss: 37.6215973 batch xy loss 5.3604126 batch wh loss 7.6908536 batch obj loss 15.0937481 batch_class_loss 9.47658253 epoch total loss: 15072.3232\n",
            "Trained batch: 365 batch loss: 42.2043953 batch xy loss 5.90292406 batch wh loss 9.43163204 batch obj loss 16.0609398 batch_class_loss 10.8089 epoch total loss: 15114.5273\n",
            "Trained batch: 366 batch loss: 50.730629 batch xy loss 6.64287615 batch wh loss 11.1624928 batch obj loss 19.8802128 batch_class_loss 13.0450468 epoch total loss: 15165.2578\n",
            "Trained batch: 367 batch loss: 40.1051254 batch xy loss 6.53185558 batch wh loss 6.98533726 batch obj loss 16.3562145 batch_class_loss 10.231719 epoch total loss: 15205.3633\n",
            "Trained batch: 368 batch loss: 45.4942703 batch xy loss 6.14356136 batch wh loss 11.9069633 batch obj loss 17.0652294 batch_class_loss 10.3785133 epoch total loss: 15250.8574\n",
            "Trained batch: 369 batch loss: 44.0656548 batch xy loss 6.28381252 batch wh loss 10.3907099 batch obj loss 16.8824444 batch_class_loss 10.5086889 epoch total loss: 15294.9229\n",
            "Trained batch: 370 batch loss: 43.0589333 batch xy loss 6.47302818 batch wh loss 8.82896805 batch obj loss 17.5091209 batch_class_loss 10.247817 epoch total loss: 15337.9814\n",
            "Trained batch: 371 batch loss: 35.5777893 batch xy loss 4.93521 batch wh loss 6.65309525 batch obj loss 15.1219425 batch_class_loss 8.86754 epoch total loss: 15373.5596\n",
            "Trained batch: 372 batch loss: 36.9821434 batch xy loss 5.8038 batch wh loss 7.18179798 batch obj loss 15.1607552 batch_class_loss 8.83579254 epoch total loss: 15410.542\n",
            "Trained batch: 373 batch loss: 47.9764442 batch xy loss 6.87250233 batch wh loss 9.2165308 batch obj loss 19.1447926 batch_class_loss 12.7426195 epoch total loss: 15458.5186\n",
            "Trained batch: 374 batch loss: 48.2657 batch xy loss 6.78367043 batch wh loss 9.65337944 batch obj loss 18.6170597 batch_class_loss 13.2115908 epoch total loss: 15506.7842\n",
            "Trained batch: 375 batch loss: 49.1284447 batch xy loss 7.04213619 batch wh loss 10.035059 batch obj loss 18.3764725 batch_class_loss 13.674778 epoch total loss: 15555.9131\n",
            "Trained batch: 376 batch loss: 38.1204605 batch xy loss 5.12011909 batch wh loss 9.08697224 batch obj loss 14.4940395 batch_class_loss 9.41932869 epoch total loss: 15594.0332\n",
            "Trained batch: 377 batch loss: 53.9994125 batch xy loss 8.45260239 batch wh loss 11.5814686 batch obj loss 19.9432812 batch_class_loss 14.0220594 epoch total loss: 15648.0322\n",
            "Trained batch: 378 batch loss: 50.4995689 batch xy loss 6.83480549 batch wh loss 10.9692059 batch obj loss 19.5516701 batch_class_loss 13.1438885 epoch total loss: 15698.5322\n",
            "Trained batch: 379 batch loss: 44.9020615 batch xy loss 6.80967808 batch wh loss 7.51222038 batch obj loss 18.9542427 batch_class_loss 11.6259165 epoch total loss: 15743.4346\n",
            "Trained batch: 380 batch loss: 50.0543785 batch xy loss 6.14647865 batch wh loss 14.0663042 batch obj loss 18.5263634 batch_class_loss 11.3152342 epoch total loss: 15793.4893\n",
            "Trained batch: 381 batch loss: 40.9500847 batch xy loss 6.35195255 batch wh loss 7.90616035 batch obj loss 16.6477165 batch_class_loss 10.0442543 epoch total loss: 15834.4395\n",
            "Trained batch: 382 batch loss: 47.925827 batch xy loss 7.0380497 batch wh loss 9.14794159 batch obj loss 19.2497692 batch_class_loss 12.4900684 epoch total loss: 15882.3652\n",
            "Trained batch: 383 batch loss: 38.8823624 batch xy loss 5.89881 batch wh loss 7.76111603 batch obj loss 15.7720757 batch_class_loss 9.45035839 epoch total loss: 15921.248\n",
            "Trained batch: 384 batch loss: 47.9295654 batch xy loss 6.83390903 batch wh loss 11.7238693 batch obj loss 18.9653015 batch_class_loss 10.4064875 epoch total loss: 15969.1777\n",
            "Trained batch: 385 batch loss: 49.2823563 batch xy loss 7.16874886 batch wh loss 9.6473341 batch obj loss 19.5018406 batch_class_loss 12.9644299 epoch total loss: 16018.46\n",
            "Trained batch: 386 batch loss: 37.7843971 batch xy loss 5.87838936 batch wh loss 6.23313236 batch obj loss 15.9082508 batch_class_loss 9.76462364 epoch total loss: 16056.2441\n",
            "Trained batch: 387 batch loss: 33.5922356 batch xy loss 4.93148041 batch wh loss 6.10703468 batch obj loss 14.0826521 batch_class_loss 8.47106934 epoch total loss: 16089.8359\n",
            "Trained batch: 388 batch loss: 33.5071831 batch xy loss 4.93476439 batch wh loss 5.76620102 batch obj loss 14.2642441 batch_class_loss 8.54197311 epoch total loss: 16123.3428\n",
            "Trained batch: 389 batch loss: 49.1665039 batch xy loss 7.98281431 batch wh loss 10.5141106 batch obj loss 18.7237072 batch_class_loss 11.9458714 epoch total loss: 16172.5098\n",
            "Trained batch: 390 batch loss: 42.5240555 batch xy loss 6.58779907 batch wh loss 9.37649536 batch obj loss 15.6940727 batch_class_loss 10.8656874 epoch total loss: 16215.0342\n",
            "Trained batch: 391 batch loss: 42.5719376 batch xy loss 6.10301065 batch wh loss 8.24640083 batch obj loss 16.5075073 batch_class_loss 11.7150192 epoch total loss: 16257.6064\n",
            "Trained batch: 392 batch loss: 43.1017151 batch xy loss 5.96645069 batch wh loss 8.59432507 batch obj loss 17.538538 batch_class_loss 11.0024042 epoch total loss: 16300.708\n",
            "Trained batch: 393 batch loss: 35.8762817 batch xy loss 4.94065428 batch wh loss 7.48565769 batch obj loss 14.5711498 batch_class_loss 8.87881756 epoch total loss: 16336.584\n",
            "Trained batch: 394 batch loss: 31.2058563 batch xy loss 4.75786304 batch wh loss 5.86882591 batch obj loss 12.779253 batch_class_loss 7.79991388 epoch total loss: 16367.79\n",
            "Trained batch: 395 batch loss: 40.1252518 batch xy loss 6.50947094 batch wh loss 6.56980324 batch obj loss 16.4108067 batch_class_loss 10.63517 epoch total loss: 16407.916\n",
            "Trained batch: 396 batch loss: 39.9990273 batch xy loss 6.08266449 batch wh loss 7.93144321 batch obj loss 16.7711792 batch_class_loss 9.21374 epoch total loss: 16447.916\n",
            "Trained batch: 397 batch loss: 43.7722816 batch xy loss 6.27922535 batch wh loss 7.27419853 batch obj loss 18.2758312 batch_class_loss 11.9430285 epoch total loss: 16491.6875\n",
            "Trained batch: 398 batch loss: 41.90028 batch xy loss 6.27511406 batch wh loss 7.16174 batch obj loss 17.4528694 batch_class_loss 11.0105543 epoch total loss: 16533.5879\n",
            "Trained batch: 399 batch loss: 32.2074547 batch xy loss 5.12368917 batch wh loss 5.64701843 batch obj loss 14.0103064 batch_class_loss 7.42644 epoch total loss: 16565.7949\n",
            "Trained batch: 400 batch loss: 44.120121 batch xy loss 6.63440323 batch wh loss 9.0990963 batch obj loss 17.2772579 batch_class_loss 11.1093655 epoch total loss: 16609.916\n",
            "Trained batch: 401 batch loss: 48.305542 batch xy loss 7.90226173 batch wh loss 9.70224667 batch obj loss 18.0415478 batch_class_loss 12.6594887 epoch total loss: 16658.2207\n",
            "Trained batch: 402 batch loss: 52.3640099 batch xy loss 8.02132702 batch wh loss 10.3591146 batch obj loss 19.7899227 batch_class_loss 14.1936436 epoch total loss: 16710.584\n",
            "Trained batch: 403 batch loss: 37.5887337 batch xy loss 5.65879345 batch wh loss 6.78309488 batch obj loss 15.5095053 batch_class_loss 9.63733768 epoch total loss: 16748.1719\n",
            "Trained batch: 404 batch loss: 35.1561966 batch xy loss 5.3943615 batch wh loss 6.60801697 batch obj loss 13.7451544 batch_class_loss 9.40866375 epoch total loss: 16783.3281\n",
            "Trained batch: 405 batch loss: 40.5779724 batch xy loss 6.14012146 batch wh loss 8.73925209 batch obj loss 15.3633413 batch_class_loss 10.3352537 epoch total loss: 16823.9062\n",
            "Trained batch: 406 batch loss: 34.090374 batch xy loss 5.14730072 batch wh loss 6.36226749 batch obj loss 13.8831024 batch_class_loss 8.69770622 epoch total loss: 16857.9961\n",
            "Trained batch: 407 batch loss: 42.9627533 batch xy loss 6.29735041 batch wh loss 7.63759422 batch obj loss 18.0057373 batch_class_loss 11.022068 epoch total loss: 16900.959\n",
            "Trained batch: 408 batch loss: 39.6098137 batch xy loss 5.84321 batch wh loss 7.76936245 batch obj loss 15.7987137 batch_class_loss 10.1985235 epoch total loss: 16940.5684\n",
            "Trained batch: 409 batch loss: 41.9902458 batch xy loss 6.3677907 batch wh loss 8.31574345 batch obj loss 16.8207302 batch_class_loss 10.4859829 epoch total loss: 16982.5586\n",
            "Trained batch: 410 batch loss: 42.3169479 batch xy loss 6.22739697 batch wh loss 8.23119926 batch obj loss 17.0515881 batch_class_loss 10.8067656 epoch total loss: 17024.875\n",
            "Trained batch: 411 batch loss: 55.1223106 batch xy loss 8.44087887 batch wh loss 10.9672756 batch obj loss 20.6966534 batch_class_loss 15.017498 epoch total loss: 17079.998\n",
            "Trained batch: 412 batch loss: 40.3398361 batch xy loss 6.08010292 batch wh loss 7.83130693 batch obj loss 16.1327057 batch_class_loss 10.2957201 epoch total loss: 17120.3379\n",
            "Trained batch: 413 batch loss: 45.5579758 batch xy loss 5.99456835 batch wh loss 10.3967943 batch obj loss 17.1209011 batch_class_loss 12.0457134 epoch total loss: 17165.8965\n",
            "Trained batch: 414 batch loss: 40.5748253 batch xy loss 5.9232378 batch wh loss 9.66662 batch obj loss 15.5731173 batch_class_loss 9.41185 epoch total loss: 17206.4707\n",
            "Trained batch: 415 batch loss: 47.6303673 batch xy loss 7.12953806 batch wh loss 10.4653254 batch obj loss 18.1768875 batch_class_loss 11.8586168 epoch total loss: 17254.1016\n",
            "Trained batch: 416 batch loss: 39.1542244 batch xy loss 5.96401739 batch wh loss 7.54994774 batch obj loss 15.2431049 batch_class_loss 10.3971548 epoch total loss: 17293.2559\n",
            "Trained batch: 417 batch loss: 42.9149551 batch xy loss 6.4668 batch wh loss 9.43480396 batch obj loss 16.4990921 batch_class_loss 10.5142574 epoch total loss: 17336.1699\n",
            "Trained batch: 418 batch loss: 47.5807724 batch xy loss 7.6036129 batch wh loss 8.94726944 batch obj loss 18.8171158 batch_class_loss 12.2127733 epoch total loss: 17383.75\n",
            "Trained batch: 419 batch loss: 41.7215538 batch xy loss 6.67780209 batch wh loss 7.38273048 batch obj loss 17.138855 batch_class_loss 10.5221682 epoch total loss: 17425.4707\n",
            "Trained batch: 420 batch loss: 38.6199493 batch xy loss 6.02573919 batch wh loss 7.75441933 batch obj loss 15.5528526 batch_class_loss 9.28693867 epoch total loss: 17464.0898\n",
            "Trained batch: 421 batch loss: 35.8502502 batch xy loss 5.63663673 batch wh loss 6.69171667 batch obj loss 15.1633492 batch_class_loss 8.35854816 epoch total loss: 17499.9395\n",
            "Trained batch: 422 batch loss: 44.0603104 batch xy loss 6.37092829 batch wh loss 9.25986099 batch obj loss 17.4118938 batch_class_loss 11.0176258 epoch total loss: 17544\n",
            "Trained batch: 423 batch loss: 52.7028732 batch xy loss 7.85998058 batch wh loss 10.6510553 batch obj loss 20.2605743 batch_class_loss 13.9312611 epoch total loss: 17596.7031\n",
            "Trained batch: 424 batch loss: 44.7356873 batch xy loss 6.87867928 batch wh loss 10.3714838 batch obj loss 16.3286343 batch_class_loss 11.1568909 epoch total loss: 17641.4395\n",
            "Trained batch: 425 batch loss: 48.111187 batch xy loss 7.17492771 batch wh loss 10.5121021 batch obj loss 17.876236 batch_class_loss 12.5479183 epoch total loss: 17689.5508\n",
            "Trained batch: 426 batch loss: 37.9560623 batch xy loss 5.72617865 batch wh loss 6.60448074 batch obj loss 16.2457867 batch_class_loss 9.37961769 epoch total loss: 17727.5078\n",
            "Trained batch: 427 batch loss: 48.5974579 batch xy loss 6.78730679 batch wh loss 11.2182922 batch obj loss 18.6769867 batch_class_loss 11.9148731 epoch total loss: 17776.1055\n",
            "Trained batch: 428 batch loss: 37.8487473 batch xy loss 4.76467609 batch wh loss 6.82697725 batch obj loss 16.3762245 batch_class_loss 9.88087082 epoch total loss: 17813.9551\n",
            "Trained batch: 429 batch loss: 37.5172386 batch xy loss 5.20064926 batch wh loss 6.22381449 batch obj loss 16.0100918 batch_class_loss 10.0826864 epoch total loss: 17851.4727\n",
            "Trained batch: 430 batch loss: 37.7903976 batch xy loss 6.3301158 batch wh loss 6.29396057 batch obj loss 15.0572557 batch_class_loss 10.1090679 epoch total loss: 17889.2637\n",
            "Trained batch: 431 batch loss: 42.0776253 batch xy loss 5.65665 batch wh loss 7.99924231 batch obj loss 17.3166256 batch_class_loss 11.1051102 epoch total loss: 17931.3418\n",
            "Trained batch: 432 batch loss: 37.7870407 batch xy loss 6.16020441 batch wh loss 5.91494036 batch obj loss 15.928 batch_class_loss 9.78389454 epoch total loss: 17969.1289\n",
            "Trained batch: 433 batch loss: 10.4900904 batch xy loss 1.59641767 batch wh loss 1.44565845 batch obj loss 4.95342064 batch_class_loss 2.49459362 epoch total loss: 17979.6191\n",
            "20201008-160732 Epoch 4 train loss 41.52336883544922, total train batches 433, 78.65324401855469 examples per second\n",
            "4 , 6.163769 , 8.232123 , 10.633376 , 16.494110 , 41.523369 , 18.572538\n",
            "\n",
            "20201008-160951 Epoch 4 val loss 18.572538375854492, total val batches 191, 88.0560073852539 examples per second\n",
            "20201008-160951 Started epoch 5 with learning rate 0.01. Current LR patience count is 2 epochs. Last lowest train loss is 41.52336883544922. Last lowest val loss is 17.39972686767578.\n",
            "Trained batch: 1 batch loss: 33.7071152 batch xy loss 4.90027952 batch wh loss 6.93349266 batch obj loss 13.4818401 batch_class_loss 8.39150429 epoch total loss: 33.7071152\n",
            "Trained batch: 2 batch loss: 45.2475204 batch xy loss 7.26334953 batch wh loss 9.80616 batch obj loss 16.879261 batch_class_loss 11.2987499 epoch total loss: 78.9546356\n",
            "Trained batch: 3 batch loss: 35.5675049 batch xy loss 5.3132844 batch wh loss 6.32817554 batch obj loss 14.4927921 batch_class_loss 9.43325233 epoch total loss: 114.522141\n",
            "Trained batch: 4 batch loss: 31.4732895 batch xy loss 4.51319742 batch wh loss 5.70790148 batch obj loss 13.8851299 batch_class_loss 7.36705875 epoch total loss: 145.995422\n",
            "Trained batch: 5 batch loss: 35.2341957 batch xy loss 5.31681347 batch wh loss 5.95493317 batch obj loss 14.8751431 batch_class_loss 9.08730221 epoch total loss: 181.229614\n",
            "Trained batch: 6 batch loss: 34.6671 batch xy loss 4.93749905 batch wh loss 7.26047373 batch obj loss 13.5936766 batch_class_loss 8.87545 epoch total loss: 215.896713\n",
            "Trained batch: 7 batch loss: 42.479908 batch xy loss 5.86017466 batch wh loss 9.16884327 batch obj loss 15.8920603 batch_class_loss 11.5588322 epoch total loss: 258.376617\n",
            "Trained batch: 8 batch loss: 44.6009712 batch xy loss 6.50339 batch wh loss 8.05820942 batch obj loss 17.5918751 batch_class_loss 12.4474955 epoch total loss: 302.9776\n",
            "Trained batch: 9 batch loss: 37.1855698 batch xy loss 5.23694134 batch wh loss 7.70655727 batch obj loss 14.4038868 batch_class_loss 9.83818531 epoch total loss: 340.163177\n",
            "Trained batch: 10 batch loss: 40.6167145 batch xy loss 6.40294743 batch wh loss 8.13826275 batch obj loss 15.7316589 batch_class_loss 10.3438454 epoch total loss: 380.779907\n",
            "Trained batch: 11 batch loss: 40.6689873 batch xy loss 6.65227604 batch wh loss 9.49746132 batch obj loss 14.5667114 batch_class_loss 9.95253944 epoch total loss: 421.448883\n",
            "Trained batch: 12 batch loss: 41.4992714 batch xy loss 6.59849453 batch wh loss 7.20599222 batch obj loss 16.8543758 batch_class_loss 10.8404074 epoch total loss: 462.948151\n",
            "Trained batch: 13 batch loss: 49.2981339 batch xy loss 7.4623127 batch wh loss 10.6253481 batch obj loss 18.5510159 batch_class_loss 12.6594543 epoch total loss: 512.246277\n",
            "Trained batch: 14 batch loss: 37.0292358 batch xy loss 5.16613865 batch wh loss 8.81614113 batch obj loss 14.3937912 batch_class_loss 8.65316391 epoch total loss: 549.275513\n",
            "Trained batch: 15 batch loss: 41.7492676 batch xy loss 6.22185 batch wh loss 7.51278639 batch obj loss 17.2945442 batch_class_loss 10.7200851 epoch total loss: 591.02478\n",
            "Trained batch: 16 batch loss: 33.7371674 batch xy loss 4.97963572 batch wh loss 6.25739384 batch obj loss 14.4265175 batch_class_loss 8.07361794 epoch total loss: 624.761963\n",
            "Trained batch: 17 batch loss: 36.6724854 batch xy loss 5.352736 batch wh loss 7.16583061 batch obj loss 15.5233583 batch_class_loss 8.63056183 epoch total loss: 661.434448\n",
            "Trained batch: 18 batch loss: 45.3033142 batch xy loss 6.95825291 batch wh loss 8.20576286 batch obj loss 18.7922287 batch_class_loss 11.3470697 epoch total loss: 706.737793\n",
            "Trained batch: 19 batch loss: 41.8718796 batch xy loss 6.80510283 batch wh loss 8.3544817 batch obj loss 16.5981903 batch_class_loss 10.1141014 epoch total loss: 748.60968\n",
            "Trained batch: 20 batch loss: 46.3525276 batch xy loss 6.45440626 batch wh loss 8.20250416 batch obj loss 19.3349476 batch_class_loss 12.3606691 epoch total loss: 794.962219\n",
            "Trained batch: 21 batch loss: 38.9147 batch xy loss 6.11626244 batch wh loss 7.3057704 batch obj loss 15.5742073 batch_class_loss 9.91845798 epoch total loss: 833.876892\n",
            "Trained batch: 22 batch loss: 38.4485359 batch xy loss 4.80825 batch wh loss 7.21410608 batch obj loss 16.5563869 batch_class_loss 9.86979485 epoch total loss: 872.325439\n",
            "Trained batch: 23 batch loss: 39.5817947 batch xy loss 6.10771084 batch wh loss 9.39222 batch obj loss 14.402132 batch_class_loss 9.67973137 epoch total loss: 911.907227\n",
            "Trained batch: 24 batch loss: 43.0724869 batch xy loss 6.77906322 batch wh loss 8.31627655 batch obj loss 16.4243832 batch_class_loss 11.5527668 epoch total loss: 954.979736\n",
            "Trained batch: 25 batch loss: 33.6282234 batch xy loss 4.51985407 batch wh loss 6.35390186 batch obj loss 13.8608446 batch_class_loss 8.8936224 epoch total loss: 988.608\n",
            "Trained batch: 26 batch loss: 37.0318909 batch xy loss 5.25224161 batch wh loss 6.93769598 batch obj loss 15.0258245 batch_class_loss 9.81613064 epoch total loss: 1025.63989\n",
            "Trained batch: 27 batch loss: 41.4236603 batch xy loss 5.85757256 batch wh loss 9.45083523 batch obj loss 15.7319021 batch_class_loss 10.3833523 epoch total loss: 1067.0636\n",
            "Trained batch: 28 batch loss: 40.9606552 batch xy loss 5.70699167 batch wh loss 9.35715 batch obj loss 15.5552645 batch_class_loss 10.3412476 epoch total loss: 1108.02429\n",
            "Trained batch: 29 batch loss: 37.6026649 batch xy loss 5.28696823 batch wh loss 6.24381828 batch obj loss 15.8647938 batch_class_loss 10.2070866 epoch total loss: 1145.62695\n",
            "Trained batch: 30 batch loss: 45.5125885 batch xy loss 6.807 batch wh loss 8.9574976 batch obj loss 17.5406418 batch_class_loss 12.2074509 epoch total loss: 1191.13953\n",
            "Trained batch: 31 batch loss: 35.6328201 batch xy loss 5.81023216 batch wh loss 6.3040781 batch obj loss 14.632719 batch_class_loss 8.88579178 epoch total loss: 1226.77234\n",
            "Trained batch: 32 batch loss: 46.4681511 batch xy loss 7.37220764 batch wh loss 8.98700142 batch obj loss 17.8222809 batch_class_loss 12.2866621 epoch total loss: 1273.24048\n",
            "Trained batch: 33 batch loss: 33.9969673 batch xy loss 4.77873421 batch wh loss 5.65760326 batch obj loss 15.0621605 batch_class_loss 8.49846935 epoch total loss: 1307.23743\n",
            "Trained batch: 34 batch loss: 43.5646553 batch xy loss 5.99292 batch wh loss 8.80851364 batch obj loss 16.7957668 batch_class_loss 11.9674578 epoch total loss: 1350.80212\n",
            "Trained batch: 35 batch loss: 38.0196381 batch xy loss 5.48043442 batch wh loss 7.73840714 batch obj loss 15.4484215 batch_class_loss 9.35237503 epoch total loss: 1388.82178\n",
            "Trained batch: 36 batch loss: 43.6531563 batch xy loss 6.72975874 batch wh loss 8.05313396 batch obj loss 17.7185802 batch_class_loss 11.1516809 epoch total loss: 1432.47498\n",
            "Trained batch: 37 batch loss: 41.907547 batch xy loss 6.14457703 batch wh loss 8.62571907 batch obj loss 16.4308491 batch_class_loss 10.7064009 epoch total loss: 1474.38257\n",
            "Trained batch: 38 batch loss: 38.5973473 batch xy loss 5.51612 batch wh loss 6.84953308 batch obj loss 16.3490734 batch_class_loss 9.88262177 epoch total loss: 1512.97986\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}